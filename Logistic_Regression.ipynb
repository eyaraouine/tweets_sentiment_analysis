{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Logistic Regression\n",
    "Welcome to week one of this specialization. You will learn about logistic regression. Concretely, you will be implementing logistic regression for sentiment analysis on tweets. Given a tweet, you will decide if it has a positive sentiment or a negative one. Specifically you will: \n",
    "\n",
    "* Learn how to extract features for logistic regression given some text\n",
    "* Implement logistic regression from scratch\n",
    "* Apply logistic regression on a natural language processing task\n",
    "* Test using your logistic regression\n",
    "* Perform error analysis\n",
    "\n",
    "## Important Note on Submission to the AutoGrader\n",
    "\n",
    "Before submitting your assignment to the AutoGrader, please make sure you are not doing the following:\n",
    "\n",
    "1. You have not added any _extra_ `print` statement(s) in the assignment.\n",
    "2. You have not added any _extra_ code cell(s) in the assignment.\n",
    "3. You have not changed any of the function parameters.\n",
    "4. You are not using any global variables inside your graded exercises. Unless specifically instructed to do so, please refrain from it and use the local variables instead.\n",
    "5. You are not changing the assignment code where it is not required, like creating _extra_ variables.\n",
    "\n",
    "If you do any of the following, you will get something like, `Grader Error: Grader feedback not found` (or similarly unexpected) error upon submitting your assignment. Before asking for help/debugging the errors in your assignment, check for these first. If this is the case, and you don't remember the changes you have made, you can get a fresh copy of the assignment by following these [instructions](https://www.coursera.org/learn/classification-vector-spaces-in-nlp/supplement/YLuAg/h-ow-to-refresh-your-workspace).\n",
    "\n",
    "Lets get started!\n",
    "\n",
    "We will be using a data set of tweets. Hopefully you will get more than 99% accuracy.  \n",
    "Run the cell below to load in the packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Import Functions and Data](#0)\n",
    "- [1 - Logistic Regression](#1)\n",
    "    - [1.1 - Sigmoid](#1-1)\n",
    "        - [Exercise 1 - sigmoid (UNQ_C1)](#ex-1)\n",
    "    - [1.2 - Cost function and Gradient](#1-2)\n",
    "        - [Exercise 2 - gradientDescent (UNQ_C2)](#ex-2)\n",
    "- [2 - Extracting the Features](#2)\n",
    "    - [Exercise 3 - extract_features (UNQ_C3)](#ex-3)\n",
    "- [3 - Training Your Model](#3)\n",
    "- [4 - Test your Logistic Regression](#4)\n",
    "    - [Exercise 4 - predict_tweet (UNQ_C4)](#ex-4)\n",
    "    - [4.1 - Check the Performance using the Test Set](#4-1)\n",
    "        - [Exercise 5 - test_logistic_regression (UNQ_C5)](#ex-5)\n",
    "- [5 - Error Analysis](#5)\n",
    "- [6 - Predict with your own Tweet](#6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='0'></a>\n",
    "## Import Functions and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run this cell to import nltk\n",
    "import nltk\n",
    "from os import getcwd\n",
    "import w1_unittest\n",
    "\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imported Functions\n",
    "\n",
    "Download the data needed for this assignment. Check out the [documentation for the twitter_samples dataset](http://www.nltk.org/howto/twitter.html).\n",
    "\n",
    "* twitter_samples: if you're running this notebook on your local computer, you will need to download it using:\n",
    "```Python\n",
    "nltk.download('twitter_samples')\n",
    "```\n",
    "\n",
    "* stopwords: if you're running this notebook on your local computer, you will need to download it using:\n",
    "```python\n",
    "nltk.download('stopwords')\n",
    "```\n",
    "\n",
    "#### Import some helper functions that we provided in the utils.py file:\n",
    "* process_tweet: cleans the text, tokenizes it into separate words, removes stopwords, and converts words to stems.\n",
    "* build_freqs: this counts how often a word in the 'corpus' (the entire set of tweets) was associated with a positive label '1' or a negative label '0', then builds the 'freqs' dictionary, where each key is the (word,label) tuple, and the value is the count of its frequency within the corpus of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = f\"{getcwd()}/../tmp2/\"\n",
    "nltk.data.path.append(filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import twitter_samples \n",
    "\n",
    "from utils import process_tweet, build_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the Data\n",
    "* The `twitter_samples` contains subsets of five thousand positive_tweets, five thousand negative_tweets, and the full set of 10,000 tweets.  \n",
    "    * If you used all three datasets, we would introduce duplicates of the positive tweets and negative tweets.  \n",
    "    * You will select just the five thousand positive tweets and five thousand negative tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the set of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train test split: 20% will be in the test set, and 80% in the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into two pieces, one for training and one for testing (validation set) \n",
    "test_pos = all_positive_tweets[4000:]\n",
    "train_pos = all_positive_tweets[:4000]\n",
    "test_neg = all_negative_tweets[4000:]\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "\n",
    "train_x = train_pos + train_neg \n",
    "test_x = test_pos + test_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create the numpy array of positive labels and negative labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine positive and negative labels\n",
    "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
    "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_y.shape = (8000, 1)\n",
      "test_y.shape = (2000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape train and test sets\n",
    "print(\"train_y.shape = \" + str(train_y.shape))\n",
    "print(\"test_y.shape = \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create the frequency dictionary using the imported build_freqs function.  \n",
    "    * We highly recommend that you open utils.py and read the build_freqs function to understand what it is doing.\n",
    "    * To view the file directory, go to the menu and click File->Open.\n",
    "\n",
    "```Python\n",
    "    for y,tweet in zip(ys, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair = (word, y)\n",
    "            if pair in freqs:\n",
    "                freqs[pair] += 1\n",
    "            else:\n",
    "                freqs[pair] = 1\n",
    "```\n",
    "* Notice how the outer for loop goes through each tweet, and the inner for loop steps through each word in a tweet.\n",
    "* The 'freqs' dictionary is the frequency dictionary that's being built. \n",
    "* The key is the tuple (word, label), such as (\"happy\",1) or (\"happy\",0).  The value stored for each key is the count of how many times the word \"happy\" was associated with a positive label, or how many times \"happy\" was associated with a negative label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(freqs) = <class 'dict'>\n",
      "len(freqs) = 11436\n"
     ]
    }
   ],
   "source": [
    "# create frequency dictionary\n",
    "freqs = build_freqs(train_x, train_y)\n",
    "\n",
    "# check the output\n",
    "print(\"type(freqs) = \" + str(type(freqs)))\n",
    "print(\"len(freqs) = \" + str(len(freqs.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected output\n",
    "```\n",
    "type(freqs) = <class 'dict'>\n",
    "len(freqs) = 11436\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Tweet\n",
    "The given function 'process_tweet' tokenizes the tweet into individual words, removes stop words and applies stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example of a positive tweet: \n",
      " #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "\n",
      "This is an example of the processed version of the tweet: \n",
      " ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "# test the function below\n",
    "print('This is an example of a positive tweet: \\n', train_x[0])\n",
    "print('\\nThis is an example of the processed version of the tweet: \\n', process_tweet(train_x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected output\n",
    "```\n",
    "This is an example of a positive tweet: \n",
    " #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
    " \n",
    "This is an example of the processes version: \n",
    " ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Logistic Regression \n",
    "\n",
    "<a name='1-1'></a>\n",
    "### 1.1 - Sigmoid\n",
    "You will learn to use logistic regression for text classification. \n",
    "* The sigmoid function is defined as: \n",
    "\n",
    "$$ h(z) = \\frac{1}{1+\\exp^{-z}} \\tag{1}$$\n",
    "\n",
    "It maps the input 'z' to a value that ranges between 0 and 1, and so it can be treated as a probability. \n",
    "\n",
    "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='./images/sigmoid_plot.jpg' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:300px;height:200px;\" /> Figure 1 </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-1'></a>\n",
    "### Exercise 1 -  sigmoid\n",
    "Implement the sigmoid function. \n",
    "* You will want this function to work if z is a scalar as well as if it is an array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li><a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html\" > numpy.exp </a> </li>\n",
    "\n",
    "</ul>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C1 GRADED FUNCTION: sigmoid\n",
    "def sigmoid(z): \n",
    "    '''\n",
    "    Input:\n",
    "        z: is the input (can be a scalar or an array)\n",
    "    Output:\n",
    "        h: the sigmoid of z\n",
    "    '''\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # calculate the sigmoid of z\n",
    "    h = 1 / (1 + np.exp(-z))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS!\n",
      "CORRECT!\n"
     ]
    }
   ],
   "source": [
    "# Testing your function \n",
    "if (sigmoid(0) == 0.5):\n",
    "    print('SUCCESS!')\n",
    "else:\n",
    "    print('Oops!')\n",
    "\n",
    "if (sigmoid(4.92) == 0.9927537604041685):\n",
    "    print('CORRECT!')\n",
    "else:\n",
    "    print('Oops again!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "w1_unittest.test_sigmoid(sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression: Regression and a Sigmoid\n",
    "\n",
    "Logistic regression takes a regular linear regression, and applies a sigmoid to the output of the linear regression.\n",
    "\n",
    "Regression:\n",
    "$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$\n",
    "Note that the $\\theta$ values are \"weights\". If you took the deep learning specialization, we referred to the weights with the 'w' vector.  In this course, we're using a different variable $\\theta$ to refer to the weights.\n",
    "\n",
    "Logistic regression\n",
    "$$ h(z) = \\frac{1}{1+\\exp^{-z}}$$\n",
    "$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$\n",
    "We will refer to 'z' as the 'logits'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1-2'></a>\n",
    "### 1.2 - Cost function and Gradient\n",
    "\n",
    "The cost function used for logistic regression is the average of the log loss across all training examples:\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)}))\\tag{5} $$\n",
    "* $m$ is the number of training examples\n",
    "* $y^{(i)}$ is the actual label of training example 'i'.\n",
    "* $h(z^{(i)})$ is the model's prediction for the training example 'i'.\n",
    "\n",
    "The loss function for a single training example is\n",
    "$$ Loss = -1 \\times \\left( y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)})) \\right)$$\n",
    "\n",
    "* All the $h$ values are between 0 and 1, so the logs will be negative. That is the reason for the factor of -1 applied to the sum of the two loss terms.\n",
    "* Note that when the model predicts 1 ($h(z(\\theta)) = 1$) and the label 'y' is also 1, the loss for that training example is 0. \n",
    "* Similarly, when the model predicts 0 ($h(z(\\theta)) = 0$) and the actual label is also 0, the loss for that training example is 0. \n",
    "* However, when the model prediction is close to 1 ($h(z(\\theta)) = 0.9999$) and the label is 0, the second term of the log loss becomes a large negative number, which is then multiplied by the overall factor of -1 to convert it to a positive loss value. $-1 \\times (1 - 0) \\times log(1 - 0.9999) \\approx 9.2$ The closer the model prediction gets to 1, the larger the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.210340371976294"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify that when the model predicts close to 1, but the actual label is 0, the loss is a large positive value\n",
    "-1 * (1 - 0) * np.log(1 - 0.9999) # loss is about 9.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Likewise, if the model predicts close to 0 ($h(z) = 0.0001$) but the actual label is 1, the first term in the loss function becomes a large number: $-1 \\times log(0.0001) \\approx 9.2$.  The closer the prediction is to zero, the larger the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.210340371976182"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify that when the model predicts close to 0 but the actual label is 1, the loss is a large positive value\n",
    "-1 * np.log(0.0001) # loss is about 9.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update the weights\n",
    "\n",
    "To update your weight vector $\\theta$, you will apply gradient descent to iteratively improve your model's predictions.  \n",
    "The gradient of the cost function $J$ with respect to one of the weights $\\theta_j$ is:\n",
    "\n",
    "$$\\nabla_{\\theta_j}J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m(h^{(i)}-y^{(i)})x^{(i)}_j \\tag{5}$$\n",
    "* 'i' is the index across all 'm' training examples.\n",
    "* 'j' is the index of the weight $\\theta_j$, so $x^{(i)}_j$ is the feature associated with weight $\\theta_j$\n",
    "\n",
    "* To update the weight $\\theta_j$, we adjust it by subtracting a fraction of the gradient determined by $\\alpha$:\n",
    "$$\\theta_j = \\theta_j - \\alpha \\times \\nabla_{\\theta_j}J(\\theta) $$\n",
    "* The learning rate $\\alpha$ is a value that we choose to control how big a single update will be.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-2'></a>\n",
    "### Exercise 2 - gradientDescent\n",
    "Implement gradient descent function.\n",
    "* The number of iterations 'num_iters\" is the number of times that you'll use the entire training set.\n",
    "* For each iteration, you'll calculate the cost function using all training examples (there are 'm' training examples), and for all features.\n",
    "* Instead of updating a single weight $\\theta_i$ at a time, we can update all the weights in the column vector:  \n",
    "$$\\mathbf{\\theta} = \\begin{pmatrix}\n",
    "\\theta_0\n",
    "\\\\\n",
    "\\theta_1\n",
    "\\\\ \n",
    "\\theta_2 \n",
    "\\\\ \n",
    "\\vdots\n",
    "\\\\ \n",
    "\\theta_n\n",
    "\\end{pmatrix}$$\n",
    "* $\\mathbf{\\theta}$ has dimensions (n+1, 1), where 'n' is the number of features, and there is one more element for the bias term $\\theta_0$ (note that the corresponding feature value $\\mathbf{x_0}$ is 1).\n",
    "* The 'logits', 'z', are calculated by multiplying the feature matrix 'x' with the weight vector 'theta'.  $z = \\mathbf{x}\\mathbf{\\theta}$\n",
    "    * $\\mathbf{x}$ has dimensions (m, n+1) \n",
    "    * $\\mathbf{\\theta}$: has dimensions (n+1, 1)\n",
    "    * $\\mathbf{z}$: has dimensions (m, 1)\n",
    "* The prediction 'h', is calculated by applying the sigmoid to each element in 'z': $h(z) = sigmoid(z)$, and has dimensions (m,1).\n",
    "* The cost function $J$ is calculated by taking the dot product of the vectors 'y' and 'log(h)'.  Since both 'y' and 'h' are column vectors (m,1), transpose the vector to the left, so that matrix multiplication of a row vector with column vector performs the dot product.\n",
    "$$J = \\frac{-1}{m} \\times \\left(\\mathbf{y}^T \\cdot log(\\mathbf{h}) + \\mathbf{(1-y)}^T \\cdot log(\\mathbf{1-h}) \\right)$$\n",
    "* The update of theta is also vectorized.  Because the dimensions of $\\mathbf{x}$ are (m, n+1), and both $\\mathbf{h}$ and $\\mathbf{y}$ are (m, 1), we need to transpose the $\\mathbf{x}$ and place it on the left in order to perform matrix multiplication, which then yields the (n+1, 1) answer we need:\n",
    "$$\\mathbf{\\theta} = \\mathbf{\\theta} - \\frac{\\alpha}{m} \\times \\left( \\mathbf{x}^T \\cdot \\left( \\mathbf{h-y} \\right) \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>use numpy.dot for matrix multiplication.</li>\n",
    "    <li>To ensure that the fraction -1/m is a decimal value, cast either the numerator or denominator (or both), like `float(1)`, or write `1.` for the float version of 1. </li>\n",
    "</ul>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C2 GRADED FUNCTION: gradientDescent\n",
    "def gradientDescent(x, y, theta, alpha, num_iters):\n",
    "    '''\n",
    "    Input:\n",
    "        x: matrix of features which is (m,n+1)\n",
    "        y: corresponding labels of the input matrix x, dimensions (m,1)\n",
    "        theta: weight vector of dimension (n+1,1)\n",
    "        alpha: learning rate\n",
    "        num_iters: number of iterations you want to train your model for\n",
    "    Output:\n",
    "        J: the final cost\n",
    "        theta: your final weight vector\n",
    "    Hint: you might want to print the cost to make sure that it is going down.\n",
    "    '''\n",
    "    ### START CODE HERE ###\n",
    "    # get 'm', the number of rows in matrix x\n",
    "    m = x.shape[0]\n",
    "    \n",
    "    for i in range(0, num_iters):\n",
    "        \n",
    "        # get z, the dot product of x and theta\n",
    "        z = np.dot(x, theta)\n",
    "        \n",
    "        # get the sigmoid of z\n",
    "        h = sigmoid(z)\n",
    "        \n",
    "        # calculate the cost function\n",
    "        J = (-1 / m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
    "\n",
    "        # update the weights theta\n",
    "        theta = theta - (alpha / m) * np.dot(x.T, (h - y))\n",
    "        \n",
    "        # Print the cost \n",
    "        print(f'Cost at iteration {i}: {J}')\n",
    "        \n",
    "    return J, theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at iteration 0: 0.6931471805599454\n",
      "Cost at iteration 1: 0.6930540741993099\n",
      "Cost at iteration 2: 0.6929615149729413\n",
      "Cost at iteration 3: 0.6928694989001036\n",
      "Cost at iteration 4: 0.6927780220289215\n",
      "Cost at iteration 5: 0.6926870804361839\n",
      "Cost at iteration 6: 0.6925966702271453\n",
      "Cost at iteration 7: 0.6925067875353308\n",
      "Cost at iteration 8: 0.692417428522341\n",
      "Cost at iteration 9: 0.6923285893776578\n",
      "Cost at iteration 10: 0.6922402663184535\n",
      "Cost at iteration 11: 0.6921524555893973\n",
      "Cost at iteration 12: 0.6920651534624667\n",
      "Cost at iteration 13: 0.6919783562367572\n",
      "Cost at iteration 14: 0.6918920602382953\n",
      "Cost at iteration 15: 0.6918062618198509\n",
      "Cost at iteration 16: 0.6917209573607517\n",
      "Cost at iteration 17: 0.691636143266698\n",
      "Cost at iteration 18: 0.6915518159695806\n",
      "Cost at iteration 19: 0.6914679719272961\n",
      "Cost at iteration 20: 0.6913846076235677\n",
      "Cost at iteration 21: 0.6913017195677635\n",
      "Cost at iteration 22: 0.691219304294718\n",
      "Cost at iteration 23: 0.691137358364553\n",
      "Cost at iteration 24: 0.6910558783625025\n",
      "Cost at iteration 25: 0.6909748608987344\n",
      "Cost at iteration 26: 0.6908943026081777\n",
      "Cost at iteration 27: 0.6908142001503472\n",
      "Cost at iteration 28: 0.690734550209172\n",
      "Cost at iteration 29: 0.6906553494928227\n",
      "Cost at iteration 30: 0.6905765947335417\n",
      "Cost at iteration 31: 0.6904982826874734\n",
      "Cost at iteration 32: 0.6904204101344955\n",
      "Cost at iteration 33: 0.6903429738780519\n",
      "Cost at iteration 34: 0.6902659707449859\n",
      "Cost at iteration 35: 0.6901893975853757\n",
      "Cost at iteration 36: 0.6901132512723692\n",
      "Cost at iteration 37: 0.6900375287020212\n",
      "Cost at iteration 38: 0.6899622267931314\n",
      "Cost at iteration 39: 0.6898873424870823\n",
      "Cost at iteration 40: 0.68981287274768\n",
      "Cost at iteration 41: 0.6897388145609951\n",
      "Cost at iteration 42: 0.6896651649352031\n",
      "Cost at iteration 43: 0.6895919209004296\n",
      "Cost at iteration 44: 0.6895190795085919\n",
      "Cost at iteration 45: 0.6894466378332456\n",
      "Cost at iteration 46: 0.6893745929694289\n",
      "Cost at iteration 47: 0.6893029420335115\n",
      "Cost at iteration 48: 0.6892316821630398\n",
      "Cost at iteration 49: 0.689160810516589\n",
      "Cost at iteration 50: 0.6890903242736095\n",
      "Cost at iteration 51: 0.6890202206342803\n",
      "Cost at iteration 52: 0.6889504968193593\n",
      "Cost at iteration 53: 0.6888811500700375\n",
      "Cost at iteration 54: 0.6888121776477902\n",
      "Cost at iteration 55: 0.6887435768342347\n",
      "Cost at iteration 56: 0.6886753449309835\n",
      "Cost at iteration 57: 0.6886074792595018\n",
      "Cost at iteration 58: 0.6885399771609656\n",
      "Cost at iteration 59: 0.6884728359961187\n",
      "Cost at iteration 60: 0.6884060531451333\n",
      "Cost at iteration 61: 0.6883396260074698\n",
      "Cost at iteration 62: 0.6882735520017378\n",
      "Cost at iteration 63: 0.6882078285655591\n",
      "Cost at iteration 64: 0.6881424531554297\n",
      "Cost at iteration 65: 0.6880774232465844\n",
      "Cost at iteration 66: 0.6880127363328615\n",
      "Cost at iteration 67: 0.6879483899265689\n",
      "Cost at iteration 68: 0.6878843815583505\n",
      "Cost at iteration 69: 0.6878207087770529\n",
      "Cost at iteration 70: 0.6877573691495962\n",
      "Cost at iteration 71: 0.6876943602608409\n",
      "Cost at iteration 72: 0.6876316797134594\n",
      "Cost at iteration 73: 0.6875693251278067\n",
      "Cost at iteration 74: 0.6875072941417928\n",
      "Cost at iteration 75: 0.687445584410755\n",
      "Cost at iteration 76: 0.6873841936073318\n",
      "Cost at iteration 77: 0.6873231194213378\n",
      "Cost at iteration 78: 0.6872623595596382\n",
      "Cost at iteration 79: 0.6872019117460266\n",
      "Cost at iteration 80: 0.6871417737211001\n",
      "Cost at iteration 81: 0.6870819432421389\n",
      "Cost at iteration 82: 0.687022418082984\n",
      "Cost at iteration 83: 0.6869631960339176\n",
      "Cost at iteration 84: 0.6869042749015435\n",
      "Cost at iteration 85: 0.6868456525086668\n",
      "Cost at iteration 86: 0.6867873266941781\n",
      "Cost at iteration 87: 0.6867292953129351\n",
      "Cost at iteration 88: 0.6866715562356468\n",
      "Cost at iteration 89: 0.686614107348757\n",
      "Cost at iteration 90: 0.6865569465543313\n",
      "Cost at iteration 91: 0.6865000717699414\n",
      "Cost at iteration 92: 0.6864434809285531\n",
      "Cost at iteration 93: 0.6863871719784128\n",
      "Cost at iteration 94: 0.6863311428829377\n",
      "Cost at iteration 95: 0.6862753916206031\n",
      "Cost at iteration 96: 0.6862199161848335\n",
      "Cost at iteration 97: 0.6861647145838923\n",
      "Cost at iteration 98: 0.686109784840775\n",
      "Cost at iteration 99: 0.6860551249930995\n",
      "Cost at iteration 100: 0.6860007330930001\n",
      "Cost at iteration 101: 0.6859466072070213\n",
      "Cost at iteration 102: 0.6858927454160118\n",
      "Cost at iteration 103: 0.6858391458150198\n",
      "Cost at iteration 104: 0.685785806513189\n",
      "Cost at iteration 105: 0.6857327256336561\n",
      "Cost at iteration 106: 0.685679901313446\n",
      "Cost at iteration 107: 0.685627331703373\n",
      "Cost at iteration 108: 0.6855750149679362\n",
      "Cost at iteration 109: 0.6855229492852221\n",
      "Cost at iteration 110: 0.685471132846803\n",
      "Cost at iteration 111: 0.6854195638576386\n",
      "Cost at iteration 112: 0.6853682405359773\n",
      "Cost at iteration 113: 0.6853171611132589\n",
      "Cost at iteration 114: 0.6852663238340178\n",
      "Cost at iteration 115: 0.6852157269557859\n",
      "Cost at iteration 116: 0.6851653687489976\n",
      "Cost at iteration 117: 0.6851152474968951\n",
      "Cost at iteration 118: 0.6850653614954335\n",
      "Cost at iteration 119: 0.6850157090531878\n",
      "Cost at iteration 120: 0.6849662884912594\n",
      "Cost at iteration 121: 0.6849170981431846\n",
      "Cost at iteration 122: 0.6848681363548419\n",
      "Cost at iteration 123: 0.6848194014843625\n",
      "Cost at iteration 124: 0.6847708919020388\n",
      "Cost at iteration 125: 0.6847226059902347\n",
      "Cost at iteration 126: 0.6846745421432979\n",
      "Cost at iteration 127: 0.6846266987674702\n",
      "Cost at iteration 128: 0.6845790742808\n",
      "Cost at iteration 129: 0.6845316671130557\n",
      "Cost at iteration 130: 0.6844844757056385\n",
      "Cost at iteration 131: 0.6844374985114974\n",
      "Cost at iteration 132: 0.6843907339950429\n",
      "Cost at iteration 133: 0.6843441806320629\n",
      "Cost at iteration 134: 0.6842978369096389\n",
      "Cost at iteration 135: 0.6842517013260621\n",
      "Cost at iteration 136: 0.6842057723907503\n",
      "Cost at iteration 137: 0.6841600486241668\n",
      "Cost at iteration 138: 0.6841145285577375\n",
      "Cost at iteration 139: 0.6840692107337702\n",
      "Cost at iteration 140: 0.6840240937053746\n",
      "Cost at iteration 141: 0.6839791760363821\n",
      "Cost at iteration 142: 0.6839344563012658\n",
      "Cost at iteration 143: 0.6838899330850629\n",
      "Cost at iteration 144: 0.6838456049832957\n",
      "Cost at iteration 145: 0.6838014706018941\n",
      "Cost at iteration 146: 0.6837575285571191\n",
      "Cost at iteration 147: 0.6837137774754851\n",
      "Cost at iteration 148: 0.6836702159936858\n",
      "Cost at iteration 149: 0.6836268427585162\n",
      "Cost at iteration 150: 0.6835836564268005\n",
      "Cost at iteration 151: 0.6835406556653164\n",
      "Cost at iteration 152: 0.6834978391507205\n",
      "Cost at iteration 153: 0.6834552055694769\n",
      "Cost at iteration 154: 0.6834127536177834\n",
      "Cost at iteration 155: 0.6833704820014991\n",
      "Cost at iteration 156: 0.683328389436074\n",
      "Cost at iteration 157: 0.6832864746464758\n",
      "Cost at iteration 158: 0.6832447363671221\n",
      "Cost at iteration 159: 0.6832031733418071\n",
      "Cost at iteration 160: 0.6831617843236346\n",
      "Cost at iteration 161: 0.6831205680749478\n",
      "Cost at iteration 162: 0.6830795233672607\n",
      "Cost at iteration 163: 0.6830386489811899\n",
      "Cost at iteration 164: 0.682997943706388\n",
      "Cost at iteration 165: 0.6829574063414752\n",
      "Cost at iteration 166: 0.6829170356939733\n",
      "Cost at iteration 167: 0.6828768305802402\n",
      "Cost at iteration 168: 0.6828367898254029\n",
      "Cost at iteration 169: 0.6827969122632932\n",
      "Cost at iteration 170: 0.6827571967363839\n",
      "Cost at iteration 171: 0.6827176420957223\n",
      "Cost at iteration 172: 0.6826782472008683\n",
      "Cost at iteration 173: 0.6826390109198313\n",
      "Cost at iteration 174: 0.6825999321290055\n",
      "Cost at iteration 175: 0.6825610097131097\n",
      "Cost at iteration 176: 0.6825222425651245\n",
      "Cost at iteration 177: 0.6824836295862304\n",
      "Cost at iteration 178: 0.6824451696857476\n",
      "Cost at iteration 179: 0.6824068617810753\n",
      "Cost at iteration 180: 0.6823687047976312\n",
      "Cost at iteration 181: 0.6823306976687925\n",
      "Cost at iteration 182: 0.682292839335836\n",
      "Cost at iteration 183: 0.6822551287478797\n",
      "Cost at iteration 184: 0.6822175648618249\n",
      "Cost at iteration 185: 0.6821801466422976\n",
      "Cost at iteration 186: 0.6821428730615908\n",
      "Cost at iteration 187: 0.6821057430996088\n",
      "Cost at iteration 188: 0.6820687557438088\n",
      "Cost at iteration 189: 0.682031909989146\n",
      "Cost at iteration 190: 0.6819952048380173\n",
      "Cost at iteration 191: 0.681958639300206\n",
      "Cost at iteration 192: 0.6819222123928265\n",
      "Cost at iteration 193: 0.6818859231402702\n",
      "Cost at iteration 194: 0.6818497705741512\n",
      "Cost at iteration 195: 0.681813753733252\n",
      "Cost at iteration 196: 0.6817778716634713\n",
      "Cost at iteration 197: 0.6817421234177695\n",
      "Cost at iteration 198: 0.6817065080561172\n",
      "Cost at iteration 199: 0.6816710246454427\n",
      "Cost at iteration 200: 0.6816356722595802\n",
      "Cost at iteration 201: 0.6816004499792182\n",
      "Cost at iteration 202: 0.6815653568918485\n",
      "Cost at iteration 203: 0.6815303920917157\n",
      "Cost at iteration 204: 0.6814955546797669\n",
      "Cost at iteration 205: 0.6814608437636013\n",
      "Cost at iteration 206: 0.6814262584574213\n",
      "Cost at iteration 207: 0.6813917978819829\n",
      "Cost at iteration 208: 0.6813574611645471\n",
      "Cost at iteration 209: 0.6813232474388311\n",
      "Cost at iteration 210: 0.6812891558449605\n",
      "Cost at iteration 211: 0.6812551855294214\n",
      "Cost at iteration 212: 0.6812213356450133\n",
      "Cost at iteration 213: 0.6811876053508013\n",
      "Cost at iteration 214: 0.6811539938120706\n",
      "Cost at iteration 215: 0.6811205002002789\n",
      "Cost at iteration 216: 0.6810871236930112\n",
      "Cost at iteration 217: 0.681053863473934\n",
      "Cost at iteration 218: 0.6810207187327495\n",
      "Cost at iteration 219: 0.6809876886651515\n",
      "Cost at iteration 220: 0.6809547724727799\n",
      "Cost at iteration 221: 0.6809219693631768\n",
      "Cost at iteration 222: 0.6808892785497425\n",
      "Cost at iteration 223: 0.6808566992516916\n",
      "Cost at iteration 224: 0.6808242306940101\n",
      "Cost at iteration 225: 0.6807918721074119\n",
      "Cost at iteration 226: 0.6807596227282962\n",
      "Cost at iteration 227: 0.6807274817987055\n",
      "Cost at iteration 228: 0.680695448566283\n",
      "Cost at iteration 229: 0.6806635222842314\n",
      "Cost at iteration 230: 0.6806317022112711\n",
      "Cost at iteration 231: 0.6805999876115991\n",
      "Cost at iteration 232: 0.6805683777548479\n",
      "Cost at iteration 233: 0.6805368719160457\n",
      "Cost at iteration 234: 0.6805054693755763\n",
      "Cost at iteration 235: 0.6804741694191369\n",
      "Cost at iteration 236: 0.6804429713377024\n",
      "Cost at iteration 237: 0.6804118744274823\n",
      "Cost at iteration 238: 0.6803808779898839\n",
      "Cost at iteration 239: 0.680349981331473\n",
      "Cost at iteration 240: 0.6803191837639354\n",
      "Cost at iteration 241: 0.6802884846040383\n",
      "Cost at iteration 242: 0.6802578831735933\n",
      "Cost at iteration 243: 0.680227378799418\n",
      "Cost at iteration 244: 0.6801969708132991\n",
      "Cost at iteration 245: 0.6801666585519555\n",
      "Cost at iteration 246: 0.6801364413570008\n",
      "Cost at iteration 247: 0.6801063185749079\n",
      "Cost at iteration 248: 0.6800762895569719\n",
      "Cost at iteration 249: 0.6800463536592742\n",
      "Cost at iteration 250: 0.6800165102426475\n",
      "Cost at iteration 251: 0.6799867586726398\n",
      "Cost at iteration 252: 0.6799570983194791\n",
      "Cost at iteration 253: 0.6799275285580396\n",
      "Cost at iteration 254: 0.6798980487678055\n",
      "Cost at iteration 255: 0.6798686583328375\n",
      "Cost at iteration 256: 0.6798393566417391\n",
      "Cost at iteration 257: 0.6798101430876221\n",
      "Cost at iteration 258: 0.6797810170680729\n",
      "Cost at iteration 259: 0.6797519779851195\n",
      "Cost at iteration 260: 0.6797230252451986\n",
      "Cost at iteration 261: 0.6796941582591219\n",
      "Cost at iteration 262: 0.6796653764420446\n",
      "Cost at iteration 263: 0.6796366792134324\n",
      "Cost at iteration 264: 0.6796080659970289\n",
      "Cost at iteration 265: 0.679579536220825\n",
      "Cost at iteration 266: 0.6795510893170261\n",
      "Cost at iteration 267: 0.6795227247220214\n",
      "Cost at iteration 268: 0.6794944418763519\n",
      "Cost at iteration 269: 0.6794662402246807\n",
      "Cost at iteration 270: 0.679438119215761\n",
      "Cost at iteration 271: 0.6794100783024071\n",
      "Cost at iteration 272: 0.6793821169414621\n",
      "Cost at iteration 273: 0.6793542345937698\n",
      "Cost at iteration 274: 0.6793264307241441\n",
      "Cost at iteration 275: 0.6792987048013392\n",
      "Cost at iteration 276: 0.6792710562980206\n",
      "Cost at iteration 277: 0.6792434846907364\n",
      "Cost at iteration 278: 0.6792159894598868\n",
      "Cost at iteration 279: 0.6791885700896972\n",
      "Cost at iteration 280: 0.6791612260681892\n",
      "Cost at iteration 281: 0.6791339568871515\n",
      "Cost at iteration 282: 0.6791067620421125\n",
      "Cost at iteration 283: 0.6790796410323129\n",
      "Cost at iteration 284: 0.679052593360677\n",
      "Cost at iteration 285: 0.6790256185337862\n",
      "Cost at iteration 286: 0.6789987160618509\n",
      "Cost at iteration 287: 0.6789718854586846\n",
      "Cost at iteration 288: 0.678945126241676\n",
      "Cost at iteration 289: 0.6789184379317627\n",
      "Cost at iteration 290: 0.6788918200534052\n",
      "Cost at iteration 291: 0.6788652721345603\n",
      "Cost at iteration 292: 0.6788387937066549\n",
      "Cost at iteration 293: 0.6788123843045606\n",
      "Cost at iteration 294: 0.6787860434665679\n",
      "Cost at iteration 295: 0.6787597707343604\n",
      "Cost at iteration 296: 0.67873356565299\n",
      "Cost at iteration 297: 0.678707427770852\n",
      "Cost at iteration 298: 0.6786813566396596\n",
      "Cost at iteration 299: 0.6786553518144188\n",
      "Cost at iteration 300: 0.678629412853406\n",
      "Cost at iteration 301: 0.6786035393181411\n",
      "Cost at iteration 302: 0.6785777307733653\n",
      "Cost at iteration 303: 0.6785519867870156\n",
      "Cost at iteration 304: 0.6785263069302023\n",
      "Cost at iteration 305: 0.6785006907771848\n",
      "Cost at iteration 306: 0.678475137905348\n",
      "Cost at iteration 307: 0.6784496478951794\n",
      "Cost at iteration 308: 0.6784242203302459\n",
      "Cost at iteration 309: 0.678398854797171\n",
      "Cost at iteration 310: 0.6783735508856117\n",
      "Cost at iteration 311: 0.6783483081882364\n",
      "Cost at iteration 312: 0.6783231263007021\n",
      "Cost at iteration 313: 0.6782980048216327\n",
      "Cost at iteration 314: 0.6782729433525962\n",
      "Cost at iteration 315: 0.6782479414980835\n",
      "Cost at iteration 316: 0.6782229988654864\n",
      "Cost at iteration 317: 0.6781981150650755\n",
      "Cost at iteration 318: 0.6781732897099801\n",
      "Cost at iteration 319: 0.6781485224161653\n",
      "Cost at iteration 320: 0.6781238128024126\n",
      "Cost at iteration 321: 0.6780991604902975\n",
      "Cost at iteration 322: 0.6780745651041696\n",
      "Cost at iteration 323: 0.6780500262711318\n",
      "Cost at iteration 324: 0.6780255436210197\n",
      "Cost at iteration 325: 0.6780011167863814\n",
      "Cost at iteration 326: 0.6779767454024572\n",
      "Cost at iteration 327: 0.6779524291071598\n",
      "Cost at iteration 328: 0.6779281675410544\n",
      "Cost at iteration 329: 0.6779039603473389\n",
      "Cost at iteration 330: 0.6778798071718244\n",
      "Cost at iteration 331: 0.6778557076629156\n",
      "Cost at iteration 332: 0.6778316614715918\n",
      "Cost at iteration 333: 0.6778076682513878\n",
      "Cost at iteration 334: 0.6777837276583748\n",
      "Cost at iteration 335: 0.6777598393511411\n",
      "Cost at iteration 336: 0.6777360029907745\n",
      "Cost at iteration 337: 0.6777122182408428\n",
      "Cost at iteration 338: 0.6776884847673755\n",
      "Cost at iteration 339: 0.6776648022388461\n",
      "Cost at iteration 340: 0.677641170326153\n",
      "Cost at iteration 341: 0.6776175887026027\n",
      "Cost at iteration 342: 0.6775940570438905\n",
      "Cost at iteration 343: 0.6775705750280846\n",
      "Cost at iteration 344: 0.6775471423356065\n",
      "Cost at iteration 345: 0.6775237586492147\n",
      "Cost at iteration 346: 0.6775004236539872\n",
      "Cost at iteration 347: 0.6774771370373043\n",
      "Cost at iteration 348: 0.677453898488831\n",
      "Cost at iteration 349: 0.6774307077005011\n",
      "Cost at iteration 350: 0.6774075643664992\n",
      "Cost at iteration 351: 0.6773844681832446\n",
      "Cost at iteration 352: 0.6773614188493748\n",
      "Cost at iteration 353: 0.6773384160657288\n",
      "Cost at iteration 354: 0.6773154595353309\n",
      "Cost at iteration 355: 0.6772925489633747\n",
      "Cost at iteration 356: 0.6772696840572058\n",
      "Cost at iteration 357: 0.6772468645263079\n",
      "Cost at iteration 358: 0.6772240900822853\n",
      "Cost at iteration 359: 0.6772013604388479\n",
      "Cost at iteration 360: 0.6771786753117953\n",
      "Cost at iteration 361: 0.6771560344190013\n",
      "Cost at iteration 362: 0.677133437480399\n",
      "Cost at iteration 363: 0.6771108842179647\n",
      "Cost at iteration 364: 0.6770883743557035\n",
      "Cost at iteration 365: 0.6770659076196338\n",
      "Cost at iteration 366: 0.6770434837377728\n",
      "Cost at iteration 367: 0.6770211024401214\n",
      "Cost at iteration 368: 0.6769987634586491\n",
      "Cost at iteration 369: 0.6769764665272804\n",
      "Cost at iteration 370: 0.6769542113818798\n",
      "Cost at iteration 371: 0.6769319977602366\n",
      "Cost at iteration 372: 0.6769098254020527\n",
      "Cost at iteration 373: 0.6768876940489257\n",
      "Cost at iteration 374: 0.6768656034443375\n",
      "Cost at iteration 375: 0.6768435533336388\n",
      "Cost at iteration 376: 0.6768215434640353\n",
      "Cost at iteration 377: 0.6767995735845745\n",
      "Cost at iteration 378: 0.6767776434461322\n",
      "Cost at iteration 379: 0.6767557528013981\n",
      "Cost at iteration 380: 0.6767339014048631\n",
      "Cost at iteration 381: 0.6767120890128056\n",
      "Cost at iteration 382: 0.6766903153832786\n",
      "Cost at iteration 383: 0.6766685802760961\n",
      "Cost at iteration 384: 0.6766468834528209\n",
      "Cost at iteration 385: 0.6766252246767502\n",
      "Cost at iteration 386: 0.6766036037129046\n",
      "Cost at iteration 387: 0.6765820203280134\n",
      "Cost at iteration 388: 0.676560474290504\n",
      "Cost at iteration 389: 0.6765389653704877\n",
      "Cost at iteration 390: 0.6765174933397478\n",
      "Cost at iteration 391: 0.6764960579717272\n",
      "Cost at iteration 392: 0.6764746590415166\n",
      "Cost at iteration 393: 0.6764532963258415\n",
      "Cost at iteration 394: 0.6764319696030507\n",
      "Cost at iteration 395: 0.6764106786531042\n",
      "Cost at iteration 396: 0.6763894232575608\n",
      "Cost at iteration 397: 0.676368203199567\n",
      "Cost at iteration 398: 0.6763470182638445\n",
      "Cost at iteration 399: 0.6763258682366797\n",
      "Cost at iteration 400: 0.6763047529059104\n",
      "Cost at iteration 401: 0.6762836720609164\n",
      "Cost at iteration 402: 0.6762626254926063\n",
      "Cost at iteration 403: 0.6762416129934071\n",
      "Cost at iteration 404: 0.6762206343572528\n",
      "Cost at iteration 405: 0.6761996893795734\n",
      "Cost at iteration 406: 0.6761787778572836\n",
      "Cost at iteration 407: 0.6761578995887719\n",
      "Cost at iteration 408: 0.67613705437389\n",
      "Cost at iteration 409: 0.6761162420139414\n",
      "Cost at iteration 410: 0.6760954623116713\n",
      "Cost at iteration 411: 0.6760747150712553\n",
      "Cost at iteration 412: 0.6760540000982899\n",
      "Cost at iteration 413: 0.6760333171997802\n",
      "Cost at iteration 414: 0.6760126661841319\n",
      "Cost at iteration 415: 0.6759920468611385\n",
      "Cost at iteration 416: 0.6759714590419731\n",
      "Cost at iteration 417: 0.6759509025391768\n",
      "Cost at iteration 418: 0.6759303771666492\n",
      "Cost at iteration 419: 0.6759098827396384\n",
      "Cost at iteration 420: 0.675889419074731\n",
      "Cost at iteration 421: 0.6758689859898417\n",
      "Cost at iteration 422: 0.6758485833042047\n",
      "Cost at iteration 423: 0.6758282108383626\n",
      "Cost at iteration 424: 0.6758078684141577\n",
      "Cost at iteration 425: 0.6757875558547222\n",
      "Cost at iteration 426: 0.6757672729844679\n",
      "Cost at iteration 427: 0.6757470196290779\n",
      "Cost at iteration 428: 0.6757267956154968\n",
      "Cost at iteration 429: 0.6757066007719211\n",
      "Cost at iteration 430: 0.6756864349277903\n",
      "Cost at iteration 431: 0.6756662979137774\n",
      "Cost at iteration 432: 0.6756461895617805\n",
      "Cost at iteration 433: 0.675626109704913\n",
      "Cost at iteration 434: 0.6756060581774945\n",
      "Cost at iteration 435: 0.6755860348150433\n",
      "Cost at iteration 436: 0.675566039454266\n",
      "Cost at iteration 437: 0.675546071933049\n",
      "Cost at iteration 438: 0.675526132090451\n",
      "Cost at iteration 439: 0.6755062197666926\n",
      "Cost at iteration 440: 0.6754863348031495\n",
      "Cost at iteration 441: 0.6754664770423422\n",
      "Cost at iteration 442: 0.6754466463279294\n",
      "Cost at iteration 443: 0.675426842504698\n",
      "Cost at iteration 444: 0.6754070654185559\n",
      "Cost at iteration 445: 0.675387314916523\n",
      "Cost at iteration 446: 0.6753675908467237\n",
      "Cost at iteration 447: 0.6753478930583781\n",
      "Cost at iteration 448: 0.6753282214017942\n",
      "Cost at iteration 449: 0.67530857572836\n",
      "Cost at iteration 450: 0.6752889558905354\n",
      "Cost at iteration 451: 0.6752693617418448\n",
      "Cost at iteration 452: 0.6752497931368682\n",
      "Cost at iteration 453: 0.6752302499312341\n",
      "Cost at iteration 454: 0.6752107319816121\n",
      "Cost at iteration 455: 0.6751912391457048\n",
      "Cost at iteration 456: 0.6751717712822398\n",
      "Cost at iteration 457: 0.6751523282509634\n",
      "Cost at iteration 458: 0.6751329099126314\n",
      "Cost at iteration 459: 0.6751135161290032\n",
      "Cost at iteration 460: 0.6750941467628339\n",
      "Cost at iteration 461: 0.6750748016778662\n",
      "Cost at iteration 462: 0.6750554807388244\n",
      "Cost at iteration 463: 0.6750361838114065\n",
      "Cost at iteration 464: 0.6750169107622769\n",
      "Cost at iteration 465: 0.6749976614590594\n",
      "Cost at iteration 466: 0.6749784357703309\n",
      "Cost at iteration 467: 0.6749592335656132\n",
      "Cost at iteration 468: 0.6749400547153664\n",
      "Cost at iteration 469: 0.6749208990909827\n",
      "Cost at iteration 470: 0.6749017665647791\n",
      "Cost at iteration 471: 0.6748826570099897\n",
      "Cost at iteration 472: 0.6748635703007608\n",
      "Cost at iteration 473: 0.6748445063121423\n",
      "Cost at iteration 474: 0.6748254649200828\n",
      "Cost at iteration 475: 0.6748064460014218\n",
      "Cost at iteration 476: 0.6747874494338826\n",
      "Cost at iteration 477: 0.6747684750960682\n",
      "Cost at iteration 478: 0.6747495228674526\n",
      "Cost at iteration 479: 0.674730592628375\n",
      "Cost at iteration 480: 0.6747116842600334\n",
      "Cost at iteration 481: 0.6746927976444789\n",
      "Cost at iteration 482: 0.6746739326646092\n",
      "Cost at iteration 483: 0.6746550892041614\n",
      "Cost at iteration 484: 0.6746362671477075\n",
      "Cost at iteration 485: 0.6746174663806463\n",
      "Cost at iteration 486: 0.6745986867891995\n",
      "Cost at iteration 487: 0.6745799282604037\n",
      "Cost at iteration 488: 0.6745611906821058\n",
      "Cost at iteration 489: 0.6745424739429562\n",
      "Cost at iteration 490: 0.6745237779324035\n",
      "Cost at iteration 491: 0.6745051025406879\n",
      "Cost at iteration 492: 0.6744864476588365\n",
      "Cost at iteration 493: 0.6744678131786565\n",
      "Cost at iteration 494: 0.6744491989927295\n",
      "Cost at iteration 495: 0.6744306049944069\n",
      "Cost at iteration 496: 0.6744120310778029\n",
      "Cost at iteration 497: 0.6743934771377901\n",
      "Cost at iteration 498: 0.6743749430699926\n",
      "Cost at iteration 499: 0.674356428770782\n",
      "Cost at iteration 500: 0.6743379341372705\n",
      "Cost at iteration 501: 0.6743194590673063\n",
      "Cost at iteration 502: 0.6743010034594685\n",
      "Cost at iteration 503: 0.6742825672130605\n",
      "Cost at iteration 504: 0.674264150228106\n",
      "Cost at iteration 505: 0.6742457524053429\n",
      "Cost at iteration 506: 0.6742273736462185\n",
      "Cost at iteration 507: 0.6742090138528839\n",
      "Cost at iteration 508: 0.674190672928189\n",
      "Cost at iteration 509: 0.6741723507756779\n",
      "Cost at iteration 510: 0.6741540472995827\n",
      "Cost at iteration 511: 0.6741357624048199\n",
      "Cost at iteration 512: 0.6741174959969838\n",
      "Cost at iteration 513: 0.6740992479823423\n",
      "Cost at iteration 514: 0.6740810182678331\n",
      "Cost at iteration 515: 0.6740628067610563\n",
      "Cost at iteration 516: 0.6740446133702718\n",
      "Cost at iteration 517: 0.6740264380043931\n",
      "Cost at iteration 518: 0.6740082805729831\n",
      "Cost at iteration 519: 0.6739901409862494\n",
      "Cost at iteration 520: 0.6739720191550393\n",
      "Cost at iteration 521: 0.6739539149908353\n",
      "Cost at iteration 522: 0.6739358284057504\n",
      "Cost at iteration 523: 0.6739177593125232\n",
      "Cost at iteration 524: 0.6738997076245136\n",
      "Cost at iteration 525: 0.6738816732556984\n",
      "Cost at iteration 526: 0.6738636561206666\n",
      "Cost at iteration 527: 0.6738456561346151\n",
      "Cost at iteration 528: 0.6738276732133434\n",
      "Cost at iteration 529: 0.6738097072732506\n",
      "Cost at iteration 530: 0.6737917582313299\n",
      "Cost at iteration 531: 0.6737738260051644\n",
      "Cost at iteration 532: 0.6737559105129238\n",
      "Cost at iteration 533: 0.6737380116733585\n",
      "Cost at iteration 534: 0.6737201294057962\n",
      "Cost at iteration 535: 0.6737022636301383\n",
      "Cost at iteration 536: 0.6736844142668544\n",
      "Cost at iteration 537: 0.673666581236979\n",
      "Cost at iteration 538: 0.6736487644621072\n",
      "Cost at iteration 539: 0.6736309638643904\n",
      "Cost at iteration 540: 0.6736131793665324\n",
      "Cost at iteration 541: 0.6735954108917855\n",
      "Cost at iteration 542: 0.673577658363946\n",
      "Cost at iteration 543: 0.6735599217073509\n",
      "Cost at iteration 544: 0.6735422008468736\n",
      "Cost at iteration 545: 0.6735244957079194\n",
      "Cost at iteration 546: 0.6735068062164229\n",
      "Cost at iteration 547: 0.6734891322988427\n",
      "Cost at iteration 548: 0.673471473882159\n",
      "Cost at iteration 549: 0.6734538308938685\n",
      "Cost at iteration 550: 0.6734362032619814\n",
      "Cost at iteration 551: 0.6734185909150174\n",
      "Cost at iteration 552: 0.6734009937820022\n",
      "Cost at iteration 553: 0.6733834117924633\n",
      "Cost at iteration 554: 0.6733658448764268\n",
      "Cost at iteration 555: 0.6733482929644135\n",
      "Cost at iteration 556: 0.6733307559874355\n",
      "Cost at iteration 557: 0.6733132338769927\n",
      "Cost at iteration 558: 0.6732957265650683\n",
      "Cost at iteration 559: 0.6732782339841269\n",
      "Cost at iteration 560: 0.6732607560671093\n",
      "Cost at iteration 561: 0.6732432927474307\n",
      "Cost at iteration 562: 0.6732258439589753\n",
      "Cost at iteration 563: 0.6732084096360945\n",
      "Cost at iteration 564: 0.673190989713603\n",
      "Cost at iteration 565: 0.6731735841267752\n",
      "Cost at iteration 566: 0.6731561928113416\n",
      "Cost at iteration 567: 0.6731388157034863\n",
      "Cost at iteration 568: 0.6731214527398431\n",
      "Cost at iteration 569: 0.6731041038574922\n",
      "Cost at iteration 570: 0.6730867689939574\n",
      "Cost at iteration 571: 0.673069448087202\n",
      "Cost at iteration 572: 0.6730521410756266\n",
      "Cost at iteration 573: 0.6730348478980654\n",
      "Cost at iteration 574: 0.6730175684937828\n",
      "Cost at iteration 575: 0.6730003028024707\n",
      "Cost at iteration 576: 0.6729830507642454\n",
      "Cost at iteration 577: 0.6729658123196439\n",
      "Cost at iteration 578: 0.6729485874096217\n",
      "Cost at iteration 579: 0.6729313759755491\n",
      "Cost at iteration 580: 0.6729141779592083\n",
      "Cost at iteration 581: 0.6728969933027904\n",
      "Cost at iteration 582: 0.6728798219488928\n",
      "Cost at iteration 583: 0.6728626638405157\n",
      "Cost at iteration 584: 0.6728455189210597\n",
      "Cost at iteration 585: 0.6728283871343221\n",
      "Cost at iteration 586: 0.6728112684244946\n",
      "Cost at iteration 587: 0.6727941627361606\n",
      "Cost at iteration 588: 0.6727770700142914\n",
      "Cost at iteration 589: 0.6727599902042452\n",
      "Cost at iteration 590: 0.6727429232517614\n",
      "Cost at iteration 591: 0.6727258691029613\n",
      "Cost at iteration 592: 0.672708827704342\n",
      "Cost at iteration 593: 0.6726917990027765\n",
      "Cost at iteration 594: 0.6726747829455086\n",
      "Cost at iteration 595: 0.6726577794801519\n",
      "Cost at iteration 596: 0.6726407885546863\n",
      "Cost at iteration 597: 0.6726238101174553\n",
      "Cost at iteration 598: 0.6726068441171634\n",
      "Cost at iteration 599: 0.6725898905028744\n",
      "Cost at iteration 600: 0.6725729492240075\n",
      "Cost at iteration 601: 0.6725560202303347\n",
      "Cost at iteration 602: 0.6725391034719795\n",
      "Cost at iteration 603: 0.6725221988994133\n",
      "Cost at iteration 604: 0.6725053064634532\n",
      "Cost at iteration 605: 0.6724884261152592\n",
      "Cost at iteration 606: 0.6724715578063322\n",
      "Cost at iteration 607: 0.6724547014885113\n",
      "Cost at iteration 608: 0.6724378571139709\n",
      "Cost at iteration 609: 0.6724210246352186\n",
      "Cost at iteration 610: 0.6724042040050935\n",
      "Cost at iteration 611: 0.6723873951767626\n",
      "Cost at iteration 612: 0.6723705981037189\n",
      "Cost at iteration 613: 0.6723538127397792\n",
      "Cost at iteration 614: 0.6723370390390815\n",
      "Cost at iteration 615: 0.6723202769560828\n",
      "Cost at iteration 616: 0.6723035264455567\n",
      "Cost at iteration 617: 0.6722867874625911\n",
      "Cost at iteration 618: 0.672270059962586\n",
      "Cost at iteration 619: 0.672253343901251\n",
      "Cost at iteration 620: 0.6722366392346036\n",
      "Cost at iteration 621: 0.6722199459189663\n",
      "Cost at iteration 622: 0.6722032639109645\n",
      "Cost at iteration 623: 0.6721865931675248\n",
      "Cost at iteration 624: 0.6721699336458722\n",
      "Cost at iteration 625: 0.6721532853035285\n",
      "Cost at iteration 626: 0.6721366480983092\n",
      "Cost at iteration 627: 0.6721200219883229\n",
      "Cost at iteration 628: 0.6721034069319676\n",
      "Cost at iteration 629: 0.6720868028879297\n",
      "Cost at iteration 630: 0.6720702098151808\n",
      "Cost at iteration 631: 0.672053627672977\n",
      "Cost at iteration 632: 0.6720370564208559\n",
      "Cost at iteration 633: 0.6720204960186349\n",
      "Cost at iteration 634: 0.6720039464264085\n",
      "Cost at iteration 635: 0.6719874076045476\n",
      "Cost at iteration 636: 0.6719708795136959\n",
      "Cost at iteration 637: 0.6719543621147697\n",
      "Cost at iteration 638: 0.6719378553689543\n",
      "Cost at iteration 639: 0.6719213592377028\n",
      "Cost at iteration 640: 0.6719048736827341\n",
      "Cost at iteration 641: 0.671888398666031\n",
      "Cost at iteration 642: 0.6718719341498383\n",
      "Cost at iteration 643: 0.6718554800966605\n",
      "Cost at iteration 644: 0.6718390364692607\n",
      "Cost at iteration 645: 0.6718226032306577\n",
      "Cost at iteration 646: 0.6718061803441252\n",
      "Cost at iteration 647: 0.671789767773189\n",
      "Cost at iteration 648: 0.6717733654816259\n",
      "Cost at iteration 649: 0.6717569734334616\n",
      "Cost at iteration 650: 0.6717405915929686\n",
      "Cost at iteration 651: 0.671724219924665\n",
      "Cost at iteration 652: 0.6717078583933123\n",
      "Cost at iteration 653: 0.6716915069639136\n",
      "Cost at iteration 654: 0.6716751656017123\n",
      "Cost at iteration 655: 0.6716588342721896\n",
      "Cost at iteration 656: 0.6716425129410637\n",
      "Cost at iteration 657: 0.671626201574287\n",
      "Cost at iteration 658: 0.671609900138046\n",
      "Cost at iteration 659: 0.6715936085987574\n",
      "Cost at iteration 660: 0.6715773269230683\n",
      "Cost at iteration 661: 0.6715610550778539\n",
      "Cost at iteration 662: 0.6715447930302155\n",
      "Cost at iteration 663: 0.6715285407474796\n",
      "Cost at iteration 664: 0.6715122981971949\n",
      "Cost at iteration 665: 0.671496065347133\n",
      "Cost at iteration 666: 0.6714798421652843\n",
      "Cost at iteration 667: 0.671463628619858\n",
      "Cost at iteration 668: 0.6714474246792798\n",
      "Cost at iteration 669: 0.6714312303121904\n",
      "Cost at iteration 670: 0.6714150454874448\n",
      "Cost at iteration 671: 0.6713988701741096\n",
      "Cost at iteration 672: 0.6713827043414616\n",
      "Cost at iteration 673: 0.6713665479589874\n",
      "Cost at iteration 674: 0.6713504009963799\n",
      "Cost at iteration 675: 0.671334263423539\n",
      "Cost at iteration 676: 0.6713181352105689\n",
      "Cost at iteration 677: 0.6713020163277765\n",
      "Cost at iteration 678: 0.6712859067456702\n",
      "Cost at iteration 679: 0.6712698064349587\n",
      "Cost at iteration 680: 0.6712537153665492\n",
      "Cost at iteration 681: 0.671237633511546\n",
      "Cost at iteration 682: 0.6712215608412494\n",
      "Cost at iteration 683: 0.6712054973271537\n",
      "Cost at iteration 684: 0.6711894429409464\n",
      "Cost at iteration 685: 0.6711733976545062\n",
      "Cost at iteration 686: 0.6711573614399023\n",
      "Cost at iteration 687: 0.6711413342693926\n",
      "Cost at iteration 688: 0.6711253161154219\n",
      "Cost at iteration 689: 0.671109306950622\n",
      "Cost at iteration 690: 0.6710933067478084\n",
      "Cost at iteration 691: 0.6710773154799803\n",
      "Cost at iteration 692: 0.671061333120319\n",
      "Cost at iteration 693: 0.6710453596421867\n",
      "Cost at iteration 694: 0.6710293950191247\n",
      "Cost at iteration 695: 0.6710134392248523\n",
      "Cost at iteration 696: 0.6709974922332662\n",
      "Cost at iteration 697: 0.6709815540184376\n",
      "Cost at iteration 698: 0.6709656245546132\n",
      "Cost at iteration 699: 0.6709497038162117\n",
      "The cost after training is 0.67094970.\n",
      "The resulting vector of weights is [4.1e-07, 0.00035658, 7.309e-05]\n"
     ]
    }
   ],
   "source": [
    "# Check the function\n",
    "# Construct a synthetic test case using numpy PRNG functions\n",
    "np.random.seed(1)\n",
    "# X input is 10 x 3 with ones for the bias terms\n",
    "tmp_X = np.append(np.ones((10, 1)), np.random.rand(10, 2) * 2000, axis=1)\n",
    "# Y Labels are 10 x 1\n",
    "tmp_Y = (np.random.rand(10, 1) > 0.35).astype(float)\n",
    "\n",
    "# Apply gradient descent\n",
    "tmp_J, tmp_theta = gradientDescent(tmp_X, tmp_Y, np.zeros((3, 1)), 1e-8, 700)\n",
    "print(f\"The cost after training is {tmp_J:.8f}.\")\n",
    "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(tmp_theta)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected output\n",
    "```\n",
    "The cost after training is 0.67094970.\n",
    "The resulting vector of weights is [4.1e-07, 0.00035658, 7.309e-05]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at iteration 0: 0.6931471805599454\n",
      "Cost at iteration 1: 0.6930540741991722\n",
      "Cost at iteration 2: 0.6929615149726678\n",
      "Cost at iteration 3: 0.692869498899696\n",
      "Cost at iteration 4: 0.692778022028382\n",
      "Cost at iteration 5: 0.6926870804355144\n",
      "Cost at iteration 6: 0.6925966702263475\n",
      "Cost at iteration 7: 0.6925067875344069\n",
      "Cost at iteration 8: 0.6924174285212924\n",
      "Cost at iteration 9: 0.6923285893764867\n",
      "Cost at iteration 10: 0.6922402663171613\n",
      "Cost at iteration 11: 0.6921524555879862\n",
      "Cost at iteration 12: 0.6920651534609382\n",
      "Cost at iteration 13: 0.6919783562351134\n",
      "Cost at iteration 14: 0.6918920602365377\n",
      "Cost at iteration 15: 0.6918062618179815\n",
      "Cost at iteration 16: 0.6917209573587718\n",
      "Cost at iteration 17: 0.6916361432646098\n",
      "Cost at iteration 18: 0.6915518159673852\n",
      "Cost at iteration 19: 0.6914679719249958\n",
      "Cost at iteration 20: 0.6913846076211638\n",
      "Cost at iteration 21: 0.6913017195652579\n",
      "Cost at iteration 22: 0.691219304292112\n",
      "Cost at iteration 23: 0.6911373583618485\n",
      "Cost at iteration 24: 0.691055878359701\n",
      "Cost at iteration 25: 0.6909748608958375\n",
      "Cost at iteration 26: 0.6908943026051868\n",
      "Cost at iteration 27: 0.6908142001472641\n",
      "Cost at iteration 28: 0.6907345502059982\n",
      "Cost at iteration 29: 0.6906553494895598\n",
      "Cost at iteration 30: 0.6905765947301911\n",
      "Cost at iteration 31: 0.6904982826840368\n",
      "Cost at iteration 32: 0.6904204101309741\n",
      "Cost at iteration 33: 0.6903429738744471\n",
      "Cost at iteration 34: 0.6902659707412996\n",
      "Cost at iteration 35: 0.6901893975816091\n",
      "Cost at iteration 36: 0.6901132512685239\n",
      "Cost at iteration 37: 0.6900375286980986\n",
      "Cost at iteration 38: 0.6899622267891328\n",
      "Cost at iteration 39: 0.689887342483009\n",
      "Cost at iteration 40: 0.6898128727435336\n",
      "Cost at iteration 41: 0.6897388145567769\n",
      "Cost at iteration 42: 0.6896651649309147\n",
      "Cost at iteration 43: 0.689591920896072\n",
      "Cost at iteration 44: 0.6895190795041666\n",
      "Cost at iteration 45: 0.6894466378287538\n",
      "Cost at iteration 46: 0.6893745929648722\n",
      "Cost at iteration 47: 0.689302942028891\n",
      "Cost at iteration 48: 0.689231682158357\n",
      "Cost at iteration 49: 0.6891608105118449\n",
      "Cost at iteration 50: 0.6890903242688053\n",
      "Cost at iteration 51: 0.6890202206294176\n",
      "Cost at iteration 52: 0.6889504968144393\n",
      "Cost at iteration 53: 0.6888811500650611\n",
      "Cost at iteration 54: 0.6888121776427591\n",
      "Cost at iteration 55: 0.6887435768291499\n",
      "Cost at iteration 56: 0.6886753449258461\n",
      "Cost at iteration 57: 0.6886074792543133\n",
      "Cost at iteration 58: 0.6885399771557268\n",
      "Cost at iteration 59: 0.688472835990831\n",
      "Cost at iteration 60: 0.6884060531397977\n",
      "Cost at iteration 61: 0.6883396260020875\n",
      "Cost at iteration 62: 0.6882735519963102\n",
      "Cost at iteration 63: 0.6882078285600872\n",
      "Cost at iteration 64: 0.6881424531499145\n",
      "Cost at iteration 65: 0.6880774232410272\n",
      "Cost at iteration 66: 0.6880127363272632\n",
      "Cost at iteration 67: 0.6879483899209308\n",
      "Cost at iteration 68: 0.6878843815526733\n",
      "Cost at iteration 69: 0.687820708771338\n",
      "Cost at iteration 70: 0.6877573691438447\n",
      "Cost at iteration 71: 0.6876943602550536\n",
      "Cost at iteration 72: 0.6876316797076374\n",
      "Cost at iteration 73: 0.6875693251219513\n",
      "Cost at iteration 74: 0.6875072941359046\n",
      "Cost at iteration 75: 0.6874455844048354\n",
      "Cost at iteration 76: 0.6873841936013817\n",
      "Cost at iteration 77: 0.6873231194153582\n",
      "Cost at iteration 78: 0.6872623595536301\n",
      "Cost at iteration 79: 0.6872019117399908\n",
      "Cost at iteration 80: 0.6871417737150377\n",
      "Cost at iteration 81: 0.6870819432360507\n",
      "Cost at iteration 82: 0.6870224180768711\n",
      "Cost at iteration 83: 0.6869631960277811\n",
      "Cost at iteration 84: 0.6869042748953842\n",
      "Cost at iteration 85: 0.6868456525024854\n",
      "Cost at iteration 86: 0.6867873266879758\n",
      "Cost at iteration 87: 0.6867292953067129\n",
      "Cost at iteration 88: 0.6866715562294051\n",
      "Cost at iteration 89: 0.6866141073424973\n",
      "Cost at iteration 90: 0.6865569465480542\n",
      "Cost at iteration 91: 0.6865000717636478\n",
      "Cost at iteration 92: 0.6864434809222437\n",
      "Cost at iteration 93: 0.6863871719720889\n",
      "Cost at iteration 94: 0.6863311428765997\n",
      "Cost at iteration 95: 0.6862753916142519\n",
      "Cost at iteration 96: 0.68621991617847\n",
      "Cost at iteration 97: 0.6861647145775176\n",
      "Cost at iteration 98: 0.6861097848343896\n",
      "Cost at iteration 99: 0.6860551249867043\n",
      "Cost at iteration 100: 0.6860007330865959\n",
      "Cost at iteration 101: 0.6859466072006091\n",
      "Cost at iteration 102: 0.6858927454095919\n",
      "Cost at iteration 103: 0.6858391458085933\n",
      "Cost at iteration 104: 0.6857858065067568\n",
      "Cost at iteration 105: 0.6857327256272187\n",
      "Cost at iteration 106: 0.6856799013070044\n",
      "Cost at iteration 107: 0.6856273316969276\n",
      "Cost at iteration 108: 0.6855750149614881\n",
      "Cost at iteration 109: 0.685522949278772\n",
      "Cost at iteration 110: 0.6854711328403515\n",
      "Cost at iteration 111: 0.6854195638511866\n",
      "Cost at iteration 112: 0.6853682405295253\n",
      "Cost at iteration 113: 0.685317161106808\n",
      "Cost at iteration 114: 0.6852663238275682\n",
      "Cost at iteration 115: 0.6852157269493385\n",
      "Cost at iteration 116: 0.6851653687425533\n",
      "Cost at iteration 117: 0.6851152474904545\n",
      "Cost at iteration 118: 0.6850653614889971\n",
      "Cost at iteration 119: 0.6850157090467562\n",
      "Cost at iteration 120: 0.6849662884848338\n",
      "Cost at iteration 121: 0.6849170981367649\n",
      "Cost at iteration 122: 0.6848681363484292\n",
      "Cost at iteration 123: 0.6848194014779576\n",
      "Cost at iteration 124: 0.6847708918956419\n",
      "Cost at iteration 125: 0.6847226059838469\n",
      "Cost at iteration 126: 0.6846745421369197\n",
      "Cost at iteration 127: 0.684626698761102\n",
      "Cost at iteration 128: 0.6845790742744426\n",
      "Cost at iteration 129: 0.6845316671067097\n",
      "Cost at iteration 130: 0.6844844756993047\n",
      "Cost at iteration 131: 0.6844374985051761\n",
      "Cost at iteration 132: 0.6843907339887347\n",
      "Cost at iteration 133: 0.6843441806257686\n",
      "Cost at iteration 134: 0.6842978369033589\n",
      "Cost at iteration 135: 0.6842517013197968\n",
      "Cost at iteration 136: 0.6842057723845008\n",
      "Cost at iteration 137: 0.6841600486179333\n",
      "Cost at iteration 138: 0.6841145285515207\n",
      "Cost at iteration 139: 0.6840692107275707\n",
      "Cost at iteration 140: 0.6840240936991929\n",
      "Cost at iteration 141: 0.6839791760302187\n",
      "Cost at iteration 142: 0.6839344562951212\n",
      "Cost at iteration 143: 0.6838899330789378\n",
      "Cost at iteration 144: 0.6838456049771905\n",
      "Cost at iteration 145: 0.6838014705958094\n",
      "Cost at iteration 146: 0.6837575285510552\n",
      "Cost at iteration 147: 0.6837137774694428\n",
      "Cost at iteration 148: 0.6836702159876653\n",
      "Cost at iteration 149: 0.6836268427525184\n",
      "Cost at iteration 150: 0.6835836564208257\n",
      "Cost at iteration 151: 0.6835406556593651\n",
      "Cost at iteration 152: 0.6834978391447932\n",
      "Cost at iteration 153: 0.6834552055635741\n",
      "Cost at iteration 154: 0.6834127536119055\n",
      "Cost at iteration 155: 0.6833704819956469\n",
      "Cost at iteration 156: 0.6833283894302474\n",
      "Cost at iteration 157: 0.6832864746406759\n",
      "Cost at iteration 158: 0.6832447363613487\n",
      "Cost at iteration 159: 0.6832031733360613\n",
      "Cost at iteration 160: 0.6831617843179165\n",
      "Cost at iteration 161: 0.683120568069258\n",
      "Cost at iteration 162: 0.6830795233615994\n",
      "Cost at iteration 163: 0.6830386489755578\n",
      "Cost at iteration 164: 0.6829979437007855\n",
      "Cost at iteration 165: 0.6829574063359027\n",
      "Cost at iteration 166: 0.6829170356884313\n",
      "Cost at iteration 167: 0.6828768305747288\n",
      "Cost at iteration 168: 0.6828367898199228\n",
      "Cost at iteration 169: 0.6827969122578451\n",
      "Cost at iteration 170: 0.6827571967309677\n",
      "Cost at iteration 171: 0.6827176420903386\n",
      "Cost at iteration 172: 0.6826782471955176\n",
      "Cost at iteration 173: 0.6826390109145137\n",
      "Cost at iteration 174: 0.6825999321237217\n",
      "Cost at iteration 175: 0.6825610097078602\n",
      "Cost at iteration 176: 0.6825222425599093\n",
      "Cost at iteration 177: 0.6824836295810501\n",
      "Cost at iteration 178: 0.6824451696806028\n",
      "Cost at iteration 179: 0.682406861775966\n",
      "Cost at iteration 180: 0.6823687047925578\n",
      "Cost at iteration 181: 0.6823306976637555\n",
      "Cost at iteration 182: 0.6822928393308357\n",
      "Cost at iteration 183: 0.6822551287429168\n",
      "Cost at iteration 184: 0.6822175648568993\n",
      "Cost at iteration 185: 0.6821801466374099\n",
      "Cost at iteration 186: 0.6821428730567413\n",
      "Cost at iteration 187: 0.6821057430947977\n",
      "Cost at iteration 188: 0.6820687557390366\n",
      "Cost at iteration 189: 0.682031909984413\n",
      "Cost at iteration 190: 0.6819952048333239\n",
      "Cost at iteration 191: 0.6819586392955523\n",
      "Cost at iteration 192: 0.681922212388213\n",
      "Cost at iteration 193: 0.6818859231356973\n",
      "Cost at iteration 194: 0.6818497705696193\n",
      "Cost at iteration 195: 0.6818137537287613\n",
      "Cost at iteration 196: 0.6817778716590221\n",
      "Cost at iteration 197: 0.6817421234133622\n",
      "Cost at iteration 198: 0.681706508051752\n",
      "Cost at iteration 199: 0.68167102464112\n",
      "Cost at iteration 200: 0.6816356722553003\n",
      "Cost at iteration 201: 0.6816004499749813\n",
      "Cost at iteration 202: 0.681565356887655\n",
      "Cost at iteration 203: 0.6815303920875658\n",
      "Cost at iteration 204: 0.6814955546756609\n",
      "Cost at iteration 205: 0.6814608437595395\n",
      "Cost at iteration 206: 0.6814262584534041\n",
      "Cost at iteration 207: 0.6813917978780106\n",
      "Cost at iteration 208: 0.6813574611606198\n",
      "Cost at iteration 209: 0.6813232474349493\n",
      "Cost at iteration 210: 0.6812891558411244\n",
      "Cost at iteration 211: 0.6812551855256312\n",
      "Cost at iteration 212: 0.6812213356412692\n",
      "Cost at iteration 213: 0.6811876053471038\n",
      "Cost at iteration 214: 0.6811539938084197\n",
      "Cost at iteration 215: 0.681120500196675\n",
      "Cost at iteration 216: 0.6810871236894546\n",
      "Cost at iteration 217: 0.6810538634704248\n",
      "Cost at iteration 218: 0.6810207187292882\n",
      "Cost at iteration 219: 0.6809876886617381\n",
      "Cost at iteration 220: 0.6809547724694149\n",
      "Cost at iteration 221: 0.6809219693598603\n",
      "Cost at iteration 222: 0.6808892785464745\n",
      "Cost at iteration 223: 0.6808566992484728\n",
      "Cost at iteration 224: 0.6808242306908404\n",
      "Cost at iteration 225: 0.6807918721042916\n",
      "Cost at iteration 226: 0.6807596227252257\n",
      "Cost at iteration 227: 0.6807274817956848\n",
      "Cost at iteration 228: 0.6806954485633125\n",
      "Cost at iteration 229: 0.6806635222813114\n",
      "Cost at iteration 230: 0.6806317022084016\n",
      "Cost at iteration 231: 0.6805999876087804\n",
      "Cost at iteration 232: 0.6805683777520802\n",
      "Cost at iteration 233: 0.6805368719133295\n",
      "Cost at iteration 234: 0.6805054693729112\n",
      "Cost at iteration 235: 0.6804741694165237\n",
      "Cost at iteration 236: 0.680442971335141\n",
      "Cost at iteration 237: 0.680411874424973\n",
      "Cost at iteration 238: 0.680380877987427\n",
      "Cost at iteration 239: 0.6803499813290685\n",
      "Cost at iteration 240: 0.6803191837615837\n",
      "Cost at iteration 241: 0.6802884846017396\n",
      "Cost at iteration 242: 0.6802578831713475\n",
      "Cost at iteration 243: 0.6802273787972255\n",
      "Cost at iteration 244: 0.6801969708111604\n",
      "Cost at iteration 245: 0.6801666585498701\n",
      "Cost at iteration 246: 0.6801364413549694\n",
      "Cost at iteration 247: 0.6801063185729306\n",
      "Cost at iteration 248: 0.6800762895550488\n",
      "Cost at iteration 249: 0.6800463536574055\n",
      "Cost at iteration 250: 0.6800165102408336\n",
      "Cost at iteration 251: 0.6799867586708804\n",
      "Cost at iteration 252: 0.679957098317775\n",
      "Cost at iteration 253: 0.6799275285563906\n",
      "Cost at iteration 254: 0.6798980487662115\n",
      "Cost at iteration 255: 0.6798686583312991\n",
      "Cost at iteration 256: 0.6798393566402564\n",
      "Cost at iteration 257: 0.6798101430861951\n",
      "Cost at iteration 258: 0.6797810170667018\n",
      "Cost at iteration 259: 0.6797519779838046\n",
      "Cost at iteration 260: 0.6797230252439399\n",
      "Cost at iteration 261: 0.6796941582579197\n",
      "Cost at iteration 262: 0.6796653764408991\n",
      "Cost at iteration 263: 0.6796366792123435\n",
      "Cost at iteration 264: 0.6796080659959969\n",
      "Cost at iteration 265: 0.6795795362198501\n",
      "Cost at iteration 266: 0.6795510893161084\n",
      "Cost at iteration 267: 0.6795227247211609\n",
      "Cost at iteration 268: 0.6794944418755491\n",
      "Cost at iteration 269: 0.6794662402239354\n",
      "Cost at iteration 270: 0.6794381192150736\n",
      "Cost at iteration 271: 0.6794100783017777\n",
      "Cost at iteration 272: 0.6793821169408907\n",
      "Cost at iteration 273: 0.6793542345932567\n",
      "Cost at iteration 274: 0.6793264307236893\n",
      "Cost at iteration 275: 0.6792987048009429\n",
      "Cost at iteration 276: 0.6792710562976829\n",
      "Cost at iteration 277: 0.6792434846904573\n",
      "Cost at iteration 278: 0.6792159894596665\n",
      "Cost at iteration 279: 0.6791885700895361\n",
      "Cost at iteration 280: 0.679161226068087\n",
      "Cost at iteration 281: 0.6791339568871085\n",
      "Cost at iteration 282: 0.6791067620421289\n",
      "Cost at iteration 283: 0.6790796410323888\n",
      "Cost at iteration 284: 0.6790525933608125\n",
      "Cost at iteration 285: 0.6790256185339815\n",
      "Cost at iteration 286: 0.6789987160621062\n",
      "Cost at iteration 287: 0.6789718854589997\n",
      "Cost at iteration 288: 0.6789451262420512\n",
      "Cost at iteration 289: 0.6789184379321982\n",
      "Cost at iteration 290: 0.6788918200539009\n",
      "Cost at iteration 291: 0.6788652721351164\n",
      "Cost at iteration 292: 0.6788387937072715\n",
      "Cost at iteration 293: 0.6788123843052377\n",
      "Cost at iteration 294: 0.6787860434673059\n",
      "Cost at iteration 295: 0.6787597707351593\n",
      "Cost at iteration 296: 0.6787335656538499\n",
      "Cost at iteration 297: 0.6787074277717728\n",
      "Cost at iteration 298: 0.6786813566406414\n",
      "Cost at iteration 299: 0.6786553518154621\n",
      "Cost at iteration 300: 0.6786294128545105\n",
      "Cost at iteration 301: 0.6786035393193072\n",
      "Cost at iteration 302: 0.6785777307745928\n",
      "Cost at iteration 303: 0.6785519867883046\n",
      "Cost at iteration 304: 0.6785263069315532\n",
      "Cost at iteration 305: 0.6785006907785975\n",
      "Cost at iteration 306: 0.6784751379068226\n",
      "Cost at iteration 307: 0.6784496478967159\n",
      "Cost at iteration 308: 0.6784242203318445\n",
      "Cost at iteration 309: 0.6783988547988318\n",
      "Cost at iteration 310: 0.6783735508873348\n",
      "Cost at iteration 311: 0.6783483081900217\n",
      "Cost at iteration 312: 0.6783231263025501\n",
      "Cost at iteration 313: 0.678298004823543\n",
      "Cost at iteration 314: 0.6782729433545692\n",
      "Cost at iteration 315: 0.6782479415001192\n",
      "Cost at iteration 316: 0.6782229988675847\n",
      "Cost at iteration 317: 0.6781981150672367\n",
      "Cost at iteration 318: 0.678173289712204\n",
      "Cost at iteration 319: 0.6781485224184525\n",
      "Cost at iteration 320: 0.6781238128047627\n",
      "Cost at iteration 321: 0.6780991604927107\n",
      "Cost at iteration 322: 0.6780745651066461\n",
      "Cost at iteration 323: 0.6780500262736715\n",
      "Cost at iteration 324: 0.6780255436236229\n",
      "Cost at iteration 325: 0.6780011167890477\n",
      "Cost at iteration 326: 0.677976745405187\n",
      "Cost at iteration 327: 0.6779524291099533\n",
      "Cost at iteration 328: 0.6779281675439115\n",
      "Cost at iteration 329: 0.6779039603502596\n",
      "Cost at iteration 330: 0.6778798071748088\n",
      "Cost at iteration 331: 0.6778557076659639\n",
      "Cost at iteration 332: 0.6778316614747039\n",
      "Cost at iteration 333: 0.6778076682545638\n",
      "Cost at iteration 334: 0.6777837276616149\n",
      "Cost at iteration 335: 0.6777598393544454\n",
      "Cost at iteration 336: 0.6777360029941428\n",
      "Cost at iteration 337: 0.6777122182442752\n",
      "Cost at iteration 338: 0.6776884847708722\n",
      "Cost at iteration 339: 0.6776648022424068\n",
      "Cost at iteration 340: 0.6776411703297782\n",
      "Cost at iteration 341: 0.6776175887062921\n",
      "Cost at iteration 342: 0.6775940570476446\n",
      "Cost at iteration 343: 0.6775705750319032\n",
      "Cost at iteration 344: 0.6775471423394895\n",
      "Cost at iteration 345: 0.6775237586531624\n",
      "Cost at iteration 346: 0.6775004236579995\n",
      "Cost at iteration 347: 0.6774771370413811\n",
      "Cost at iteration 348: 0.6774538984929728\n",
      "Cost at iteration 349: 0.6774307077047076\n",
      "Cost at iteration 350: 0.6774075643707707\n",
      "Cost at iteration 351: 0.6773844681875807\n",
      "Cost at iteration 352: 0.677361418853776\n",
      "Cost at iteration 353: 0.677338416070195\n",
      "Cost at iteration 354: 0.6773154595398622\n",
      "Cost at iteration 355: 0.6772925489679708\n",
      "Cost at iteration 356: 0.677269684061867\n",
      "Cost at iteration 357: 0.6772468645310342\n",
      "Cost at iteration 358: 0.677224090087077\n",
      "Cost at iteration 359: 0.6772013604437048\n",
      "Cost at iteration 360: 0.6771786753167174\n",
      "Cost at iteration 361: 0.6771560344239888\n",
      "Cost at iteration 362: 0.6771334374854516\n",
      "Cost at iteration 363: 0.6771108842230827\n",
      "Cost at iteration 364: 0.6770883743608869\n",
      "Cost at iteration 365: 0.6770659076248826\n",
      "Cost at iteration 366: 0.6770434837430872\n",
      "Cost at iteration 367: 0.6770211024455013\n",
      "Cost at iteration 368: 0.6769987634640945\n",
      "Cost at iteration 369: 0.6769764665327914\n",
      "Cost at iteration 370: 0.6769542113874563\n",
      "Cost at iteration 371: 0.676931997765879\n",
      "Cost at iteration 372: 0.6769098254077603\n",
      "Cost at iteration 373: 0.6768876940546993\n",
      "Cost at iteration 374: 0.6768656034501768\n",
      "Cost at iteration 375: 0.6768435533395437\n",
      "Cost at iteration 376: 0.676821543470006\n",
      "Cost at iteration 377: 0.6767995735906112\n",
      "Cost at iteration 378: 0.6767776434522347\n",
      "Cost at iteration 379: 0.6767557528075665\n",
      "Cost at iteration 380: 0.6767339014110973\n",
      "Cost at iteration 381: 0.6767120890191056\n",
      "Cost at iteration 382: 0.6766903153896445\n",
      "Cost at iteration 383: 0.6766685802825281\n",
      "Cost at iteration 384: 0.6766468834593188\n",
      "Cost at iteration 385: 0.6766252246833142\n",
      "Cost at iteration 386: 0.6766036037195343\n",
      "Cost at iteration 387: 0.6765820203347095\n",
      "Cost at iteration 388: 0.6765604742972662\n",
      "Cost at iteration 389: 0.6765389653773157\n",
      "Cost at iteration 390: 0.6765174933466418\n",
      "Cost at iteration 391: 0.6764960579786874\n",
      "Cost at iteration 392: 0.676474659048543\n",
      "Cost at iteration 393: 0.6764532963329342\n",
      "Cost at iteration 394: 0.6764319696102095\n",
      "Cost at iteration 395: 0.676410678660329\n",
      "Cost at iteration 396: 0.676389423264852\n",
      "Cost at iteration 397: 0.6763682032069243\n",
      "Cost at iteration 398: 0.6763470182712681\n",
      "Cost at iteration 399: 0.6763258682441693\n",
      "Cost at iteration 400: 0.6763047529134666\n",
      "Cost at iteration 401: 0.6762836720685388\n",
      "Cost at iteration 402: 0.6762626255002949\n",
      "Cost at iteration 403: 0.676241613001162\n",
      "Cost at iteration 404: 0.6762206343650741\n",
      "Cost at iteration 405: 0.6761996893874609\n",
      "Cost at iteration 406: 0.6761787778652376\n",
      "Cost at iteration 407: 0.6761578995967922\n",
      "Cost at iteration 408: 0.6761370543819767\n",
      "Cost at iteration 409: 0.6761162420220943\n",
      "Cost at iteration 410: 0.6760954623198905\n",
      "Cost at iteration 411: 0.6760747150795411\n",
      "Cost at iteration 412: 0.6760540001066419\n",
      "Cost at iteration 413: 0.6760333172081987\n",
      "Cost at iteration 414: 0.6760126661926167\n",
      "Cost at iteration 415: 0.6759920468696898\n",
      "Cost at iteration 416: 0.6759714590505908\n",
      "Cost at iteration 417: 0.6759509025478608\n",
      "Cost at iteration 418: 0.6759303771753996\n",
      "Cost at iteration 419: 0.6759098827484552\n",
      "Cost at iteration 420: 0.6758894190836141\n",
      "Cost at iteration 421: 0.6758689859987914\n",
      "Cost at iteration 422: 0.6758485833132207\n",
      "Cost at iteration 423: 0.6758282108474454\n",
      "Cost at iteration 424: 0.6758078684233069\n",
      "Cost at iteration 425: 0.6757875558639377\n",
      "Cost at iteration 426: 0.6757672729937498\n",
      "Cost at iteration 427: 0.6757470196384263\n",
      "Cost at iteration 428: 0.6757267956249117\n",
      "Cost at iteration 429: 0.6757066007814023\n",
      "Cost at iteration 430: 0.6756864349373379\n",
      "Cost at iteration 431: 0.6756662979233915\n",
      "Cost at iteration 432: 0.6756461895714612\n",
      "Cost at iteration 433: 0.6756261097146599\n",
      "Cost at iteration 434: 0.6756060581873079\n",
      "Cost at iteration 435: 0.6755860348249233\n",
      "Cost at iteration 436: 0.6755660394642122\n",
      "Cost at iteration 437: 0.6755460719430617\n",
      "Cost at iteration 438: 0.67552613210053\n",
      "Cost at iteration 439: 0.675506219776838\n",
      "Cost at iteration 440: 0.6754863348133613\n",
      "Cost at iteration 441: 0.6754664770526206\n",
      "Cost at iteration 442: 0.6754466463382743\n",
      "Cost at iteration 443: 0.6754268425151093\n",
      "Cost at iteration 444: 0.6754070654290336\n",
      "Cost at iteration 445: 0.6753873149270673\n",
      "Cost at iteration 446: 0.6753675908573342\n",
      "Cost at iteration 447: 0.675347893069055\n",
      "Cost at iteration 448: 0.6753282214125372\n",
      "Cost at iteration 449: 0.6753085757391695\n",
      "Cost at iteration 450: 0.6752889559014115\n",
      "Cost at iteration 451: 0.6752693617527872\n",
      "Cost at iteration 452: 0.6752497931478769\n",
      "Cost at iteration 453: 0.6752302499423092\n",
      "Cost at iteration 454: 0.6752107319927535\n",
      "Cost at iteration 455: 0.6751912391569124\n",
      "Cost at iteration 456: 0.6751717712935138\n",
      "Cost at iteration 457: 0.6751523282623036\n",
      "Cost at iteration 458: 0.675132909924038\n",
      "Cost at iteration 459: 0.6751135161404762\n",
      "Cost at iteration 460: 0.675094146774373\n",
      "Cost at iteration 461: 0.6750748016894716\n",
      "Cost at iteration 462: 0.6750554807504962\n",
      "Cost at iteration 463: 0.6750361838231443\n",
      "Cost at iteration 464: 0.675016910774081\n",
      "Cost at iteration 465: 0.6749976614709299\n",
      "Cost at iteration 466: 0.6749784357822677\n",
      "Cost at iteration 467: 0.6749592335776161\n",
      "Cost at iteration 468: 0.6749400547274356\n",
      "Cost at iteration 469: 0.6749208991031183\n",
      "Cost at iteration 470: 0.6749017665769805\n",
      "Cost at iteration 471: 0.6748826570222574\n",
      "Cost at iteration 472: 0.6748635703130944\n",
      "Cost at iteration 473: 0.6748445063245422\n",
      "Cost at iteration 474: 0.6748254649325489\n",
      "Cost at iteration 475: 0.6748064460139536\n",
      "Cost at iteration 476: 0.6747874494464807\n",
      "Cost at iteration 477: 0.6747684751087325\n",
      "Cost at iteration 478: 0.6747495228801826\n",
      "Cost at iteration 479: 0.6747305926411711\n",
      "Cost at iteration 480: 0.6747116842728954\n",
      "Cost at iteration 481: 0.6746927976574073\n",
      "Cost at iteration 482: 0.6746739326776034\n",
      "Cost at iteration 483: 0.6746550892172217\n",
      "Cost at iteration 484: 0.6746362671608336\n",
      "Cost at iteration 485: 0.6746174663938386\n",
      "Cost at iteration 486: 0.6745986868024577\n",
      "Cost at iteration 487: 0.6745799282737277\n",
      "Cost at iteration 488: 0.6745611906954957\n",
      "Cost at iteration 489: 0.674542473956412\n",
      "Cost at iteration 490: 0.6745237779459249\n",
      "Cost at iteration 491: 0.6745051025542754\n",
      "Cost at iteration 492: 0.6744864476724898\n",
      "Cost at iteration 493: 0.6744678131923754\n",
      "Cost at iteration 494: 0.6744491990065143\n",
      "Cost at iteration 495: 0.6744306050082574\n",
      "Cost at iteration 496: 0.6744120310917193\n",
      "Cost at iteration 497: 0.6743934771517721\n",
      "Cost at iteration 498: 0.6743749430840404\n",
      "Cost at iteration 499: 0.6743564287848953\n",
      "Cost at iteration 500: 0.6743379341514495\n",
      "Cost at iteration 501: 0.6743194590815511\n",
      "Cost at iteration 502: 0.6743010034737789\n",
      "Cost at iteration 503: 0.6742825672274365\n",
      "Cost at iteration 504: 0.6742641502425475\n",
      "Cost at iteration 505: 0.67424575241985\n",
      "Cost at iteration 506: 0.6742273736607912\n",
      "Cost at iteration 507: 0.6742090138675221\n",
      "Cost at iteration 508: 0.6741906729428928\n",
      "Cost at iteration 509: 0.674172350790447\n",
      "Cost at iteration 510: 0.6741540473144174\n",
      "Cost at iteration 511: 0.67413576241972\n",
      "Cost at iteration 512: 0.6741174960119491\n",
      "Cost at iteration 513: 0.6740992479973733\n",
      "Cost at iteration 514: 0.6740810182829295\n",
      "Cost at iteration 515: 0.674062806776218\n",
      "Cost at iteration 516: 0.6740446133854987\n",
      "Cost at iteration 517: 0.6740264380196852\n",
      "Cost at iteration 518: 0.6740082805883405\n",
      "Cost at iteration 519: 0.6739901410016721\n",
      "Cost at iteration 520: 0.6739720191705274\n",
      "Cost at iteration 521: 0.6739539150063887\n",
      "Cost at iteration 522: 0.6739358284213689\n",
      "Cost at iteration 523: 0.6739177593282066\n",
      "Cost at iteration 524: 0.6738997076402621\n",
      "Cost at iteration 525: 0.6738816732715122\n",
      "Cost at iteration 526: 0.6738636561365454\n",
      "Cost at iteration 527: 0.6738456561505589\n",
      "Cost at iteration 528: 0.6738276732293524\n",
      "Cost at iteration 529: 0.6738097072893245\n",
      "Cost at iteration 530: 0.6737917582474687\n",
      "Cost at iteration 531: 0.6737738260213684\n",
      "Cost at iteration 532: 0.6737559105291927\n",
      "Cost at iteration 533: 0.673738011689692\n",
      "Cost at iteration 534: 0.673720129422195\n",
      "Cost at iteration 535: 0.6737022636466018\n",
      "Cost at iteration 536: 0.6736844142833828\n",
      "Cost at iteration 537: 0.6736665812535721\n",
      "Cost at iteration 538: 0.6736487644787652\n",
      "Cost at iteration 539: 0.6736309638811132\n",
      "Cost at iteration 540: 0.6736131793833198\n",
      "Cost at iteration 541: 0.6735954109086375\n",
      "Cost at iteration 542: 0.673577658380863\n",
      "Cost at iteration 543: 0.6735599217243324\n",
      "Cost at iteration 544: 0.6735422008639197\n",
      "Cost at iteration 545: 0.6735244957250303\n",
      "Cost at iteration 546: 0.6735068062335983\n",
      "Cost at iteration 547: 0.6734891323160825\n",
      "Cost at iteration 548: 0.6734714738994634\n",
      "Cost at iteration 549: 0.6734538309112373\n",
      "Cost at iteration 550: 0.6734362032794148\n",
      "Cost at iteration 551: 0.6734185909325152\n",
      "Cost at iteration 552: 0.6734009937995644\n",
      "Cost at iteration 553: 0.67338341181009\n",
      "Cost at iteration 554: 0.6733658448941177\n",
      "Cost at iteration 555: 0.6733482929821688\n",
      "Cost at iteration 556: 0.6733307560052553\n",
      "Cost at iteration 557: 0.6733132338948766\n",
      "Cost at iteration 558: 0.6732957265830164\n",
      "Cost at iteration 559: 0.6732782340021392\n",
      "Cost at iteration 560: 0.673260756085186\n",
      "Cost at iteration 561: 0.6732432927655714\n",
      "Cost at iteration 562: 0.67322584397718\n",
      "Cost at iteration 563: 0.6732084096543635\n",
      "Cost at iteration 564: 0.6731909897319359\n",
      "Cost at iteration 565: 0.673173584145172\n",
      "Cost at iteration 566: 0.6731561928298024\n",
      "Cost at iteration 567: 0.6731388157220113\n",
      "Cost at iteration 568: 0.6731214527584319\n",
      "Cost at iteration 569: 0.673104103876145\n",
      "Cost at iteration 570: 0.6730867690126741\n",
      "Cost at iteration 571: 0.6730694481059827\n",
      "Cost at iteration 572: 0.673052141094471\n",
      "Cost at iteration 573: 0.6730348479169734\n",
      "Cost at iteration 574: 0.6730175685127546\n",
      "Cost at iteration 575: 0.6730003028215066\n",
      "Cost at iteration 576: 0.6729830507833449\n",
      "Cost at iteration 577: 0.6729658123388069\n",
      "Cost at iteration 578: 0.6729485874288484\n",
      "Cost at iteration 579: 0.6729313759948394\n",
      "Cost at iteration 580: 0.6729141779785621\n",
      "Cost at iteration 581: 0.6728969933222079\n",
      "Cost at iteration 582: 0.6728798219683739\n",
      "Cost at iteration 583: 0.6728626638600606\n",
      "Cost at iteration 584: 0.6728455189406679\n",
      "Cost at iteration 585: 0.6728283871539936\n",
      "Cost at iteration 586: 0.6728112684442294\n",
      "Cost at iteration 587: 0.6727941627559587\n",
      "Cost at iteration 588: 0.6727770700341531\n",
      "Cost at iteration 589: 0.67275999022417\n",
      "Cost at iteration 590: 0.6727429232717497\n",
      "Cost at iteration 591: 0.6727258691230127\n",
      "Cost at iteration 592: 0.672708827724457\n",
      "Cost at iteration 593: 0.6726917990229543\n",
      "Cost at iteration 594: 0.6726747829657497\n",
      "Cost at iteration 595: 0.672657779500456\n",
      "Cost at iteration 596: 0.6726407885750535\n",
      "Cost at iteration 597: 0.6726238101378854\n",
      "Cost at iteration 598: 0.6726068441376567\n",
      "Cost at iteration 599: 0.6725898905234309\n",
      "Cost at iteration 600: 0.6725729492446266\n",
      "Cost at iteration 601: 0.6725560202510169\n",
      "Cost at iteration 602: 0.6725391034927246\n",
      "Cost at iteration 603: 0.6725221989202212\n",
      "Cost at iteration 604: 0.6725053064843239\n",
      "Cost at iteration 605: 0.6724884261361929\n",
      "Cost at iteration 606: 0.6724715578273287\n",
      "Cost at iteration 607: 0.6724547015095703\n",
      "Cost at iteration 608: 0.6724378571350925\n",
      "Cost at iteration 609: 0.6724210246564031\n",
      "Cost at iteration 610: 0.6724042040263407\n",
      "Cost at iteration 611: 0.6723873951980722\n",
      "Cost at iteration 612: 0.6723705981250911\n",
      "Cost at iteration 613: 0.672353812761214\n",
      "Cost at iteration 614: 0.6723370390605788\n",
      "Cost at iteration 615: 0.6723202769776427\n",
      "Cost at iteration 616: 0.672303526467179\n",
      "Cost at iteration 617: 0.6722867874842757\n",
      "Cost at iteration 618: 0.6722700599843331\n",
      "Cost at iteration 619: 0.6722533439230605\n",
      "Cost at iteration 620: 0.6722366392564754\n",
      "Cost at iteration 621: 0.6722199459409004\n",
      "Cost at iteration 622: 0.6722032639329607\n",
      "Cost at iteration 623: 0.6721865931895832\n",
      "Cost at iteration 624: 0.6721699336679928\n",
      "Cost at iteration 625: 0.6721532853257111\n",
      "Cost at iteration 626: 0.6721366481205542\n",
      "Cost at iteration 627: 0.6721200220106298\n",
      "Cost at iteration 628: 0.6721034069543367\n",
      "Cost at iteration 629: 0.6720868029103606\n",
      "Cost at iteration 630: 0.6720702098376737\n",
      "Cost at iteration 631: 0.6720536276955319\n",
      "Cost at iteration 632: 0.672037056443473\n",
      "Cost at iteration 633: 0.6720204960413135\n",
      "Cost at iteration 634: 0.672003946449149\n",
      "Cost at iteration 635: 0.6719874076273499\n",
      "Cost at iteration 636: 0.67197087953656\n",
      "Cost at iteration 637: 0.6719543621376957\n",
      "Cost at iteration 638: 0.6719378553919417\n",
      "Cost at iteration 639: 0.671921359260752\n",
      "Cost at iteration 640: 0.671904873705845\n",
      "Cost at iteration 641: 0.6718883986892035\n",
      "Cost at iteration 642: 0.6718719341730723\n",
      "Cost at iteration 643: 0.6718554801199561\n",
      "Cost at iteration 644: 0.6718390364926178\n",
      "Cost at iteration 645: 0.6718226032540763\n",
      "Cost at iteration 646: 0.6718061803676052\n",
      "Cost at iteration 647: 0.6717897677967304\n",
      "Cost at iteration 648: 0.6717733655052287\n",
      "Cost at iteration 649: 0.6717569734571258\n",
      "Cost at iteration 650: 0.6717405916166941\n",
      "Cost at iteration 651: 0.6717242199484517\n",
      "Cost at iteration 652: 0.6717078584171603\n",
      "Cost at iteration 653: 0.6716915069878228\n",
      "Cost at iteration 654: 0.6716751656256824\n",
      "Cost at iteration 655: 0.6716588342962209\n",
      "Cost at iteration 656: 0.671642512965156\n",
      "Cost at iteration 657: 0.6716262015984406\n",
      "Cost at iteration 658: 0.6716099001622604\n",
      "Cost at iteration 659: 0.6715936086230327\n",
      "Cost at iteration 660: 0.6715773269474047\n",
      "Cost at iteration 661: 0.6715610551022512\n",
      "Cost at iteration 662: 0.6715447930546736\n",
      "Cost at iteration 663: 0.6715285407719985\n",
      "Cost at iteration 664: 0.6715122982217749\n",
      "Cost at iteration 665: 0.6714960653717738\n",
      "Cost at iteration 666: 0.6714798421899857\n",
      "Cost at iteration 667: 0.6714636286446201\n",
      "Cost at iteration 668: 0.6714474247041023\n",
      "Cost at iteration 669: 0.6714312303370739\n",
      "Cost at iteration 670: 0.671415045512389\n",
      "Cost at iteration 671: 0.671398870199114\n",
      "Cost at iteration 672: 0.6713827043665269\n",
      "Cost at iteration 673: 0.6713665479841129\n",
      "Cost at iteration 674: 0.6713504010215658\n",
      "Cost at iteration 675: 0.6713342634487856\n",
      "Cost at iteration 676: 0.6713181352358759\n",
      "Cost at iteration 677: 0.6713020163531437\n",
      "Cost at iteration 678: 0.6712859067710978\n",
      "Cost at iteration 679: 0.6712698064604465\n",
      "Cost at iteration 680: 0.6712537153920971\n",
      "Cost at iteration 681: 0.6712376335371544\n",
      "Cost at iteration 682: 0.6712215608669176\n",
      "Cost at iteration 683: 0.6712054973528822\n",
      "Cost at iteration 684: 0.6711894429667351\n",
      "Cost at iteration 685: 0.671173397680355\n",
      "Cost at iteration 686: 0.6711573614658113\n",
      "Cost at iteration 687: 0.6711413342953614\n",
      "Cost at iteration 688: 0.6711253161414508\n",
      "Cost at iteration 689: 0.6711093069767107\n",
      "Cost at iteration 690: 0.6710933067739568\n",
      "Cost at iteration 691: 0.6710773155061887\n",
      "Cost at iteration 692: 0.6710613331465873\n",
      "Cost at iteration 693: 0.6710453596685149\n",
      "Cost at iteration 694: 0.6710293950455125\n",
      "Cost at iteration 695: 0.6710134392512999\n",
      "Cost at iteration 696: 0.6709974922597733\n",
      "Cost at iteration 697: 0.6709815540450046\n",
      "Cost at iteration 698: 0.6709656245812398\n",
      "Cost at iteration 699: 0.6709497038428978\n",
      "Cost at iteration 0: 0.6931471805599454\n",
      "Cost at iteration 1: 1.6894221525385182\n",
      "Cost at iteration 2: 11.065673682088107\n",
      "Cost at iteration 3: 5.973392815720472\n",
      "Cost at iteration 4: 5.751591830336954\n",
      "Cost at iteration 5: 9.000548933822536\n",
      "Cost at iteration 6: 2.2842012747398357\n",
      "Cost at iteration 7: 4.07041260321668\n",
      "Cost at iteration 8: 5.381729557291599\n",
      "Cost at iteration 9: 8.474747973396997\n",
      "Cost at iteration 10: 2.6499363556787374\n",
      "Cost at iteration 11: 4.159915321625812\n",
      "Cost at iteration 12: 4.32718766034075\n",
      "Cost at iteration 13: 7.958098184709417\n",
      "Cost at iteration 14: 2.9805561710717634\n",
      "Cost at iteration 15: 4.969719515156601\n",
      "Cost at iteration 16: 3.943633513681007\n",
      "Cost at iteration 17: 6.6714790750517245\n",
      "Cost at iteration 18: 3.8680219384712227\n",
      "Cost at iteration 19: 5.50488367545567\n",
      "Cost at iteration 20: 3.744582606791216\n",
      "Cost at iteration 21: 5.757801507067866\n",
      "Cost at iteration 22: 4.075711321735501\n",
      "Cost at iteration 23: 6.1360392417278895\n",
      "Cost at iteration 24: 3.822191453650178\n",
      "Cost at iteration 25: 5.448736829991803\n",
      "Cost at iteration 26: 3.5606176140758343\n",
      "Cost at iteration 27: 5.736080526171801\n",
      "Cost at iteration 28: 4.134583913790649\n",
      "Cost at iteration 29: 6.5044107216039935\n",
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "#Test your function\n",
    "w1_unittest.test_gradientDescent(gradientDescent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Extracting the Features\n",
    "\n",
    "* Given a list of tweets, extract the features and store them in a matrix. You will extract two features.\n",
    "    * The first feature is the number of positive words in a tweet.\n",
    "    * The second feature is the number of negative words in a tweet. \n",
    "* Then train your logistic regression classifier on these features.\n",
    "* Test the classifier on a validation set. \n",
    "\n",
    "<a name='ex-3'></a>\n",
    "### Exercise 3 - extract_features\n",
    "Implement the extract_features function. \n",
    "* This function takes in a single tweet.\n",
    "* Process the tweet using the imported `process_tweet` function and save the list of tweet words.\n",
    "* Loop through each word in the list of processed words\n",
    "    * For each word, check the 'freqs' dictionary for the count when that word has a positive '1' label. (Check for the key (word, 1.0)\n",
    "    * Do the same for the count for when the word is associated with the negative label '0'. (Check for the key (word, 0.0).)\n",
    "\n",
    "**Note:** In the implementation instructions provided above, the prediction of being positive or negative depends on feature vector which counts-in duplicate words - this is different from what you have seen in the lecture videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Make sure you handle cases when the (word, label) key is not found in the dictionary. </li>\n",
    "    <li> Search the web for hints about using the 'get' function of a Python dictionary.  Here is an <a href=\"https://www.programiz.com/python-programming/methods/dictionary/get\" > example </a> </li>\n",
    "</ul>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C3 GRADED FUNCTION: extract_features\n",
    "def extract_features(tweet, freqs, process_tweet=process_tweet):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet: a string containing one tweet\n",
    "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
    "    Output: \n",
    "        x: a feature vector of dimension (1,3)\n",
    "    '''\n",
    "    # process_tweet tokenizes, stems, and removes stopwords\n",
    "    word_l = process_tweet(tweet)\n",
    "    \n",
    "    # 3 elements for [bias, positive, negative] counts\n",
    "    x = np.zeros(3) \n",
    "    \n",
    "    # bias term is set to 1\n",
    "    x[0] = 1 \n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    " # loop through each word in the list of words\n",
    "    for word in word_l:\n",
    "        # increment the word count for the positive label 1\n",
    "        x[1] += freqs.get((word, 1), 0)\n",
    "        \n",
    "        # increment the word count for the negative label 0\n",
    "        x[2] += freqs.get((word, 0), 0)\n",
    "    \n",
    "    x = x[None, :]  # adding batch dimension for further processing\n",
    "    assert(x.shape == (1, 3))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.000e+00 3.133e+03 6.100e+01]]\n"
     ]
    }
   ],
   "source": [
    "# Check your function\n",
    "# test 1\n",
    "# test on training data\n",
    "tmp1 = extract_features(train_x[0], freqs)\n",
    "print(tmp1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected output\n",
    "```\n",
    "[[1.000e+00 3.133e+03 6.100e+01]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# test 2:\n",
    "# check for when the words are not in the freqs dictionary\n",
    "tmp2 = extract_features('blorb bleeeeb bloooob', freqs)\n",
    "print(tmp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected output\n",
    "```\n",
    "[[1. 0. 0.]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "#Test your function\n",
    "w1_unittest.test_extract_features(extract_features, freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Training Your Model\n",
    "\n",
    "To train the model:\n",
    "* Stack the features for all training examples into a matrix X. \n",
    "* Call `gradientDescent`, which you've implemented above.\n",
    "\n",
    "This section is given to you.  Please read it for understanding and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at iteration 0: 0.6931471805599454\n",
      "Cost at iteration 1: 0.6920131778768511\n",
      "Cost at iteration 2: 0.6908826769826056\n",
      "Cost at iteration 3: 0.6897556660874083\n",
      "Cost at iteration 4: 0.6886321334025985\n",
      "Cost at iteration 5: 0.6875120671413247\n",
      "Cost at iteration 6: 0.686395455519214\n",
      "Cost at iteration 7: 0.685282286755043\n",
      "Cost at iteration 8: 0.6841725490714047\n",
      "Cost at iteration 9: 0.6830662306953775\n",
      "Cost at iteration 10: 0.6819633198591915\n",
      "Cost at iteration 11: 0.6808638048008918\n",
      "Cost at iteration 12: 0.6797676737650029\n",
      "Cost at iteration 13: 0.6786749150031863\n",
      "Cost at iteration 14: 0.6775855167749\n",
      "Cost at iteration 15: 0.676499467348051\n",
      "Cost at iteration 16: 0.6754167549996475\n",
      "Cost at iteration 17: 0.6743373680164446\n",
      "Cost at iteration 18: 0.6732612946955893\n",
      "Cost at iteration 19: 0.6721885233452576\n",
      "Cost at iteration 20: 0.6711190422852893\n",
      "Cost at iteration 21: 0.6700528398478184\n",
      "Cost at iteration 22: 0.6689899043778962\n",
      "Cost at iteration 23: 0.6679302242341107\n",
      "Cost at iteration 24: 0.6668737877891997\n",
      "Cost at iteration 25: 0.6658205834306583\n",
      "Cost at iteration 26: 0.6647705995613398\n",
      "Cost at iteration 27: 0.6637238246000506\n",
      "Cost at iteration 28: 0.6626802469821382\n",
      "Cost at iteration 29: 0.661639855160073\n",
      "Cost at iteration 30: 0.6606026376040212\n",
      "Cost at iteration 31: 0.6595685828024137\n",
      "Cost at iteration 32: 0.6585376792625041\n",
      "Cost at iteration 33: 0.6575099155109212\n",
      "Cost at iteration 34: 0.6564852800942127\n",
      "Cost at iteration 35: 0.6554637615793825\n",
      "Cost at iteration 36: 0.6544453485544176\n",
      "Cost at iteration 37: 0.6534300296288083\n",
      "Cost at iteration 38: 0.6524177934340597\n",
      "Cost at iteration 39: 0.6514086286241946\n",
      "Cost at iteration 40: 0.6504025238762479\n",
      "Cost at iteration 41: 0.649399467890752\n",
      "Cost at iteration 42: 0.6483994493922137\n",
      "Cost at iteration 43: 0.6474024571295818\n",
      "Cost at iteration 44: 0.6464084798767065\n",
      "Cost at iteration 45: 0.6454175064327881\n",
      "Cost at iteration 46: 0.6444295256228193\n",
      "Cost at iteration 47: 0.6434445262980151\n",
      "Cost at iteration 48: 0.6424624973362361\n",
      "Cost at iteration 49: 0.6414834276424012\n",
      "Cost at iteration 50: 0.640507306148891\n",
      "Cost at iteration 51: 0.639534121815943\n",
      "Cost at iteration 52: 0.6385638636320364\n",
      "Cost at iteration 53: 0.6375965206142672\n",
      "Cost at iteration 54: 0.6366320818087159\n",
      "Cost at iteration 55: 0.6356705362908037\n",
      "Cost at iteration 56: 0.6347118731656415\n",
      "Cost at iteration 57: 0.633756081568367\n",
      "Cost at iteration 58: 0.6328031506644755\n",
      "Cost at iteration 59: 0.6318530696501387\n",
      "Cost at iteration 60: 0.6309058277525178\n",
      "Cost at iteration 61: 0.6299614142300629\n",
      "Cost at iteration 62: 0.6290198183728076\n",
      "Cost at iteration 63: 0.6280810295026523\n",
      "Cost at iteration 64: 0.6271450369736384\n",
      "Cost at iteration 65: 0.6262118301722156\n",
      "Cost at iteration 66: 0.625281398517498\n",
      "Cost at iteration 67: 0.6243537314615127\n",
      "Cost at iteration 68: 0.6234288184894405\n",
      "Cost at iteration 69: 0.6225066491198454\n",
      "Cost at iteration 70: 0.6215872129048992\n",
      "Cost at iteration 71: 0.6206704994305937\n",
      "Cost at iteration 72: 0.6197564983169497\n",
      "Cost at iteration 73: 0.6188451992182116\n",
      "Cost at iteration 74: 0.6179365918230387\n",
      "Cost at iteration 75: 0.6170306658546874\n",
      "Cost at iteration 76: 0.6161274110711837\n",
      "Cost at iteration 77: 0.6152268172654906\n",
      "Cost at iteration 78: 0.6143288742656645\n",
      "Cost at iteration 79: 0.6134335719350082\n",
      "Cost at iteration 80: 0.6125409001722122\n",
      "Cost at iteration 81: 0.6116508489114919\n",
      "Cost at iteration 82: 0.6107634081227158\n",
      "Cost at iteration 83: 0.6098785678115266\n",
      "Cost at iteration 84: 0.6089963180194572\n",
      "Cost at iteration 85: 0.6081166488240359\n",
      "Cost at iteration 86: 0.6072395503388901\n",
      "Cost at iteration 87: 0.6063650127138385\n",
      "Cost at iteration 88: 0.6054930261349794\n",
      "Cost at iteration 89: 0.6046235808247722\n",
      "Cost at iteration 90: 0.6037566670421121\n",
      "Cost at iteration 91: 0.6028922750823995\n",
      "Cost at iteration 92: 0.6020303952776015\n",
      "Cost at iteration 93: 0.6011710179963098\n",
      "Cost at iteration 94: 0.6003141336437917\n",
      "Cost at iteration 95: 0.5994597326620349\n",
      "Cost at iteration 96: 0.598607805529787\n",
      "Cost at iteration 97: 0.5977583427625908\n",
      "Cost at iteration 98: 0.5969113349128121\n",
      "Cost at iteration 99: 0.5960667725696642\n",
      "Cost at iteration 100: 0.595224646359226\n",
      "Cost at iteration 101: 0.5943849469444554\n",
      "Cost at iteration 102: 0.5935476650251992\n",
      "Cost at iteration 103: 0.5927127913381949\n",
      "Cost at iteration 104: 0.5918803166570721\n",
      "Cost at iteration 105: 0.5910502317923458\n",
      "Cost at iteration 106: 0.5902225275914077\n",
      "Cost at iteration 107: 0.5893971949385112\n",
      "Cost at iteration 108: 0.5885742247547535\n",
      "Cost at iteration 109: 0.5877536079980533\n",
      "Cost at iteration 110: 0.5869353356631246\n",
      "Cost at iteration 111: 0.586119398781446\n",
      "Cost at iteration 112: 0.5853057884212264\n",
      "Cost at iteration 113: 0.5844944956873681\n",
      "Cost at iteration 114: 0.583685511721424\n",
      "Cost at iteration 115: 0.5828788277015542\n",
      "Cost at iteration 116: 0.5820744348424763\n",
      "Cost at iteration 117: 0.5812723243954147\n",
      "Cost at iteration 118: 0.580472487648045\n",
      "Cost at iteration 119: 0.5796749159244367\n",
      "Cost at iteration 120: 0.5788796005849913\n",
      "Cost at iteration 121: 0.5780865330263791\n",
      "Cost at iteration 122: 0.577295704681472\n",
      "Cost at iteration 123: 0.5765071070192733\n",
      "Cost at iteration 124: 0.5757207315448469\n",
      "Cost at iteration 125: 0.5749365697992402\n",
      "Cost at iteration 126: 0.5741546133594086\n",
      "Cost at iteration 127: 0.5733748538381345\n",
      "Cost at iteration 128: 0.572597282883945\n",
      "Cost at iteration 129: 0.5718218921810276\n",
      "Cost at iteration 130: 0.5710486734491437\n",
      "Cost at iteration 131: 0.5702776184435389\n",
      "Cost at iteration 132: 0.5695087189548527\n",
      "Cost at iteration 133: 0.5687419668090244\n",
      "Cost at iteration 134: 0.5679773538672\n",
      "Cost at iteration 135: 0.5672148720256335\n",
      "Cost at iteration 136: 0.5664545132155889\n",
      "Cost at iteration 137: 0.5656962694032406\n",
      "Cost at iteration 138: 0.5649401325895703\n",
      "Cost at iteration 139: 0.5641860948102642\n",
      "Cost at iteration 140: 0.5634341481356073\n",
      "Cost at iteration 141: 0.562684284670377\n",
      "Cost at iteration 142: 0.561936496553735\n",
      "Cost at iteration 143: 0.5611907759591177\n",
      "Cost at iteration 144: 0.5604471150941259\n",
      "Cost at iteration 145: 0.5597055062004124\n",
      "Cost at iteration 146: 0.5589659415535687\n",
      "Cost at iteration 147: 0.5582284134630109\n",
      "Cost at iteration 148: 0.5574929142718641\n",
      "Cost at iteration 149: 0.556759436356846\n",
      "Cost at iteration 150: 0.5560279721281493\n",
      "Cost at iteration 151: 0.5552985140293231\n",
      "Cost at iteration 152: 0.5545710545371534\n",
      "Cost at iteration 153: 0.5538455861615433\n",
      "Cost at iteration 154: 0.5531221014453918\n",
      "Cost at iteration 155: 0.5524005929644712\n",
      "Cost at iteration 156: 0.5516810533273055\n",
      "Cost at iteration 157: 0.5509634751750465\n",
      "Cost at iteration 158: 0.5502478511813499\n",
      "Cost at iteration 159: 0.5495341740522508\n",
      "Cost at iteration 160: 0.5488224365260387\n",
      "Cost at iteration 161: 0.5481126313731313\n",
      "Cost at iteration 162: 0.5474047513959486\n",
      "Cost at iteration 163: 0.5466987894287864\n",
      "Cost at iteration 164: 0.5459947383376882\n",
      "Cost at iteration 165: 0.5452925910203191\n",
      "Cost at iteration 166: 0.5445923404058366\n",
      "Cost at iteration 167: 0.5438939794547628\n",
      "Cost at iteration 168: 0.5431975011588556\n",
      "Cost at iteration 169: 0.5425028985409799\n",
      "Cost at iteration 170: 0.5418101646549791\n",
      "Cost at iteration 171: 0.5411192925855439\n",
      "Cost at iteration 172: 0.5404302754480849\n",
      "Cost at iteration 173: 0.5397431063886009\n",
      "Cost at iteration 174: 0.5390577785835501\n",
      "Cost at iteration 175: 0.5383742852397198\n",
      "Cost at iteration 176: 0.5376926195940958\n",
      "Cost at iteration 177: 0.5370127749137323\n",
      "Cost at iteration 178: 0.5363347444956222\n",
      "Cost at iteration 179: 0.5356585216665657\n",
      "Cost at iteration 180: 0.5349840997830404\n",
      "Cost at iteration 181: 0.5343114722310709\n",
      "Cost at iteration 182: 0.5336406324260984\n",
      "Cost at iteration 183: 0.5329715738128503\n",
      "Cost at iteration 184: 0.5323042898652101\n",
      "Cost at iteration 185: 0.5316387740860865\n",
      "Cost at iteration 186: 0.5309750200072842\n",
      "Cost at iteration 187: 0.5303130211893736\n",
      "Cost at iteration 188: 0.52965277122156\n",
      "Cost at iteration 189: 0.5289942637215558\n",
      "Cost at iteration 190: 0.5283374923354486\n",
      "Cost at iteration 191: 0.5276824507375737\n",
      "Cost at iteration 192: 0.5270291326303839\n",
      "Cost at iteration 193: 0.5263775317443203\n",
      "Cost at iteration 194: 0.5257276418376844\n",
      "Cost at iteration 195: 0.5250794566965085\n",
      "Cost at iteration 196: 0.5244329701344281\n",
      "Cost at iteration 197: 0.5237881759925535\n",
      "Cost at iteration 198: 0.5231450681393415\n",
      "Cost at iteration 199: 0.5225036404704688\n",
      "Cost at iteration 200: 0.5218638869087039\n",
      "Cost at iteration 201: 0.5212258014037805\n",
      "Cost at iteration 202: 0.5205893779322709\n",
      "Cost at iteration 203: 0.5199546104974588\n",
      "Cost at iteration 204: 0.5193214931292148\n",
      "Cost at iteration 205: 0.5186900198838691\n",
      "Cost at iteration 206: 0.5180601848440871\n",
      "Cost at iteration 207: 0.517431982118744\n",
      "Cost at iteration 208: 0.5168054058428001\n",
      "Cost at iteration 209: 0.5161804501771767\n",
      "Cost at iteration 210: 0.5155571093086322\n",
      "Cost at iteration 211: 0.5149353774496382\n",
      "Cost at iteration 212: 0.5143152488382569\n",
      "Cost at iteration 213: 0.5136967177380173\n",
      "Cost at iteration 214: 0.5130797784377941\n",
      "Cost at iteration 215: 0.5124644252516849\n",
      "Cost at iteration 216: 0.511850652518889\n",
      "Cost at iteration 217: 0.511238454603586\n",
      "Cost at iteration 218: 0.5106278258948151\n",
      "Cost at iteration 219: 0.5100187608063551\n",
      "Cost at iteration 220: 0.5094112537766042\n",
      "Cost at iteration 221: 0.5088052992684606\n",
      "Cost at iteration 222: 0.508200891769204\n",
      "Cost at iteration 223: 0.5075980257903762\n",
      "Cost at iteration 224: 0.5069966958676635\n",
      "Cost at iteration 225: 0.5063968965607792\n",
      "Cost at iteration 226: 0.5057986224533463\n",
      "Cost at iteration 227: 0.5052018681527801\n",
      "Cost at iteration 228: 0.5046066282901731\n",
      "Cost at iteration 229: 0.5040128975201782\n",
      "Cost at iteration 230: 0.5034206705208938\n",
      "Cost at iteration 231: 0.5028299419937488\n",
      "Cost at iteration 232: 0.5022407066633882\n",
      "Cost at iteration 233: 0.5016529592775593\n",
      "Cost at iteration 234: 0.5010666946069982\n",
      "Cost at iteration 235: 0.5004819074453163\n",
      "Cost at iteration 236: 0.49989859260888897\n",
      "Cost at iteration 237: 0.4993167449367421\n",
      "Cost at iteration 238: 0.4987363592904418\n",
      "Cost at iteration 239: 0.4981574305539824\n",
      "Cost at iteration 240: 0.49757995363367674\n",
      "Cost at iteration 241: 0.4970039234580452\n",
      "Cost at iteration 242: 0.49642933497770697\n",
      "Cost at iteration 243: 0.4958561831652704\n",
      "Cost at iteration 244: 0.4952844630152244\n",
      "Cost at iteration 245: 0.4947141695438306\n",
      "Cost at iteration 246: 0.4941452977890156\n",
      "Cost at iteration 247: 0.49357784281026373\n",
      "Cost at iteration 248: 0.49301179968851055\n",
      "Cost at iteration 249: 0.49244716352603646\n",
      "Cost at iteration 250: 0.49188392944636145\n",
      "Cost at iteration 251: 0.49132209259413956\n",
      "Cost at iteration 252: 0.49076164813505435\n",
      "Cost at iteration 253: 0.49020259125571447\n",
      "Cost at iteration 254: 0.48964491716355074\n",
      "Cost at iteration 255: 0.4890886210867119\n",
      "Cost at iteration 256: 0.48853369827396287\n",
      "Cost at iteration 257: 0.48798014399458145\n",
      "Cost at iteration 258: 0.48742795353825796\n",
      "Cost at iteration 259: 0.48687712221499285\n",
      "Cost at iteration 260: 0.48632764535499645\n",
      "Cost at iteration 261: 0.4857795183085888\n",
      "Cost at iteration 262: 0.48523273644609966\n",
      "Cost at iteration 263: 0.4846872951577694\n",
      "Cost at iteration 264: 0.48414318985365024\n",
      "Cost at iteration 265: 0.48360041596350756\n",
      "Cost at iteration 266: 0.4830589689367229\n",
      "Cost at iteration 267: 0.4825188442421956\n",
      "Cost at iteration 268: 0.48198003736824696\n",
      "Cost at iteration 269: 0.48144254382252305\n",
      "Cost at iteration 270: 0.48090635913189955\n",
      "Cost at iteration 271: 0.48037147884238585\n",
      "Cost at iteration 272: 0.4798378985190303\n",
      "Cost at iteration 273: 0.47930561374582575\n",
      "Cost at iteration 274: 0.47877462012561595\n",
      "Cost at iteration 275: 0.478244913280001\n",
      "Cost at iteration 276: 0.47771648884924606\n",
      "Cost at iteration 277: 0.47718934249218714\n",
      "Cost at iteration 278: 0.47666346988614006\n",
      "Cost at iteration 279: 0.4761388667268083\n",
      "Cost at iteration 280: 0.4756155287281925\n",
      "Cost at iteration 281: 0.47509345162249916\n",
      "Cost at iteration 282: 0.47457263116005083\n",
      "Cost at iteration 283: 0.4740530631091963\n",
      "Cost at iteration 284: 0.4735347432562213\n",
      "Cost at iteration 285: 0.47301766740525986\n",
      "Cost at iteration 286: 0.47250183137820556\n",
      "Cost at iteration 287: 0.4719872310146244\n",
      "Cost at iteration 288: 0.4714738621716665\n",
      "Cost at iteration 289: 0.4709617207239799\n",
      "Cost at iteration 290: 0.47045080256362354\n",
      "Cost at iteration 291: 0.4699411035999814\n",
      "Cost at iteration 292: 0.46943261975967693\n",
      "Cost at iteration 293: 0.46892534698648775\n",
      "Cost at iteration 294: 0.46841928124126103\n",
      "Cost at iteration 295: 0.46791441850182935\n",
      "Cost at iteration 296: 0.46741075476292643\n",
      "Cost at iteration 297: 0.4669082860361044\n",
      "Cost at iteration 298: 0.46640700834965004\n",
      "Cost at iteration 299: 0.4659069177485034\n",
      "Cost at iteration 300: 0.46540801029417433\n",
      "Cost at iteration 301: 0.46491028206466195\n",
      "Cost at iteration 302: 0.4644137291543732\n",
      "Cost at iteration 303: 0.46391834767404144\n",
      "Cost at iteration 304: 0.463424133750647\n",
      "Cost at iteration 305: 0.4629310835273369\n",
      "Cost at iteration 306: 0.46243919316334486\n",
      "Cost at iteration 307: 0.4619484588339131\n",
      "Cost at iteration 308: 0.46145887673021285\n",
      "Cost at iteration 309: 0.46097044305926693\n",
      "Cost at iteration 310: 0.46048315404387136\n",
      "Cost at iteration 311: 0.45999700592251785\n",
      "Cost at iteration 312: 0.45951199494931755\n",
      "Cost at iteration 313: 0.45902811739392335\n",
      "Cost at iteration 314: 0.45854536954145514\n",
      "Cost at iteration 315: 0.4580637476924226\n",
      "Cost at iteration 316: 0.45758324816265084\n",
      "Cost at iteration 317: 0.45710386728320485\n",
      "Cost at iteration 318: 0.45662560140031566\n",
      "Cost at iteration 319: 0.4561484468753052\n",
      "Cost at iteration 320: 0.45567240008451354\n",
      "Cost at iteration 321: 0.4551974574192245\n",
      "Cost at iteration 322: 0.45472361528559385\n",
      "Cost at iteration 323: 0.4542508701045758\n",
      "Cost at iteration 324: 0.4537792183118511\n",
      "Cost at iteration 325: 0.45330865635775563\n",
      "Cost at iteration 326: 0.45283918070720813\n",
      "Cost at iteration 327: 0.45237078783964024\n",
      "Cost at iteration 328: 0.4519034742489249\n",
      "Cost at iteration 329: 0.45143723644330663\n",
      "Cost at iteration 330: 0.45097207094533154\n",
      "Cost at iteration 331: 0.4505079742917778\n",
      "Cost at iteration 332: 0.4500449430335863\n",
      "Cost at iteration 333: 0.4495829737357922\n",
      "Cost at iteration 334: 0.4491220629774564\n",
      "Cost at iteration 335: 0.44866220735159745\n",
      "Cost at iteration 336: 0.44820340346512416\n",
      "Cost at iteration 337: 0.44774564793876787\n",
      "Cost at iteration 338: 0.44728893740701575\n",
      "Cost at iteration 339: 0.44683326851804434\n",
      "Cost at iteration 340: 0.44637863793365323\n",
      "Cost at iteration 341: 0.4459250423291991\n",
      "Cost at iteration 342: 0.44547247839353044\n",
      "Cost at iteration 343: 0.44502094282892213\n",
      "Cost at iteration 344: 0.44457043235101096\n",
      "Cost at iteration 345: 0.44412094368873073\n",
      "Cost at iteration 346: 0.4436724735842489\n",
      "Cost at iteration 347: 0.4432250187929018\n",
      "Cost at iteration 348: 0.4427785760831325\n",
      "Cost at iteration 349: 0.4423331422364263\n",
      "Cost at iteration 350: 0.4418887140472491\n",
      "Cost at iteration 351: 0.4414452883229849\n",
      "Cost at iteration 352: 0.44100286188387294\n",
      "Cost at iteration 353: 0.44056143156294697\n",
      "Cost at iteration 354: 0.44012099420597317\n",
      "Cost at iteration 355: 0.4396815466713896\n",
      "Cost at iteration 356: 0.43924308583024513\n",
      "Cost at iteration 357: 0.4388056085661389\n",
      "Cost at iteration 358: 0.4383691117751611\n",
      "Cost at iteration 359: 0.43793359236583185\n",
      "Cost at iteration 360: 0.4374990472590431\n",
      "Cost at iteration 361: 0.43706547338799884\n",
      "Cost at iteration 362: 0.4366328676981563\n",
      "Cost at iteration 363: 0.43620122714716775\n",
      "Cost at iteration 364: 0.4357705487048222\n",
      "Cost at iteration 365: 0.4353408293529874\n",
      "Cost at iteration 366: 0.4349120660855528\n",
      "Cost at iteration 367: 0.4344842559083714\n",
      "Cost at iteration 368: 0.43405739583920433\n",
      "Cost at iteration 369: 0.4336314829076625\n",
      "Cost at iteration 370: 0.43320651415515177\n",
      "Cost at iteration 371: 0.4327824866348159\n",
      "Cost at iteration 372: 0.4323593974114814\n",
      "Cost at iteration 373: 0.4319372435616022\n",
      "Cost at iteration 374: 0.431516022173204\n",
      "Cost at iteration 375: 0.4310957303458302\n",
      "Cost at iteration 376: 0.4306763651904868\n",
      "Cost at iteration 377: 0.43025792382958833\n",
      "Cost at iteration 378: 0.4298404033969042\n",
      "Cost at iteration 379: 0.42942380103750466\n",
      "Cost at iteration 380: 0.42900811390770766\n",
      "Cost at iteration 381: 0.4285933391750258\n",
      "Cost at iteration 382: 0.4281794740181135\n",
      "Cost at iteration 383: 0.4277665156267145\n",
      "Cost at iteration 384: 0.42735446120160975\n",
      "Cost at iteration 385: 0.4269433079545653\n",
      "Cost at iteration 386: 0.4265330531082806\n",
      "Cost at iteration 387: 0.42612369389633736\n",
      "Cost at iteration 388: 0.42571522756314845\n",
      "Cost at iteration 389: 0.42530765136390686\n",
      "Cost at iteration 390: 0.42490096256453513\n",
      "Cost at iteration 391: 0.42449515844163516\n",
      "Cost at iteration 392: 0.4240902362824384\n",
      "Cost at iteration 393: 0.42368619338475566\n",
      "Cost at iteration 394: 0.42328302705692794\n",
      "Cost at iteration 395: 0.4228807346177771\n",
      "Cost at iteration 396: 0.4224793133965568\n",
      "Cost at iteration 397: 0.42207876073290407\n",
      "Cost at iteration 398: 0.4216790739767906\n",
      "Cost at iteration 399: 0.42128025048847473\n",
      "Cost at iteration 400: 0.4208822876384535\n",
      "Cost at iteration 401: 0.42048518280741476\n",
      "Cost at iteration 402: 0.4200889333861901\n",
      "Cost at iteration 403: 0.4196935367757075\n",
      "Cost at iteration 404: 0.4192989903869443\n",
      "Cost at iteration 405: 0.4189052916408808\n",
      "Cost at iteration 406: 0.41851243796845383\n",
      "Cost at iteration 407: 0.41812042681051015\n",
      "Cost at iteration 408: 0.4177292556177612\n",
      "Cost at iteration 409: 0.41733892185073673\n",
      "Cost at iteration 410: 0.4169494229797399\n",
      "Cost at iteration 411: 0.41656075648480206\n",
      "Cost at iteration 412: 0.4161729198556371\n",
      "Cost at iteration 413: 0.4157859105915977\n",
      "Cost at iteration 414: 0.4153997262016303\n",
      "Cost at iteration 415: 0.4150143642042309\n",
      "Cost at iteration 416: 0.4146298221274009\n",
      "Cost at iteration 417: 0.41424609750860397\n",
      "Cost at iteration 418: 0.41386318789472165\n",
      "Cost at iteration 419: 0.4134810908420107\n",
      "Cost at iteration 420: 0.4130998039160595\n",
      "Cost at iteration 421: 0.41271932469174555\n",
      "Cost at iteration 422: 0.41233965075319295\n",
      "Cost at iteration 423: 0.4119607796937294\n",
      "Cost at iteration 424: 0.41158270911584455\n",
      "Cost at iteration 425: 0.4112054366311479\n",
      "Cost at iteration 426: 0.41082895986032675\n",
      "Cost at iteration 427: 0.41045327643310536\n",
      "Cost at iteration 428: 0.4100783839882029\n",
      "Cost at iteration 429: 0.4097042801732929\n",
      "Cost at iteration 430: 0.409330962644962\n",
      "Cost at iteration 431: 0.4089584290686695\n",
      "Cost at iteration 432: 0.4085866771187071\n",
      "Cost at iteration 433: 0.4082157044781581\n",
      "Cost at iteration 434: 0.4078455088388579\n",
      "Cost at iteration 435: 0.40747608790135414\n",
      "Cost at iteration 436: 0.4071074393748668\n",
      "Cost at iteration 437: 0.4067395609772491\n",
      "Cost at iteration 438: 0.4063724504349481\n",
      "Cost at iteration 439: 0.4060061054829658\n",
      "Cost at iteration 440: 0.40564052386482047\n",
      "Cost at iteration 441: 0.405275703332508\n",
      "Cost at iteration 442: 0.40491164164646337\n",
      "Cost at iteration 443: 0.40454833657552275\n",
      "Cost at iteration 444: 0.4041857858968852\n",
      "Cost at iteration 445: 0.40382398739607533\n",
      "Cost at iteration 446: 0.4034629388669053\n",
      "Cost at iteration 447: 0.4031026381114377\n",
      "Cost at iteration 448: 0.4027430829399481\n",
      "Cost at iteration 449: 0.40238427117088826\n",
      "Cost at iteration 450: 0.40202620063084965\n",
      "Cost at iteration 451: 0.4016688691545259\n",
      "Cost at iteration 452: 0.40131227458467733\n",
      "Cost at iteration 453: 0.40095641477209415\n",
      "Cost at iteration 454: 0.40060128757556085\n",
      "Cost at iteration 455: 0.4002468908618199\n",
      "Cost at iteration 456: 0.39989322250553627\n",
      "Cost at iteration 457: 0.3995402803892619\n",
      "Cost at iteration 458: 0.39918806240340077\n",
      "Cost at iteration 459: 0.39883656644617305\n",
      "Cost at iteration 460: 0.3984857904235807\n",
      "Cost at iteration 461: 0.3981357322493727\n",
      "Cost at iteration 462: 0.3977863898450103\n",
      "Cost at iteration 463: 0.3974377611396326\n",
      "Cost at iteration 464: 0.3970898440700224\n",
      "Cost at iteration 465: 0.39674263658057213\n",
      "Cost at iteration 466: 0.3963961366232503\n",
      "Cost at iteration 467: 0.39605034215756696\n",
      "Cost at iteration 468: 0.3957052511505412\n",
      "Cost at iteration 469: 0.39536086157666717\n",
      "Cost at iteration 470: 0.39501717141788084\n",
      "Cost at iteration 471: 0.3946741786635276\n",
      "Cost at iteration 472: 0.39433188131032876\n",
      "Cost at iteration 473: 0.39399027736234915\n",
      "Cost at iteration 474: 0.39364936483096485\n",
      "Cost at iteration 475: 0.39330914173483034\n",
      "Cost at iteration 476: 0.3929696060998467\n",
      "Cost at iteration 477: 0.3926307559591297\n",
      "Cost at iteration 478: 0.3922925893529775\n",
      "Cost at iteration 479: 0.39195510432883945\n",
      "Cost at iteration 480: 0.39161829894128436\n",
      "Cost at iteration 481: 0.39128217125196896\n",
      "Cost at iteration 482: 0.3909467193296072\n",
      "Cost at iteration 483: 0.3906119412499387\n",
      "Cost at iteration 484: 0.3902778350956983\n",
      "Cost at iteration 485: 0.3899443989565848\n",
      "Cost at iteration 486: 0.3896116309292311\n",
      "Cost at iteration 487: 0.3892795291171733\n",
      "Cost at iteration 488: 0.38894809163082034\n",
      "Cost at iteration 489: 0.38861731658742454\n",
      "Cost at iteration 490: 0.38828720211105083\n",
      "Cost at iteration 491: 0.38795774633254776\n",
      "Cost at iteration 492: 0.3876289473895172\n",
      "Cost at iteration 493: 0.3873008034262852\n",
      "Cost at iteration 494: 0.38697331259387263\n",
      "Cost at iteration 495: 0.38664647304996586\n",
      "Cost at iteration 496: 0.3863202829588877\n",
      "Cost at iteration 497: 0.38599474049156873\n",
      "Cost at iteration 498: 0.3856698438255184\n",
      "Cost at iteration 499: 0.3853455911447962\n",
      "Cost at iteration 500: 0.3850219806399838\n",
      "Cost at iteration 501: 0.3846990105081559\n",
      "Cost at iteration 502: 0.38437667895285266\n",
      "Cost at iteration 503: 0.3840549841840518\n",
      "Cost at iteration 504: 0.3837339244181401\n",
      "Cost at iteration 505: 0.3834134978778859\n",
      "Cost at iteration 506: 0.3830937027924119\n",
      "Cost at iteration 507: 0.38277453739716694\n",
      "Cost at iteration 508: 0.38245599993389934\n",
      "Cost at iteration 509: 0.38213808865062937\n",
      "Cost at iteration 510: 0.38182080180162214\n",
      "Cost at iteration 511: 0.38150413764736085\n",
      "Cost at iteration 512: 0.38118809445451984\n",
      "Cost at iteration 513: 0.38087267049593837\n",
      "Cost at iteration 514: 0.3805578640505933\n",
      "Cost at iteration 515: 0.3802436734035731\n",
      "Cost at iteration 516: 0.3799300968460523\n",
      "Cost at iteration 517: 0.3796171326752639\n",
      "Cost at iteration 518: 0.3793047791944748\n",
      "Cost at iteration 519: 0.3789930347129589\n",
      "Cost at iteration 520: 0.3786818975459719\n",
      "Cost at iteration 521: 0.3783713660147258\n",
      "Cost at iteration 522: 0.37806143844636275\n",
      "Cost at iteration 523: 0.3777521131739305\n",
      "Cost at iteration 524: 0.3774433885363565\n",
      "Cost at iteration 525: 0.3771352628784234\n",
      "Cost at iteration 526: 0.3768277345507436\n",
      "Cost at iteration 527: 0.3765208019097345\n",
      "Cost at iteration 528: 0.3762144633175941\n",
      "Cost at iteration 529: 0.3759087171422758\n",
      "Cost at iteration 530: 0.3756035617574645\n",
      "Cost at iteration 531: 0.37529899554255186\n",
      "Cost at iteration 532: 0.3749950168826122\n",
      "Cost at iteration 533: 0.3746916241683783\n",
      "Cost at iteration 534: 0.3743888157962177\n",
      "Cost at iteration 535: 0.37408659016810825\n",
      "Cost at iteration 536: 0.37378494569161475\n",
      "Cost at iteration 537: 0.3734838807798653\n",
      "Cost at iteration 538: 0.3731833938515276\n",
      "Cost at iteration 539: 0.3728834833307857\n",
      "Cost at iteration 540: 0.3725841476473164\n",
      "Cost at iteration 541: 0.37228538523626653\n",
      "Cost at iteration 542: 0.37198719453822937\n",
      "Cost at iteration 543: 0.37168957399922214\n",
      "Cost at iteration 544: 0.371392522070663\n",
      "Cost at iteration 545: 0.37109603720934775\n",
      "Cost at iteration 546: 0.3708001178774285\n",
      "Cost at iteration 547: 0.37050476254238984\n",
      "Cost at iteration 548: 0.37020996967702713\n",
      "Cost at iteration 549: 0.36991573775942405\n",
      "Cost at iteration 550: 0.3696220652729305\n",
      "Cost at iteration 551: 0.3693289507061403\n",
      "Cost at iteration 552: 0.3690363925528696\n",
      "Cost at iteration 553: 0.36874438931213466\n",
      "Cost at iteration 554: 0.3684529394881301\n",
      "Cost at iteration 555: 0.36816204159020777\n",
      "Cost at iteration 556: 0.3678716941328545\n",
      "Cost at iteration 557: 0.3675818956356711\n",
      "Cost at iteration 558: 0.367292644623351\n",
      "Cost at iteration 559: 0.3670039396256587\n",
      "Cost at iteration 560: 0.36671577917740933\n",
      "Cost at iteration 561: 0.36642816181844684\n",
      "Cost at iteration 562: 0.36614108609362367\n",
      "Cost at iteration 563: 0.36585455055277943\n",
      "Cost at iteration 564: 0.3655685537507208\n",
      "Cost at iteration 565: 0.3652830942472001\n",
      "Cost at iteration 566: 0.3649981706068959\n",
      "Cost at iteration 567: 0.36471378139939137\n",
      "Cost at iteration 568: 0.3644299251991548\n",
      "Cost at iteration 569: 0.36414660058551906\n",
      "Cost at iteration 570: 0.3638638061426618\n",
      "Cost at iteration 571: 0.36358154045958463\n",
      "Cost at iteration 572: 0.3632998021300942\n",
      "Cost at iteration 573: 0.36301858975278184\n",
      "Cost at iteration 574: 0.3627379019310037\n",
      "Cost at iteration 575: 0.3624577372728614\n",
      "Cost at iteration 576: 0.3621780943911824\n",
      "Cost at iteration 577: 0.3618989719035005\n",
      "Cost at iteration 578: 0.3616203684320366\n",
      "Cost at iteration 579: 0.36134228260367923\n",
      "Cost at iteration 580: 0.36106471304996546\n",
      "Cost at iteration 581: 0.36078765840706206\n",
      "Cost at iteration 582: 0.3605111173157462\n",
      "Cost at iteration 583: 0.36023508842138646\n",
      "Cost at iteration 584: 0.3599595703739246\n",
      "Cost at iteration 585: 0.35968456182785635\n",
      "Cost at iteration 586: 0.3594100614422125\n",
      "Cost at iteration 587: 0.3591360678805413\n",
      "Cost at iteration 588: 0.35886257981088926\n",
      "Cost at iteration 589: 0.3585895959057829\n",
      "Cost at iteration 590: 0.3583171148422107\n",
      "Cost at iteration 591: 0.35804513530160476\n",
      "Cost at iteration 592: 0.3577736559698228\n",
      "Cost at iteration 593: 0.3575026755371298\n",
      "Cost at iteration 594: 0.35723219269818074\n",
      "Cost at iteration 595: 0.35696220615200197\n",
      "Cost at iteration 596: 0.35669271460197416\n",
      "Cost at iteration 597: 0.3564237167558141\n",
      "Cost at iteration 598: 0.35615521132555755\n",
      "Cost at iteration 599: 0.3558871970275413\n",
      "Cost at iteration 600: 0.3556196725823859\n",
      "Cost at iteration 601: 0.35535263671497863\n",
      "Cost at iteration 602: 0.3550860881544558\n",
      "Cost at iteration 603: 0.3548200256341858\n",
      "Cost at iteration 604: 0.35455444789175194\n",
      "Cost at iteration 605: 0.3542893536689354\n",
      "Cost at iteration 606: 0.3540247417116985\n",
      "Cost at iteration 607: 0.3537606107701677\n",
      "Cost at iteration 608: 0.35349695959861654\n",
      "Cost at iteration 609: 0.35323378695544955\n",
      "Cost at iteration 610: 0.3529710916031853\n",
      "Cost at iteration 611: 0.3527088723084396\n",
      "Cost at iteration 612: 0.3524471278419096\n",
      "Cost at iteration 613: 0.3521858569783568\n",
      "Cost at iteration 614: 0.3519250584965915\n",
      "Cost at iteration 615: 0.35166473117945557\n",
      "Cost at iteration 616: 0.3514048738138074\n",
      "Cost at iteration 617: 0.351145485190505\n",
      "Cost at iteration 618: 0.35088656410439023\n",
      "Cost at iteration 619: 0.350628109354273\n",
      "Cost at iteration 620: 0.35037011974291543\n",
      "Cost at iteration 621: 0.35011259407701595\n",
      "Cost at iteration 622: 0.34985553116719337\n",
      "Cost at iteration 623: 0.3495989298279718\n",
      "Cost at iteration 624: 0.34934278887776465\n",
      "Cost at iteration 625: 0.34908710713885915\n",
      "Cost at iteration 626: 0.3488318834374015\n",
      "Cost at iteration 627: 0.3485771166033806\n",
      "Cost at iteration 628: 0.3483228054706136\n",
      "Cost at iteration 629: 0.3480689488767303\n",
      "Cost at iteration 630: 0.347815545663158\n",
      "Cost at iteration 631: 0.34756259467510664\n",
      "Cost at iteration 632: 0.3473100947615537\n",
      "Cost at iteration 633: 0.34705804477522917\n",
      "Cost at iteration 634: 0.3468064435726011\n",
      "Cost at iteration 635: 0.34655529001386026\n",
      "Cost at iteration 636: 0.3463045829629059\n",
      "Cost at iteration 637: 0.3460543212873306\n",
      "Cost at iteration 638: 0.3458045038584066\n",
      "Cost at iteration 639: 0.34555512955107004\n",
      "Cost at iteration 640: 0.34530619724390754\n",
      "Cost at iteration 641: 0.3450577058191416\n",
      "Cost at iteration 642: 0.34480965416261555\n",
      "Cost at iteration 643: 0.3445620411637807\n",
      "Cost at iteration 644: 0.3443148657156807\n",
      "Cost at iteration 645: 0.3440681267149386\n",
      "Cost at iteration 646: 0.34382182306174186\n",
      "Cost at iteration 647: 0.3435759536598291\n",
      "Cost at iteration 648: 0.3433305174164759\n",
      "Cost at iteration 649: 0.3430855132424807\n",
      "Cost at iteration 650: 0.3428409400521516\n",
      "Cost at iteration 651: 0.34259679676329197\n",
      "Cost at iteration 652: 0.34235308229718714\n",
      "Cost at iteration 653: 0.34210979557859095\n",
      "Cost at iteration 654: 0.34186693553571174\n",
      "Cost at iteration 655: 0.3416245011001994\n",
      "Cost at iteration 656: 0.34138249120713154\n",
      "Cost at iteration 657: 0.34114090479500025\n",
      "Cost at iteration 658: 0.34089974080569924\n",
      "Cost at iteration 659: 0.3406589981845095\n",
      "Cost at iteration 660: 0.3404186758800876\n",
      "Cost at iteration 661: 0.34017877284445136\n",
      "Cost at iteration 662: 0.3399392880329674\n",
      "Cost at iteration 663: 0.339700220404338\n",
      "Cost at iteration 664: 0.3394615689205882\n",
      "Cost at iteration 665: 0.3392233325470528\n",
      "Cost at iteration 666: 0.33898551025236384\n",
      "Cost at iteration 667: 0.3387481010084374\n",
      "Cost at iteration 668: 0.33851110379046134\n",
      "Cost at iteration 669: 0.3382745175768823\n",
      "Cost at iteration 670: 0.33803834134939337\n",
      "Cost at iteration 671: 0.3378025740929215\n",
      "Cost at iteration 672: 0.33756721479561497\n",
      "Cost at iteration 673: 0.33733226244883074\n",
      "Cost at iteration 674: 0.33709771604712285\n",
      "Cost at iteration 675: 0.33686357458822924\n",
      "Cost at iteration 676: 0.3366298370730599\n",
      "Cost at iteration 677: 0.33639650250568487\n",
      "Cost at iteration 678: 0.3361635698933216\n",
      "Cost at iteration 679: 0.33593103824632337\n",
      "Cost at iteration 680: 0.33569890657816687\n",
      "Cost at iteration 681: 0.3354671739054406\n",
      "Cost at iteration 682: 0.33523583924783273\n",
      "Cost at iteration 683: 0.335004901628119\n",
      "Cost at iteration 684: 0.33477436007215133\n",
      "Cost at iteration 685: 0.3345442136088459\n",
      "Cost at iteration 686: 0.3343144612701715\n",
      "Cost at iteration 687: 0.3340851020911377\n",
      "Cost at iteration 688: 0.33385613510978324\n",
      "Cost at iteration 689: 0.33362755936716487\n",
      "Cost at iteration 690: 0.3333993739073452\n",
      "Cost at iteration 691: 0.33317157777738204\n",
      "Cost at iteration 692: 0.3329441700273163\n",
      "Cost at iteration 693: 0.3327171497101609\n",
      "Cost at iteration 694: 0.3324905158818898\n",
      "Cost at iteration 695: 0.33226426760142613\n",
      "Cost at iteration 696: 0.3320384039306315\n",
      "Cost at iteration 697: 0.33181292393429457\n",
      "Cost at iteration 698: 0.3315878266801202\n",
      "Cost at iteration 699: 0.3313631112387181\n",
      "Cost at iteration 700: 0.3311387766835922\n",
      "Cost at iteration 701: 0.33091482209112927\n",
      "Cost at iteration 702: 0.3306912465405885\n",
      "Cost at iteration 703: 0.33046804911409017\n",
      "Cost at iteration 704: 0.33024522889660524\n",
      "Cost at iteration 705: 0.3300227849759443\n",
      "Cost at iteration 706: 0.329800716442747\n",
      "Cost at iteration 707: 0.32957902239047165\n",
      "Cost at iteration 708: 0.32935770191538394\n",
      "Cost at iteration 709: 0.32913675411654714\n",
      "Cost at iteration 710: 0.328916178095811\n",
      "Cost at iteration 711: 0.32869597295780134\n",
      "Cost at iteration 712: 0.3284761378099103\n",
      "Cost at iteration 713: 0.328256671762285\n",
      "Cost at iteration 714: 0.3280375739278177\n",
      "Cost at iteration 715: 0.3278188434221355\n",
      "Cost at iteration 716: 0.32760047936359016\n",
      "Cost at iteration 717: 0.32738248087324745\n",
      "Cost at iteration 718: 0.3271648470748779\n",
      "Cost at iteration 719: 0.3269475770949456\n",
      "Cost at iteration 720: 0.32673067006259887\n",
      "Cost at iteration 721: 0.32651412510966005\n",
      "Cost at iteration 722: 0.3262979413706154\n",
      "Cost at iteration 723: 0.3260821179826054\n",
      "Cost at iteration 724: 0.32586665408541465\n",
      "Cost at iteration 725: 0.3256515488214621\n",
      "Cost at iteration 726: 0.32543680133579117\n",
      "Cost at iteration 727: 0.3252224107760604\n",
      "Cost at iteration 728: 0.32500837629253293\n",
      "Cost at iteration 729: 0.32479469703806757\n",
      "Cost at iteration 730: 0.3245813721681089\n",
      "Cost at iteration 731: 0.32436840084067753\n",
      "Cost at iteration 732: 0.3241557822163609\n",
      "Cost at iteration 733: 0.3239435154583032\n",
      "Cost at iteration 734: 0.3237315997321967\n",
      "Cost at iteration 735: 0.3235200342062716\n",
      "Cost at iteration 736: 0.3233088180512871\n",
      "Cost at iteration 737: 0.32309795044052164\n",
      "Cost at iteration 738: 0.3228874305497641\n",
      "Cost at iteration 739: 0.3226772575573043\n",
      "Cost at iteration 740: 0.3224674306439234\n",
      "Cost at iteration 741: 0.3222579489928857\n",
      "Cost at iteration 742: 0.32204881178992817\n",
      "Cost at iteration 743: 0.32184001822325264\n",
      "Cost at iteration 744: 0.32163156748351573\n",
      "Cost at iteration 745: 0.32142345876382056\n",
      "Cost at iteration 746: 0.32121569125970717\n",
      "Cost at iteration 747: 0.3210082641691441\n",
      "Cost at iteration 748: 0.3208011766925189\n",
      "Cost at iteration 749: 0.32059442803263016\n",
      "Cost at iteration 750: 0.3203880173946775\n",
      "Cost at iteration 751: 0.3201819439862536\n",
      "Cost at iteration 752: 0.3199762070173355\n",
      "Cost at iteration 753: 0.3197708057002751\n",
      "Cost at iteration 754: 0.31956573924979154\n",
      "Cost at iteration 755: 0.31936100688296154\n",
      "Cost at iteration 756: 0.31915660781921157\n",
      "Cost at iteration 757: 0.31895254128030875\n",
      "Cost at iteration 758: 0.31874880649035275\n",
      "Cost at iteration 759: 0.3185454026757669\n",
      "Cost at iteration 760: 0.31834232906529\n",
      "Cost at iteration 761: 0.31813958488996774\n",
      "Cost at iteration 762: 0.31793716938314454\n",
      "Cost at iteration 763: 0.31773508178045473\n",
      "Cost at iteration 764: 0.3175333213198149\n",
      "Cost at iteration 765: 0.31733188724141503\n",
      "Cost at iteration 766: 0.31713077878771057\n",
      "Cost at iteration 767: 0.31692999520341397\n",
      "Cost at iteration 768: 0.3167295357354869\n",
      "Cost at iteration 769: 0.31652939963313176\n",
      "Cost at iteration 770: 0.3163295861477836\n",
      "Cost at iteration 771: 0.3161300945331024\n",
      "Cost at iteration 772: 0.31593092404496453\n",
      "Cost at iteration 773: 0.315732073941455\n",
      "Cost at iteration 774: 0.3155335434828597\n",
      "Cost at iteration 775: 0.31533533193165697\n",
      "Cost at iteration 776: 0.31513743855251014\n",
      "Cost at iteration 777: 0.3149398626122595\n",
      "Cost at iteration 778: 0.3147426033799142\n",
      "Cost at iteration 779: 0.31454566012664514\n",
      "Cost at iteration 780: 0.3143490321257764\n",
      "Cost at iteration 781: 0.3141527186527781\n",
      "Cost at iteration 782: 0.31395671898525834\n",
      "Cost at iteration 783: 0.3137610324029558\n",
      "Cost at iteration 784: 0.31356565818773174\n",
      "Cost at iteration 785: 0.31337059562356273\n",
      "Cost at iteration 786: 0.3131758439965331\n",
      "Cost at iteration 787: 0.31298140259482693\n",
      "Cost at iteration 788: 0.3127872707087212\n",
      "Cost at iteration 789: 0.3125934476305777\n",
      "Cost at iteration 790: 0.3123999326548362\n",
      "Cost at iteration 791: 0.3122067250780063\n",
      "Cost at iteration 792: 0.3120138241986607\n",
      "Cost at iteration 793: 0.3118212293174275\n",
      "Cost at iteration 794: 0.3116289397369829\n",
      "Cost at iteration 795: 0.31143695476204414\n",
      "Cost at iteration 796: 0.3112452736993619\n",
      "Cost at iteration 797: 0.3110538958577132\n",
      "Cost at iteration 798: 0.3108628205478945\n",
      "Cost at iteration 799: 0.31067204708271395\n",
      "Cost at iteration 800: 0.3104815747769847\n",
      "Cost at iteration 801: 0.3102914029475177\n",
      "Cost at iteration 802: 0.3101015309131145\n",
      "Cost at iteration 803: 0.3099119579945601\n",
      "Cost at iteration 804: 0.3097226835146164\n",
      "Cost at iteration 805: 0.3095337067980147\n",
      "Cost at iteration 806: 0.30934502717144885\n",
      "Cost at iteration 807: 0.30915664396356873\n",
      "Cost at iteration 808: 0.3089685565049726\n",
      "Cost at iteration 809: 0.308780764128201\n",
      "Cost at iteration 810: 0.3085932661677291\n",
      "Cost at iteration 811: 0.30840606195996073\n",
      "Cost at iteration 812: 0.3082191508432209\n",
      "Cost at iteration 813: 0.30803253215774923\n",
      "Cost at iteration 814: 0.3078462052456937\n",
      "Cost at iteration 815: 0.30766016945110314\n",
      "Cost at iteration 816: 0.307474424119921\n",
      "Cost at iteration 817: 0.3072889685999786\n",
      "Cost at iteration 818: 0.3071038022409887\n",
      "Cost at iteration 819: 0.3069189243945386\n",
      "Cost at iteration 820: 0.30673433441408365\n",
      "Cost at iteration 821: 0.3065500316549408\n",
      "Cost at iteration 822: 0.3063660154742822\n",
      "Cost at iteration 823: 0.3061822852311283\n",
      "Cost at iteration 824: 0.3059988402863417\n",
      "Cost at iteration 825: 0.3058156800026207\n",
      "Cost at iteration 826: 0.3056328037444928\n",
      "Cost at iteration 827: 0.3054502108783083\n",
      "Cost at iteration 828: 0.30526790077223415\n",
      "Cost at iteration 829: 0.30508587279624727\n",
      "Cost at iteration 830: 0.3049041263221285\n",
      "Cost at iteration 831: 0.3047226607234562\n",
      "Cost at iteration 832: 0.3045414753756002\n",
      "Cost at iteration 833: 0.30436056965571523\n",
      "Cost at iteration 834: 0.3041799429427349\n",
      "Cost at iteration 835: 0.3039995946173657\n",
      "Cost at iteration 836: 0.30381952406208057\n",
      "Cost at iteration 837: 0.3036397306611127\n",
      "Cost at iteration 838: 0.30346021380044996\n",
      "Cost at iteration 839: 0.3032809728678281\n",
      "Cost at iteration 840: 0.30310200725272524\n",
      "Cost at iteration 841: 0.3029233163463556\n",
      "Cost at iteration 842: 0.3027448995416636\n",
      "Cost at iteration 843: 0.30256675623331775\n",
      "Cost at iteration 844: 0.3023888858177049\n",
      "Cost at iteration 845: 0.30221128769292394\n",
      "Cost at iteration 846: 0.30203396125878035\n",
      "Cost at iteration 847: 0.3018569059167802\n",
      "Cost at iteration 848: 0.3016801210701239\n",
      "Cost at iteration 849: 0.3015036061237007\n",
      "Cost at iteration 850: 0.3013273604840831\n",
      "Cost at iteration 851: 0.30115138355952054\n",
      "Cost at iteration 852: 0.300975674759934\n",
      "Cost at iteration 853: 0.30080023349691\n",
      "Cost at iteration 854: 0.3006250591836953\n",
      "Cost at iteration 855: 0.30045015123519075\n",
      "Cost at iteration 856: 0.3002755090679458\n",
      "Cost at iteration 857: 0.3001011321001528\n",
      "Cost at iteration 858: 0.29992701975164165\n",
      "Cost at iteration 859: 0.29975317144387376\n",
      "Cost at iteration 860: 0.2995795865999366\n",
      "Cost at iteration 861: 0.29940626464453857\n",
      "Cost at iteration 862: 0.2992332050040028\n",
      "Cost at iteration 863: 0.299060407106262\n",
      "Cost at iteration 864: 0.29888787038085296\n",
      "Cost at iteration 865: 0.2987155942589111\n",
      "Cost at iteration 866: 0.29854357817316474\n",
      "Cost at iteration 867: 0.29837182155793024\n",
      "Cost at iteration 868: 0.29820032384910583\n",
      "Cost at iteration 869: 0.298029084484167\n",
      "Cost at iteration 870: 0.2978581029021604\n",
      "Cost at iteration 871: 0.2976873785436993\n",
      "Cost at iteration 872: 0.2975169108509575\n",
      "Cost at iteration 873: 0.2973466992676646\n",
      "Cost at iteration 874: 0.29717674323910037\n",
      "Cost at iteration 875: 0.2970070422120896\n",
      "Cost at iteration 876: 0.29683759563499723\n",
      "Cost at iteration 877: 0.29666840295772223\n",
      "Cost at iteration 878: 0.2964994636316934\n",
      "Cost at iteration 879: 0.29633077710986366\n",
      "Cost at iteration 880: 0.2961623428467051\n",
      "Cost at iteration 881: 0.29599416029820347\n",
      "Cost at iteration 882: 0.2958262289218536\n",
      "Cost at iteration 883: 0.29565854817665416\n",
      "Cost at iteration 884: 0.2954911175231023\n",
      "Cost at iteration 885: 0.29532393642318877\n",
      "Cost at iteration 886: 0.29515700434039305\n",
      "Cost at iteration 887: 0.2949903207396782\n",
      "Cost at iteration 888: 0.29482388508748575\n",
      "Cost at iteration 889: 0.294657696851731\n",
      "Cost at iteration 890: 0.2944917555017979\n",
      "Cost at iteration 891: 0.29432606050853405\n",
      "Cost at iteration 892: 0.294160611344246\n",
      "Cost at iteration 893: 0.2939954074826941\n",
      "Cost at iteration 894: 0.29383044839908784\n",
      "Cost at iteration 895: 0.29366573357008097\n",
      "Cost at iteration 896: 0.29350126247376657\n",
      "Cost at iteration 897: 0.29333703458967225\n",
      "Cost at iteration 898: 0.29317304939875544\n",
      "Cost at iteration 899: 0.29300930638339845\n",
      "Cost at iteration 900: 0.292845805027404\n",
      "Cost at iteration 901: 0.29268254481599004\n",
      "Cost at iteration 902: 0.2925195252357855\n",
      "Cost at iteration 903: 0.2923567457748253\n",
      "Cost at iteration 904: 0.2921942059225457\n",
      "Cost at iteration 905: 0.2920319051697796\n",
      "Cost at iteration 906: 0.29186984300875224\n",
      "Cost at iteration 907: 0.291708018933076\n",
      "Cost at iteration 908: 0.29154643243774614\n",
      "Cost at iteration 909: 0.29138508301913624\n",
      "Cost at iteration 910: 0.2912239701749934\n",
      "Cost at iteration 911: 0.2910630934044338\n",
      "Cost at iteration 912: 0.2909024522079383\n",
      "Cost at iteration 913: 0.2907420460873478\n",
      "Cost at iteration 914: 0.2905818745458585\n",
      "Cost at iteration 915: 0.2904219370880178\n",
      "Cost at iteration 916: 0.2902622332197196\n",
      "Cost at iteration 917: 0.29010276244819994\n",
      "Cost at iteration 918: 0.28994352428203257\n",
      "Cost at iteration 919: 0.2897845182311244\n",
      "Cost at iteration 920: 0.28962574380671124\n",
      "Cost at iteration 921: 0.2894672005213534\n",
      "Cost at iteration 922: 0.2893088878889312\n",
      "Cost at iteration 923: 0.28915080542464083\n",
      "Cost at iteration 924: 0.28899295264498986\n",
      "Cost at iteration 925: 0.28883532906779297\n",
      "Cost at iteration 926: 0.2886779342121674\n",
      "Cost at iteration 927: 0.2885207675985294\n",
      "Cost at iteration 928: 0.28836382874858907\n",
      "Cost at iteration 929: 0.2882071171853467\n",
      "Cost at iteration 930: 0.2880506324330883\n",
      "Cost at iteration 931: 0.28789437401738144\n",
      "Cost at iteration 932: 0.287738341465071\n",
      "Cost at iteration 933: 0.28758253430427505\n",
      "Cost at iteration 934: 0.28742695206438074\n",
      "Cost at iteration 935: 0.28727159427604\n",
      "Cost at iteration 936: 0.2871164604711654\n",
      "Cost at iteration 937: 0.2869615501829262\n",
      "Cost at iteration 938: 0.2868068629457442\n",
      "Cost at iteration 939: 0.2866523982952895\n",
      "Cost at iteration 940: 0.2864981557684764\n",
      "Cost at iteration 941: 0.2863441349034598\n",
      "Cost at iteration 942: 0.2861903352396305\n",
      "Cost at iteration 943: 0.2860367563176116\n",
      "Cost at iteration 944: 0.2858833976792546\n",
      "Cost at iteration 945: 0.2857302588676348\n",
      "Cost at iteration 946: 0.285577339427048\n",
      "Cost at iteration 947: 0.28542463890300623\n",
      "Cost at iteration 948: 0.2852721568422336\n",
      "Cost at iteration 949: 0.28511989279266275\n",
      "Cost at iteration 950: 0.28496784630343086\n",
      "Cost at iteration 951: 0.2848160169248754\n",
      "Cost at iteration 952: 0.2846644042085306\n",
      "Cost at iteration 953: 0.2845130077071235\n",
      "Cost at iteration 954: 0.2843618269745701\n",
      "Cost at iteration 955: 0.2842108615659711\n",
      "Cost at iteration 956: 0.284060111037609\n",
      "Cost at iteration 957: 0.28390957494694324\n",
      "Cost at iteration 958: 0.28375925285260706\n",
      "Cost at iteration 959: 0.28360914431440365\n",
      "Cost at iteration 960: 0.28345924889330193\n",
      "Cost at iteration 961: 0.2833095661514335\n",
      "Cost at iteration 962: 0.2831600956520881\n",
      "Cost at iteration 963: 0.2830108369597107\n",
      "Cost at iteration 964: 0.28286178963989705\n",
      "Cost at iteration 965: 0.2827129532593905\n",
      "Cost at iteration 966: 0.28256432738607795\n",
      "Cost at iteration 967: 0.2824159115889864\n",
      "Cost at iteration 968: 0.28226770543827934\n",
      "Cost at iteration 969: 0.2821197085052528\n",
      "Cost at iteration 970: 0.2819719203623321\n",
      "Cost at iteration 971: 0.2818243405830678\n",
      "Cost at iteration 972: 0.2816769687421326\n",
      "Cost at iteration 973: 0.2815298044153174\n",
      "Cost at iteration 974: 0.28138284717952744\n",
      "Cost at iteration 975: 0.28123609661277976\n",
      "Cost at iteration 976: 0.2810895522941985\n",
      "Cost at iteration 977: 0.280943213804012\n",
      "Cost at iteration 978: 0.28079708072354903\n",
      "Cost at iteration 979: 0.2806511526352355\n",
      "Cost at iteration 980: 0.2805054291225908\n",
      "Cost at iteration 981: 0.28035990977022424\n",
      "Cost at iteration 982: 0.28021459416383193\n",
      "Cost at iteration 983: 0.28006948189019276\n",
      "Cost at iteration 984: 0.2799245725371656\n",
      "Cost at iteration 985: 0.27977986569368507\n",
      "Cost at iteration 986: 0.27963536094975905\n",
      "Cost at iteration 987: 0.2794910578964648\n",
      "Cost at iteration 988: 0.2793469561259451\n",
      "Cost at iteration 989: 0.27920305523140587\n",
      "Cost at iteration 990: 0.27905935480711197\n",
      "Cost at iteration 991: 0.2789158544483842\n",
      "Cost at iteration 992: 0.2787725537515959\n",
      "Cost at iteration 993: 0.27862945231416975\n",
      "Cost at iteration 994: 0.27848654973457415\n",
      "Cost at iteration 995: 0.27834384561232006\n",
      "Cost at iteration 996: 0.27820133954795795\n",
      "Cost at iteration 997: 0.2780590311430741\n",
      "Cost at iteration 998: 0.2779169200002875\n",
      "Cost at iteration 999: 0.2777750057232467\n",
      "Cost at iteration 1000: 0.2776332879166265\n",
      "Cost at iteration 1001: 0.27749176618612453\n",
      "Cost at iteration 1002: 0.27735044013845855\n",
      "Cost at iteration 1003: 0.2772093093813624\n",
      "Cost at iteration 1004: 0.27706837352358366\n",
      "Cost at iteration 1005: 0.2769276321748799\n",
      "Cost at iteration 1006: 0.2767870849460158\n",
      "Cost at iteration 1007: 0.2766467314487597\n",
      "Cost at iteration 1008: 0.2765065712958808\n",
      "Cost at iteration 1009: 0.2763666041011458\n",
      "Cost at iteration 1010: 0.2762268294793157\n",
      "Cost at iteration 1011: 0.27608724704614307\n",
      "Cost at iteration 1012: 0.2759478564183684\n",
      "Cost at iteration 1013: 0.27580865721371745\n",
      "Cost at iteration 1014: 0.27566964905089786\n",
      "Cost at iteration 1015: 0.27553083154959646\n",
      "Cost at iteration 1016: 0.27539220433047557\n",
      "Cost at iteration 1017: 0.27525376701517074\n",
      "Cost at iteration 1018: 0.2751155192262872\n",
      "Cost at iteration 1019: 0.27497746058739686\n",
      "Cost at iteration 1020: 0.2748395907230355\n",
      "Cost at iteration 1021: 0.27470190925869964\n",
      "Cost at iteration 1022: 0.27456441582084345\n",
      "Cost at iteration 1023: 0.27442711003687614\n",
      "Cost at iteration 1024: 0.2742899915351584\n",
      "Cost at iteration 1025: 0.2741530599450001\n",
      "Cost at iteration 1026: 0.27401631489665673\n",
      "Cost at iteration 1027: 0.2738797560213271\n",
      "Cost at iteration 1028: 0.2737433829511495\n",
      "Cost at iteration 1029: 0.27360719531919997\n",
      "Cost at iteration 1030: 0.27347119275948834\n",
      "Cost at iteration 1031: 0.2733353749069558\n",
      "Cost at iteration 1032: 0.27319974139747216\n",
      "Cost at iteration 1033: 0.2730642918678328\n",
      "Cost at iteration 1034: 0.2729290259557556\n",
      "Cost at iteration 1035: 0.2727939432998782\n",
      "Cost at iteration 1036: 0.27265904353975556\n",
      "Cost at iteration 1037: 0.2725243263158567\n",
      "Cost at iteration 1038: 0.2723897912695618\n",
      "Cost at iteration 1039: 0.27225543804315966\n",
      "Cost at iteration 1040: 0.27212126627984495\n",
      "Cost at iteration 1041: 0.27198727562371516\n",
      "Cost at iteration 1042: 0.2718534657197679\n",
      "Cost at iteration 1043: 0.2717198362138981\n",
      "Cost at iteration 1044: 0.2715863867528957\n",
      "Cost at iteration 1045: 0.2714531169844418\n",
      "Cost at iteration 1046: 0.27132002655710735\n",
      "Cost at iteration 1047: 0.2711871151203492\n",
      "Cost at iteration 1048: 0.2710543823245082\n",
      "Cost at iteration 1049: 0.27092182782080587\n",
      "Cost at iteration 1050: 0.2707894512613421\n",
      "Cost at iteration 1051: 0.2706572522990925\n",
      "Cost at iteration 1052: 0.2705252305879053\n",
      "Cost at iteration 1053: 0.2703933857824992\n",
      "Cost at iteration 1054: 0.27026171753846007\n",
      "Cost at iteration 1055: 0.27013022551223914\n",
      "Cost at iteration 1056: 0.2699989093611495\n",
      "Cost at iteration 1057: 0.2698677687433641\n",
      "Cost at iteration 1058: 0.2697368033179126\n",
      "Cost at iteration 1059: 0.2696060127446793\n",
      "Cost at iteration 1060: 0.2694753966844003\n",
      "Cost at iteration 1061: 0.2693449547986604\n",
      "Cost at iteration 1062: 0.26921468674989146\n",
      "Cost at iteration 1063: 0.2690845922013692\n",
      "Cost at iteration 1064: 0.2689546708172107\n",
      "Cost at iteration 1065: 0.268824922262372\n",
      "Cost at iteration 1066: 0.2686953462026452\n",
      "Cost at iteration 1067: 0.2685659423046565\n",
      "Cost at iteration 1068: 0.2684367102358633\n",
      "Cost at iteration 1069: 0.2683076496645515\n",
      "Cost at iteration 1070: 0.26817876025983334\n",
      "Cost at iteration 1071: 0.2680500416916447\n",
      "Cost at iteration 1072: 0.2679214936307428\n",
      "Cost at iteration 1073: 0.2677931157487035\n",
      "Cost at iteration 1074: 0.26766490771791873\n",
      "Cost at iteration 1075: 0.2675368692115943\n",
      "Cost at iteration 1076: 0.2674089999037475\n",
      "Cost at iteration 1077: 0.2672812994692041\n",
      "Cost at iteration 1078: 0.26715376758359666\n",
      "Cost at iteration 1079: 0.2670264039233614\n",
      "Cost at iteration 1080: 0.26689920816573626\n",
      "Cost at iteration 1081: 0.26677217998875813\n",
      "Cost at iteration 1082: 0.26664531907126077\n",
      "Cost at iteration 1083: 0.2665186250928723\n",
      "Cost at iteration 1084: 0.2663920977340125\n",
      "Cost at iteration 1085: 0.266265736675891\n",
      "Cost at iteration 1086: 0.26613954160050435\n",
      "Cost at iteration 1087: 0.26601351219063407\n",
      "Cost at iteration 1088: 0.26588764812984383\n",
      "Cost at iteration 1089: 0.26576194910247786\n",
      "Cost at iteration 1090: 0.2656364147936575\n",
      "Cost at iteration 1091: 0.2655110448892802\n",
      "Cost at iteration 1092: 0.26538583907601593\n",
      "Cost at iteration 1093: 0.26526079704130573\n",
      "Cost at iteration 1094: 0.26513591847335893\n",
      "Cost at iteration 1095: 0.2650112030611511\n",
      "Cost at iteration 1096: 0.26488665049442167\n",
      "Cost at iteration 1097: 0.26476226046367163\n",
      "Cost at iteration 1098: 0.2646380326601612\n",
      "Cost at iteration 1099: 0.2645139667759079\n",
      "Cost at iteration 1100: 0.2643900625036835\n",
      "Cost at iteration 1101: 0.26426631953701285\n",
      "Cost at iteration 1102: 0.26414273757017076\n",
      "Cost at iteration 1103: 0.2640193162981802\n",
      "Cost at iteration 1104: 0.2638960554168098\n",
      "Cost at iteration 1105: 0.26377295462257194\n",
      "Cost at iteration 1106: 0.2636500136127203\n",
      "Cost at iteration 1107: 0.26352723208524753\n",
      "Cost at iteration 1108: 0.26340460973888374\n",
      "Cost at iteration 1109: 0.26328214627309326\n",
      "Cost at iteration 1110: 0.26315984138807336\n",
      "Cost at iteration 1111: 0.26303769478475164\n",
      "Cost at iteration 1112: 0.2629157061647839\n",
      "Cost at iteration 1113: 0.26279387523055214\n",
      "Cost at iteration 1114: 0.2626722016851622\n",
      "Cost at iteration 1115: 0.2625506852324418\n",
      "Cost at iteration 1116: 0.26242932557693804\n",
      "Cost at iteration 1117: 0.262308122423916\n",
      "Cost at iteration 1118: 0.2621870754793558\n",
      "Cost at iteration 1119: 0.26206618444995106\n",
      "Cost at iteration 1120: 0.2619454490431064\n",
      "Cost at iteration 1121: 0.2618248689669357\n",
      "Cost at iteration 1122: 0.2617044439302597\n",
      "Cost at iteration 1123: 0.261584173642604\n",
      "Cost at iteration 1124: 0.2614640578141972\n",
      "Cost at iteration 1125: 0.2613440961559685\n",
      "Cost at iteration 1126: 0.26122428837954587\n",
      "Cost at iteration 1127: 0.26110463419725377\n",
      "Cost at iteration 1128: 0.2609851333221114\n",
      "Cost at iteration 1129: 0.2608657854678304\n",
      "Cost at iteration 1130: 0.26074659034881303\n",
      "Cost at iteration 1131: 0.2606275476801498\n",
      "Cost at iteration 1132: 0.26050865717761795\n",
      "Cost at iteration 1133: 0.26038991855767873\n",
      "Cost at iteration 1134: 0.2602713315374764\n",
      "Cost at iteration 1135: 0.26015289583483514\n",
      "Cost at iteration 1136: 0.2600346111682578\n",
      "Cost at iteration 1137: 0.25991647725692363\n",
      "Cost at iteration 1138: 0.2597984938206863\n",
      "Cost at iteration 1139: 0.259680660580072\n",
      "Cost at iteration 1140: 0.25956297725627747\n",
      "Cost at iteration 1141: 0.259445443571168\n",
      "Cost at iteration 1142: 0.2593280592472755\n",
      "Cost at iteration 1143: 0.25921082400779644\n",
      "Cost at iteration 1144: 0.2590937375765901\n",
      "Cost at iteration 1145: 0.2589767996781766\n",
      "Cost at iteration 1146: 0.2588600100377348\n",
      "Cost at iteration 1147: 0.2587433683811007\n",
      "Cost at iteration 1148: 0.258626874434765\n",
      "Cost at iteration 1149: 0.25851052792587187\n",
      "Cost at iteration 1150: 0.2583943285822165\n",
      "Cost at iteration 1151: 0.2582782761322434\n",
      "Cost at iteration 1152: 0.25816237030504474\n",
      "Cost at iteration 1153: 0.258046610830358\n",
      "Cost at iteration 1154: 0.2579309974385644\n",
      "Cost at iteration 1155: 0.25781552986068723\n",
      "Cost at iteration 1156: 0.25770020782838926\n",
      "Cost at iteration 1157: 0.2575850310739718\n",
      "Cost at iteration 1158: 0.2574699993303721\n",
      "Cost at iteration 1159: 0.2573551123311621\n",
      "Cost at iteration 1160: 0.25724036981054615\n",
      "Cost at iteration 1161: 0.2571257715033594\n",
      "Cost at iteration 1162: 0.25701131714506587\n",
      "Cost at iteration 1163: 0.2568970064717567\n",
      "Cost at iteration 1164: 0.25678283922014816\n",
      "Cost at iteration 1165: 0.25666881512758044\n",
      "Cost at iteration 1166: 0.25655493393201506\n",
      "Cost at iteration 1167: 0.25644119537203347\n",
      "Cost at iteration 1168: 0.25632759918683534\n",
      "Cost at iteration 1169: 0.25621414511623664\n",
      "Cost at iteration 1170: 0.25610083290066776\n",
      "Cost at iteration 1171: 0.255987662281172\n",
      "Cost at iteration 1172: 0.25587463299940366\n",
      "Cost at iteration 1173: 0.2557617447976263\n",
      "Cost at iteration 1174: 0.255648997418711\n",
      "Cost at iteration 1175: 0.25553639060613453\n",
      "Cost at iteration 1176: 0.2554239241039778\n",
      "Cost at iteration 1177: 0.255311597656924\n",
      "Cost at iteration 1178: 0.25519941101025667\n",
      "Cost at iteration 1179: 0.2550873639098586\n",
      "Cost at iteration 1180: 0.2549754561022094\n",
      "Cost at iteration 1181: 0.2548636873343841\n",
      "Cost at iteration 1182: 0.25475205735405154\n",
      "Cost at iteration 1183: 0.2546405659094727\n",
      "Cost at iteration 1184: 0.25452921274949863\n",
      "Cost at iteration 1185: 0.2544179976235691\n",
      "Cost at iteration 1186: 0.254306920281711\n",
      "Cost at iteration 1187: 0.25419598047453623\n",
      "Cost at iteration 1188: 0.25408517795324054\n",
      "Cost at iteration 1189: 0.25397451246960157\n",
      "Cost at iteration 1190: 0.253863983775977\n",
      "Cost at iteration 1191: 0.2537535916253036\n",
      "Cost at iteration 1192: 0.2536433357710946\n",
      "Cost at iteration 1193: 0.25353321596743905\n",
      "Cost at iteration 1194: 0.2534232319689994\n",
      "Cost at iteration 1195: 0.25331338353101024\n",
      "Cost at iteration 1196: 0.25320367040927677\n",
      "Cost at iteration 1197: 0.2530940923601728\n",
      "Cost at iteration 1198: 0.2529846491406394\n",
      "Cost at iteration 1199: 0.2528753405081834\n",
      "Cost at iteration 1200: 0.2527661662208754\n",
      "Cost at iteration 1201: 0.25265712603734863\n",
      "Cost at iteration 1202: 0.2525482197167969\n",
      "Cost at iteration 1203: 0.2524394470189734\n",
      "Cost at iteration 1204: 0.25233080770418903\n",
      "Cost at iteration 1205: 0.2522223015333104\n",
      "Cost at iteration 1206: 0.2521139282677589\n",
      "Cost at iteration 1207: 0.2520056876695089\n",
      "Cost at iteration 1208: 0.2518975795010859\n",
      "Cost at iteration 1209: 0.25178960352556534\n",
      "Cost at iteration 1210: 0.25168175950657085\n",
      "Cost at iteration 1211: 0.25157404720827276\n",
      "Cost at iteration 1212: 0.25146646639538656\n",
      "Cost at iteration 1213: 0.2513590168331714\n",
      "Cost at iteration 1214: 0.25125169828742855\n",
      "Cost at iteration 1215: 0.2511445105244998\n",
      "Cost at iteration 1216: 0.25103745331126603\n",
      "Cost at iteration 1217: 0.25093052641514546\n",
      "Cost at iteration 1218: 0.2508237296040926\n",
      "Cost at iteration 1219: 0.2507170626465963\n",
      "Cost at iteration 1220: 0.2506105253116786\n",
      "Cost at iteration 1221: 0.25050411736889283\n",
      "Cost at iteration 1222: 0.2503978385883225\n",
      "Cost at iteration 1223: 0.25029168874057955\n",
      "Cost at iteration 1224: 0.25018566759680305\n",
      "Cost at iteration 1225: 0.2500797749286577\n",
      "Cost at iteration 1226: 0.24997401050833223\n",
      "Cost at iteration 1227: 0.24986837410853802\n",
      "Cost at iteration 1228: 0.24976286550250765\n",
      "Cost at iteration 1229: 0.24965748446399347\n",
      "Cost at iteration 1230: 0.24955223076726601\n",
      "Cost at iteration 1231: 0.2494471041871127\n",
      "Cost at iteration 1232: 0.24934210449883626\n",
      "Cost at iteration 1233: 0.24923723147825352\n",
      "Cost at iteration 1234: 0.24913248490169365\n",
      "Cost at iteration 1235: 0.24902786454599699\n",
      "Cost at iteration 1236: 0.24892337018851363\n",
      "Cost at iteration 1237: 0.24881900160710163\n",
      "Cost at iteration 1238: 0.24871475858012618\n",
      "Cost at iteration 1239: 0.24861064088645768\n",
      "Cost at iteration 1240: 0.2485066483054706\n",
      "Cost at iteration 1241: 0.24840278061704205\n",
      "Cost at iteration 1242: 0.24829903760155037\n",
      "Cost at iteration 1243: 0.24819541903987363\n",
      "Cost at iteration 1244: 0.2480919247133885\n",
      "Cost at iteration 1245: 0.2479885544039685\n",
      "Cost at iteration 1246: 0.24788530789398294\n",
      "Cost at iteration 1247: 0.24778218496629542\n",
      "Cost at iteration 1248: 0.2476791854042625\n",
      "Cost at iteration 1249: 0.24757630899173222\n",
      "Cost at iteration 1250: 0.24747355551304295\n",
      "Cost at iteration 1251: 0.24737092475302183\n",
      "Cost at iteration 1252: 0.2472684164969835\n",
      "Cost at iteration 1253: 0.24716603053072878\n",
      "Cost at iteration 1254: 0.2470637666405433\n",
      "Cost at iteration 1255: 0.2469616246131962\n",
      "Cost at iteration 1256: 0.24685960423593872\n",
      "Cost at iteration 1257: 0.24675770529650287\n",
      "Cost at iteration 1258: 0.2466559275831002\n",
      "Cost at iteration 1259: 0.24655427088442047\n",
      "Cost at iteration 1260: 0.24645273498963027\n",
      "Cost at iteration 1261: 0.24635131968837157\n",
      "Cost at iteration 1262: 0.24625002477076083\n",
      "Cost at iteration 1263: 0.2461488500273872\n",
      "Cost at iteration 1264: 0.24604779524931158\n",
      "Cost at iteration 1265: 0.24594686022806525\n",
      "Cost at iteration 1266: 0.2458460447556484\n",
      "Cost at iteration 1267: 0.245745348624529\n",
      "Cost at iteration 1268: 0.24564477162764156\n",
      "Cost at iteration 1269: 0.2455443135583857\n",
      "Cost at iteration 1270: 0.24544397421062508\n",
      "Cost at iteration 1271: 0.24534375337868586\n",
      "Cost at iteration 1272: 0.2452436508573556\n",
      "Cost at iteration 1273: 0.24514366644188212\n",
      "Cost at iteration 1274: 0.24504379992797198\n",
      "Cost at iteration 1275: 0.24494405111178932\n",
      "Cost at iteration 1276: 0.24484441978995475\n",
      "Cost at iteration 1277: 0.24474490575954383\n",
      "Cost at iteration 1278: 0.2446455088180862\n",
      "Cost at iteration 1279: 0.24454622876356386\n",
      "Cost at iteration 1280: 0.2444470653944105\n",
      "Cost at iteration 1281: 0.2443480185095097\n",
      "Cost at iteration 1282: 0.24424908790819416\n",
      "Cost at iteration 1283: 0.24415027339024428\n",
      "Cost at iteration 1284: 0.24405157475588687\n",
      "Cost at iteration 1285: 0.24395299180579402\n",
      "Cost at iteration 1286: 0.2438545243410821\n",
      "Cost at iteration 1287: 0.24375617216331005\n",
      "Cost at iteration 1288: 0.24365793507447864\n",
      "Cost at iteration 1289: 0.24355981287702919\n",
      "Cost at iteration 1290: 0.24346180537384204\n",
      "Cost at iteration 1291: 0.2433639123682358\n",
      "Cost at iteration 1292: 0.2432661336639659\n",
      "Cost at iteration 1293: 0.2431684690652235\n",
      "Cost at iteration 1294: 0.24307091837663433\n",
      "Cost at iteration 1295: 0.24297348140325734\n",
      "Cost at iteration 1296: 0.2428761579505838\n",
      "Cost at iteration 1297: 0.24277894782453588\n",
      "Cost at iteration 1298: 0.24268185083146565\n",
      "Cost at iteration 1299: 0.24258486677815394\n",
      "Cost at iteration 1300: 0.24248799547180885\n",
      "Cost at iteration 1301: 0.2423912367200652\n",
      "Cost at iteration 1302: 0.24229459033098275\n",
      "Cost at iteration 1303: 0.24219805611304548\n",
      "Cost at iteration 1304: 0.24210163387516023\n",
      "Cost at iteration 1305: 0.24200532342665562\n",
      "Cost at iteration 1306: 0.24190912457728103\n",
      "Cost at iteration 1307: 0.2418130371372053\n",
      "Cost at iteration 1308: 0.24171706091701567\n",
      "Cost at iteration 1309: 0.24162119572771656\n",
      "Cost at iteration 1310: 0.24152544138072876\n",
      "Cost at iteration 1311: 0.24142979768788783\n",
      "Cost at iteration 1312: 0.24133426446144343\n",
      "Cost at iteration 1313: 0.24123884151405803\n",
      "Cost at iteration 1314: 0.24114352865880567\n",
      "Cost at iteration 1315: 0.241048325709171\n",
      "Cost at iteration 1316: 0.24095323247904832\n",
      "Cost at iteration 1317: 0.24085824878274026\n",
      "Cost at iteration 1318: 0.24076337443495657\n",
      "Cost at iteration 1319: 0.2406686092508135\n",
      "Cost at iteration 1320: 0.24057395304583218\n",
      "Cost at iteration 1321: 0.240479405635938\n",
      "Cost at iteration 1322: 0.2403849668374592\n",
      "Cost at iteration 1323: 0.24029063646712592\n",
      "Cost at iteration 1324: 0.24019641434206904\n",
      "Cost at iteration 1325: 0.2401023002798194\n",
      "Cost at iteration 1326: 0.2400082940983063\n",
      "Cost at iteration 1327: 0.23991439561585673\n",
      "Cost at iteration 1328: 0.2398206046511943\n",
      "Cost at iteration 1329: 0.2397269210234381\n",
      "Cost at iteration 1330: 0.2396333445521015\n",
      "Cost at iteration 1331: 0.2395398750570916\n",
      "Cost at iteration 1332: 0.23944651235870748\n",
      "Cost at iteration 1333: 0.2393532562776399\n",
      "Cost at iteration 1334: 0.2392601066349696\n",
      "Cost at iteration 1335: 0.23916706325216663\n",
      "Cost at iteration 1336: 0.23907412595108932\n",
      "Cost at iteration 1337: 0.23898129455398312\n",
      "Cost at iteration 1338: 0.23888856888347965\n",
      "Cost at iteration 1339: 0.23879594876259558\n",
      "Cost at iteration 1340: 0.23870343401473174\n",
      "Cost at iteration 1341: 0.238611024463672\n",
      "Cost at iteration 1342: 0.23851871993358234\n",
      "Cost at iteration 1343: 0.23842652024900984\n",
      "Cost at iteration 1344: 0.23833442523488152\n",
      "Cost at iteration 1345: 0.2382424347165035\n",
      "Cost at iteration 1346: 0.23815054851955988\n",
      "Cost at iteration 1347: 0.238058766470112\n",
      "Cost at iteration 1348: 0.23796708839459702\n",
      "Cost at iteration 1349: 0.2378755141198272\n",
      "Cost at iteration 1350: 0.23778404347298915\n",
      "Cost at iteration 1351: 0.23769267628164212\n",
      "Cost at iteration 1352: 0.2376014123737178\n",
      "Cost at iteration 1353: 0.23751025157751876\n",
      "Cost at iteration 1354: 0.23741919372171794\n",
      "Cost at iteration 1355: 0.23732823863535732\n",
      "Cost at iteration 1356: 0.23723738614784703\n",
      "Cost at iteration 1357: 0.23714663608896455\n",
      "Cost at iteration 1358: 0.23705598828885355\n",
      "Cost at iteration 1359: 0.23696544257802307\n",
      "Cost at iteration 1360: 0.23687499878734639\n",
      "Cost at iteration 1361: 0.23678465674806032\n",
      "Cost at iteration 1362: 0.236694416291764\n",
      "Cost at iteration 1363: 0.23660427725041808\n",
      "Cost at iteration 1364: 0.2365142394563437\n",
      "Cost at iteration 1365: 0.2364243027422217\n",
      "Cost at iteration 1366: 0.23633446694109148\n",
      "Cost at iteration 1367: 0.23624473188635034\n",
      "Cost at iteration 1368: 0.23615509741175209\n",
      "Cost at iteration 1369: 0.23606556335140647\n",
      "Cost at iteration 1370: 0.2359761295397783\n",
      "Cost at iteration 1371: 0.23588679581168626\n",
      "Cost at iteration 1372: 0.23579756200230198\n",
      "Cost at iteration 1373: 0.23570842794714958\n",
      "Cost at iteration 1374: 0.235619393482104\n",
      "Cost at iteration 1375: 0.2355304584433908\n",
      "Cost at iteration 1376: 0.2354416226675848\n",
      "Cost at iteration 1377: 0.23535288599160925\n",
      "Cost at iteration 1378: 0.2352642482527352\n",
      "Cost at iteration 1379: 0.23517570928858011\n",
      "Cost at iteration 1380: 0.23508726893710744\n",
      "Cost at iteration 1381: 0.2349989270366254\n",
      "Cost at iteration 1382: 0.23491068342578628\n",
      "Cost at iteration 1383: 0.2348225379435853\n",
      "Cost at iteration 1384: 0.23473449042936\n",
      "Cost at iteration 1385: 0.23464654072278923\n",
      "Cost at iteration 1386: 0.23455868866389223\n",
      "Cost at iteration 1387: 0.23447093409302777\n",
      "Cost at iteration 1388: 0.2343832768508933\n",
      "Cost at iteration 1389: 0.2342957167785241\n",
      "Cost at iteration 1390: 0.2342082537172923\n",
      "Cost at iteration 1391: 0.23412088750890597\n",
      "Cost at iteration 1392: 0.23403361799540864\n",
      "Cost at iteration 1393: 0.23394644501917788\n",
      "Cost at iteration 1394: 0.2338593684229248\n",
      "Cost at iteration 1395: 0.23377238804969297\n",
      "Cost at iteration 1396: 0.23368550374285807\n",
      "Cost at iteration 1397: 0.23359871534612603\n",
      "Cost at iteration 1398: 0.23351202270353333\n",
      "Cost at iteration 1399: 0.23342542565944532\n",
      "Cost at iteration 1400: 0.23333892405855577\n",
      "Cost at iteration 1401: 0.23325251774588582\n",
      "Cost at iteration 1402: 0.23316620656678347\n",
      "Cost at iteration 1403: 0.23307999036692217\n",
      "Cost at iteration 1404: 0.2329938689923005\n",
      "Cost at iteration 1405: 0.2329078422892412\n",
      "Cost at iteration 1406: 0.23282191010439013\n",
      "Cost at iteration 1407: 0.23273607228471577\n",
      "Cost at iteration 1408: 0.232650328677508\n",
      "Cost at iteration 1409: 0.2325646791303778\n",
      "Cost at iteration 1410: 0.23247912349125574\n",
      "Cost at iteration 1411: 0.23239366160839192\n",
      "Cost at iteration 1412: 0.2323082933303544\n",
      "Cost at iteration 1413: 0.23222301850602922\n",
      "Cost at iteration 1414: 0.2321378369846186\n",
      "Cost at iteration 1415: 0.232052748615641\n",
      "Cost at iteration 1416: 0.23196775324892988\n",
      "Cost at iteration 1417: 0.231882850734633\n",
      "Cost at iteration 1418: 0.23179804092321152\n",
      "Cost at iteration 1419: 0.23171332366543926\n",
      "Cost at iteration 1420: 0.2316286988124021\n",
      "Cost at iteration 1421: 0.2315441662154968\n",
      "Cost at iteration 1422: 0.2314597257264306\n",
      "Cost at iteration 1423: 0.23137537719721987\n",
      "Cost at iteration 1424: 0.23129112048019015\n",
      "Cost at iteration 1425: 0.23120695542797473\n",
      "Cost at iteration 1426: 0.23112288189351382\n",
      "Cost at iteration 1427: 0.23103889973005423\n",
      "Cost at iteration 1428: 0.2309550087911484\n",
      "Cost at iteration 1429: 0.23087120893065327\n",
      "Cost at iteration 1430: 0.2307875000027301\n",
      "Cost at iteration 1431: 0.2307038818618432\n",
      "Cost at iteration 1432: 0.2306203543627595\n",
      "Cost at iteration 1433: 0.23053691736054757\n",
      "Cost at iteration 1434: 0.23045357071057693\n",
      "Cost at iteration 1435: 0.2303703142685173\n",
      "Cost at iteration 1436: 0.23028714789033788\n",
      "Cost at iteration 1437: 0.23020407143230642\n",
      "Cost at iteration 1438: 0.23012108475098864\n",
      "Cost at iteration 1439: 0.23003818770324744\n",
      "Cost at iteration 1440: 0.22995538014624206\n",
      "Cost at iteration 1441: 0.22987266193742745\n",
      "Cost at iteration 1442: 0.22979003293455347\n",
      "Cost at iteration 1443: 0.22970749299566404\n",
      "Cost at iteration 1444: 0.22962504197909664\n",
      "Cost at iteration 1445: 0.2295426797434813\n",
      "Cost at iteration 1446: 0.22946040614774008\n",
      "Cost at iteration 1447: 0.2293782210510862\n",
      "Cost at iteration 1448: 0.22929612431302338\n",
      "Cost at iteration 1449: 0.229214115793345\n",
      "Cost at iteration 1450: 0.22913219535213358\n",
      "Cost at iteration 1451: 0.22905036284975971\n",
      "Cost at iteration 1452: 0.2289686181468818\n",
      "Cost at iteration 1453: 0.22888696110444479\n",
      "Cost at iteration 1454: 0.22880539158368002\n",
      "Cost at iteration 1455: 0.22872390944610396\n",
      "Cost at iteration 1456: 0.22864251455351797\n",
      "Cost at iteration 1457: 0.22856120676800726\n",
      "Cost at iteration 1458: 0.22847998595194022\n",
      "Cost at iteration 1459: 0.2283988519679679\n",
      "Cost at iteration 1460: 0.22831780467902313\n",
      "Cost at iteration 1461: 0.2282368439483198\n",
      "Cost at iteration 1462: 0.22815596963935234\n",
      "Cost at iteration 1463: 0.22807518161589474\n",
      "Cost at iteration 1464: 0.22799447974200018\n",
      "Cost at iteration 1465: 0.2279138638819999\n",
      "Cost at iteration 1466: 0.2278333339005031\n",
      "Cost at iteration 1467: 0.22775288966239549\n",
      "Cost at iteration 1468: 0.2276725310328394\n",
      "Cost at iteration 1469: 0.22759225787727252\n",
      "Cost at iteration 1470: 0.22751207006140733\n",
      "Cost at iteration 1471: 0.22743196745123065\n",
      "Cost at iteration 1472: 0.22735194991300264\n",
      "Cost at iteration 1473: 0.22727201731325627\n",
      "Cost at iteration 1474: 0.22719216951879675\n",
      "Cost at iteration 1475: 0.22711240639670063\n",
      "Cost at iteration 1476: 0.22703272781431524\n",
      "Cost at iteration 1477: 0.2269531336392581\n",
      "Cost at iteration 1478: 0.22687362373941597\n",
      "Cost at iteration 1479: 0.2267941979829445\n",
      "Cost at iteration 1480: 0.2267148562382674\n",
      "Cost at iteration 1481: 0.22663559837407576\n",
      "Cost at iteration 1482: 0.2265564242593275\n",
      "Cost at iteration 1483: 0.22647733376324664\n",
      "Cost at iteration 1484: 0.22639832675532245\n",
      "Cost at iteration 1485: 0.22631940310530915\n",
      "Cost at iteration 1486: 0.22624056268322504\n",
      "Cost at iteration 1487: 0.22616180535935185\n",
      "Cost at iteration 1488: 0.22608313100423413\n",
      "Cost at iteration 1489: 0.22600453948867866\n",
      "Cost at iteration 1490: 0.22592603068375366\n",
      "Cost at iteration 1491: 0.22584760446078822\n",
      "Cost at iteration 1492: 0.22576926069137157\n",
      "Cost at iteration 1493: 0.22569099924735272\n",
      "Cost at iteration 1494: 0.2256128200008394\n",
      "Cost at iteration 1495: 0.22553472282419784\n",
      "Cost at iteration 1496: 0.22545670759005174\n",
      "Cost at iteration 1497: 0.22537877417128196\n",
      "Cost at iteration 1498: 0.2253009224410256\n",
      "Cost at iteration 1499: 0.2252231522726757\n",
      "The cost after training is 0.22522315.\n",
      "The resulting vector of weights is [6e-08, 0.00053818, -0.0005583]\n"
     ]
    }
   ],
   "source": [
    "# collect the features 'x' and stack them into a matrix 'X'\n",
    "X = np.zeros((len(train_x), 3))\n",
    "for i in range(len(train_x)):\n",
    "    X[i, :]= extract_features(train_x[i], freqs)\n",
    "\n",
    "# training labels corresponding to X\n",
    "Y = train_y\n",
    "\n",
    "# Apply gradient descent\n",
    "J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 1500)\n",
    "print(f\"The cost after training is {J:.8f}.\")\n",
    "print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "```\n",
    "The cost after training is 0.22522315.\n",
    "The resulting vector of weights is [6e-08, 0.00053818, -0.0005583]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 -  Test your Logistic Regression\n",
    "\n",
    "It is time for you to test your logistic regression function on some new input that your model has not seen before. \n",
    "<a name='ex-4'></a>\n",
    "### Exercise 4 - predict_tweet\n",
    "Implement `predict_tweet`.\n",
    "Predict whether a tweet is positive or negative.\n",
    "\n",
    "* Given a tweet, process it, then extract the features.\n",
    "* Apply the model's learned weights on the features to get the logits.\n",
    "* Apply the sigmoid to the logits to get the prediction (a value between 0 and 1).\n",
    "\n",
    "$$y_{pred} = sigmoid(\\mathbf{x} \\cdot \\theta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C4 GRADED FUNCTION: predict_tweet\n",
    "def predict_tweet(tweet, freqs, theta):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet: a string\n",
    "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
    "        theta: (3,1) vector of weights\n",
    "    Output: \n",
    "        y_pred: the probability of a tweet being positive or negative\n",
    "    '''\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # extract the features of the tweet and store it into x\n",
    "    x = extract_features(tweet,freqs)\n",
    "    \n",
    "    # make the prediction using x and theta\n",
    "    y_pred = sigmoid(np.dot(x,theta))\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am happy -> 0.519275\n",
      "I am bad -> 0.494347\n",
      "this movie should have been great. -> 0.515979\n",
      "great -> 0.516065\n",
      "great great -> 0.532096\n",
      "great great great -> 0.548062\n",
      "great great great great -> 0.563929\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to test your function\n",
    "for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n",
    "    print( '%s -> %f' % (tweet, predict_tweet(tweet, freqs, theta)))    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "```\n",
    "I am happy -> 0.519275\n",
    "I am bad -> 0.494347\n",
    "this movie should have been great. -> 0.515979\n",
    "great -> 0.516065\n",
    "great great -> 0.532096\n",
    "great great great -> 0.548062\n",
    "great great great great -> 0.563929\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.83110307]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feel free to check the sentiment of your own tweet below\n",
    "my_tweet = 'I am learning :)'\n",
    "predict_tweet(my_tweet, freqs, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "w1_unittest.test_predict_tweet(predict_tweet, freqs, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4-1'></a>\n",
    "### 4.1 -  Check the Performance using the Test Set\n",
    "After training your model using the training set above, check how your model might perform on real, unseen data, by testing it against the test set.\n",
    "\n",
    "<a name='ex-5'></a>\n",
    "### Exercise 5 - test_logistic_regression\n",
    "Implement `test_logistic_regression`. \n",
    "* Given the test data and the weights of your trained model, calculate the accuracy of your logistic regression model. \n",
    "* Use your 'predict_tweet' function to make predictions on each tweet in the test set.\n",
    "* If the prediction is > 0.5, set the model's classification 'y_hat' to 1, otherwise set the model's classification 'y_hat' to 0.\n",
    "* A prediction is accurate when the y_hat equals the test_y.  Sum up all the instances when they are equal and divide by m.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Use np.asarray() to convert a list to a numpy array</li>\n",
    "    <li>Use numpy.squeeze() to make an (m,1) dimensional array into an (m,) array </li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C5 GRADED FUNCTION: test_logistic_regression\n",
    "def test_logistic_regression(test_x, test_y, freqs, theta, predict_tweet=predict_tweet):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        test_x: a list of tweets\n",
    "        test_y: (m, 1) vector with the corresponding labels for the list of tweets\n",
    "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
    "        theta: weight vector of dimension (3, 1)\n",
    "    Output: \n",
    "        accuracy: (# of tweets classified correctly) / (total # of tweets)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # the list for storing predictions\n",
    "    y_hat = []\n",
    "    for tweet in test_x:\n",
    "        # get the label prediction for the tweet\n",
    "        y_pred = predict_tweet(tweet, freqs, theta)\n",
    "        \n",
    "        if y_pred > 0.5:\n",
    "            # append 1.0 to the list\n",
    "            y_hat.append(1.0)\n",
    "        else:\n",
    "            # append 0 to the list\n",
    "            y_hat.append(0.0)\n",
    "\n",
    "    # With the above implementation, y_hat is a list, but test_y is (m,1) array\n",
    "    # convert both to one-dimensional arrays in order to compare them using the '==' operator\n",
    "    y_hat = np.array(y_hat)\n",
    "    test_y = test_y.squeeze()\n",
    "    \n",
    "    # compare the predicted labels with the true labels and calculate accuracy\n",
    "    accuracy = np.mean(y_hat == test_y)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression model's accuracy = 0.9950\n"
     ]
    }
   ],
   "source": [
    "tmp_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\n",
    "print(f\"Logistic regression model's accuracy = {tmp_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output: \n",
    "```0.9950```  \n",
    "Pretty good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "w1_unittest.unittest_test_logistic_regression(test_logistic_regression, freqs, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5 -  Error Analysis\n",
    "\n",
    "In this part you will see some tweets that your model misclassified. Why do you think the misclassifications happened? Specifically what kind of tweets does your model misclassify?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Predicted Tweet\n",
      "THE TWEET IS: @MarkBreech Not sure it would be good thing 4 my bottom daring 2 say 2 Miss B but Im gonna be so stubborn on mouth soaping ! #NotHavingit :p\n",
      "THE PROCESSED TWEET IS: ['sure', 'would', 'good', 'thing', '4', 'bottom', 'dare', '2', 'say', '2', 'miss', 'b', 'im', 'gonna', 'stubborn', 'mouth', 'soap', 'nothavingit', ':p']\n",
      "1\t0.48901497\tb'sure would good thing 4 bottom dare 2 say 2 miss b im gonna stubborn mouth soap nothavingit :p'\n",
      "THE TWEET IS: I'm playing Brain Dots : ) #BrainDots\n",
      "http://t.co/UGQzOx0huu\n",
      "THE PROCESSED TWEET IS: [\"i'm\", 'play', 'brain', 'dot', 'braindot']\n",
      "1\t0.48418949\tb\"i'm play brain dot braindot\"\n",
      "THE TWEET IS: I'm playing Brain Dots : ) #BrainDots http://t.co/aOKldo3GMj http://t.co/xWCM9qyRG5\n",
      "THE PROCESSED TWEET IS: [\"i'm\", 'play', 'brain', 'dot', 'braindot']\n",
      "1\t0.48418949\tb\"i'm play brain dot braindot\"\n",
      "THE TWEET IS: I'm playing Brain Dots : ) #BrainDots http://t.co/R2JBO8iNww http://t.co/ow5BBwdEMY\n",
      "THE PROCESSED TWEET IS: [\"i'm\", 'play', 'brain', 'dot', 'braindot']\n",
      "1\t0.48418949\tb\"i'm play brain dot braindot\"\n",
      "THE TWEET IS: off to the park to get some sunlight : )\n",
      "THE PROCESSED TWEET IS: ['park', 'get', 'sunlight']\n",
      "1\t0.49636374\tb'park get sunlight'\n",
      "THE TWEET IS: @msarosh Uff Itna Miss karhy thy ap :p\n",
      "THE PROCESSED TWEET IS: ['uff', 'itna', 'miss', 'karhi', 'thi', 'ap', ':p']\n",
      "1\t0.48237069\tb'uff itna miss karhi thi ap :p'\n",
      "THE TWEET IS: @phenomyoutube u probs had more fun with david than me : (\n",
      "THE PROCESSED TWEET IS: ['u', 'prob', 'fun', 'david']\n",
      "0\t0.50988239\tb'u prob fun david'\n",
      "THE TWEET IS: pats jay : (\n",
      "THE PROCESSED TWEET IS: ['pat', 'jay']\n",
      "0\t0.50040365\tb'pat jay'\n",
      "THE TWEET IS: my beloved grandmother : ( https://t.co/wt4oXq5xCf\n",
      "THE PROCESSED TWEET IS: ['belov', 'grandmoth']\n",
      "0\t0.50000002\tb'belov grandmoth'\n",
      "THE TWEET IS: Sr. Financial Analyst - Expedia, Inc.: (#Bellevue, WA) http://t.co/ktknMhvwCI #Finance #ExpediaJobs #Job #Jobs #Hiring\n",
      "THE PROCESSED TWEET IS: ['sr', 'financi', 'analyst', 'expedia', 'inc', 'bellevu', 'wa', 'financ', 'expediajob', 'job', 'job', 'hire']\n",
      "0\t0.50648681\tb'sr financi analyst expedia inc bellevu wa financ expediajob job job hire'\n"
     ]
    }
   ],
   "source": [
    "# Some error analysis done for you\n",
    "print('Label Predicted Tweet')\n",
    "for x,y in zip(test_x,test_y):\n",
    "    y_hat = predict_tweet(x, freqs, theta)\n",
    "\n",
    "    if np.abs(y - (y_hat > 0.5)) > 0:\n",
    "        print('THE TWEET IS:', x)\n",
    "        print('THE PROCESSED TWEET IS:', process_tweet(x))\n",
    "        print('%d\\t%0.8f\\t%s' % (y, y_hat, ' '.join(process_tweet(x)).encode('ascii', 'ignore')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later in this specialization, we will see how we can use deeplearning to improve the prediction performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6'></a>\n",
    "## 6 - Predict with your own Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ridicul', 'bright', 'movi', 'plot', 'terribl', 'sad', 'end']\n",
      "[[0.48125423]]\n",
      "Negative sentiment\n"
     ]
    }
   ],
   "source": [
    "# Feel free to change the tweet below\n",
    "my_tweet = 'This is a ridiculously bright movie. The plot was terrible and I was sad until the ending!'\n",
    "print(process_tweet(my_tweet))\n",
    "y_hat = predict_tweet(my_tweet, freqs, theta)\n",
    "print(y_hat)\n",
    "if y_hat > 0.5:\n",
    "    print('Positive sentiment')\n",
    "else: \n",
    "    print('Negative sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
